<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>A Concise Proof of the Central Limit Theorem, and Its Actually Useful Version, the Berry-Esseen Theorem | Fan Pu  Zeng</title>
    <meta name="author" content="Fan Pu  Zeng">
    <meta name="description" content="The Central Limit Theorem is widely used in statistics and machine learning, as it allows us to assume that given enough samples, the mean of the samples will follow a normal distribution. This holds even if the samples come from a distribution that is not normally distributed. In this post, we prove the Central Limit Theorem, and then take a look at the Berry-Esseen Theorem, which actually provides a quantitative bound on the convergence of the distribution and can therefore be actually used in deriving theoretical bounds.
">
    <meta name="keywords" content="fanpu, fan pu, fanpu zeng, fan pu zeng, zengfanpu, fanpuzeng, fzeng, CMU, cmu, carnegie mellon, cmu courses, school of computer science, scs, machine learning, computer science, ml, theory, courses, course reviews, CS, Jane Street">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    
    <!-- Sidebar Table of Contents -->
    <link href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css" rel="stylesheet">
    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon_new.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fanpu.io/blog/2022/central-limit-theorem-and-berry-esseen/">
    <!-- Dark Mode -->
    

    <!-- Twitter cards -->
    <meta name="twitter:site" content="@FanPu_Zeng">
    <meta name="twitter:creator" content="@fanpu">
    <meta name="og:title" content="A Concise Proof of the Central Limit Theorem, and Its Actually Useful Version, the Berry-Esseen Theorem">

    
    <meta name="twitter:card" content="summary_large_image">
    
    <meta name="og:image" content="https://fanpu.io/assets/img/posts/maine.webp">

    

    
    <meta name="og:description" content="The Central Limit Theorem is widely used in statistics and machine learning, as it allows us to assume that given enough samples, the mean of the samples will follow a normal distribution. This holds even if the samples come from a distribution that is not normally distributed. In this post, we prove the Central Limit Theorem, and then take a look at the Berry-Esseen Theorem, which actually provides a quantitative bound on the convergence of the distribution and can therefore be actually used in deriving theoretical bounds.
">
    

    <!-- end of Twitter cards -->
  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Fan Pu </span>Zeng</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/courses/">CMU Course Reviews</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cmu-online/">CMU Online</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/summaries/">ML Paper Summaries</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">Repositories</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        
        <div class="row">
          <!-- sidebar, which will move to the top on a small screen -->
          <div class="col-sm-3">
            <nav id="toc-sidebar" class="sticky-top"></nav>
          </div>
          <!-- main content area -->
          <div class="col-sm-9">
            <!-- _layouts/post.html -->

<div class="post">

  <div style="display:none">
    $$
    \newcommand{\bone}{\mathbf{1}}
    \newcommand{\bbeta}{\mathbf{\beta}}
    \newcommand{\bdelta}{\mathbf{\delta}}
    \newcommand{\bepsilon}{\mathbf{\epsilon}}
    \newcommand{\blambda}{\mathbf{\lambda}}
    \newcommand{\bomega}{\mathbf{\omega}}
    \newcommand{\bpi}{\mathbf{\pi}}
    \newcommand{\bphi}{\mathbf{\phi}}
    \newcommand{\bvphi}{\mathbf{\varphi}}
    \newcommand{\bpsi}{\mathbf{\psi}}
    \newcommand{\bsigma}{\mathbf{\sigma}}
    \newcommand{\btheta}{\mathbf{\theta}}
    \newcommand{\btau}{\mathbf{\tau}}
    \newcommand{\ba}{\mathbf{a}}
    \newcommand{\bb}{\mathbf{b}}
    \newcommand{\bc}{\mathbf{c}}
    \newcommand{\bd}{\mathbf{d}}
    \newcommand{\be}{\mathbf{e}}
    \newcommand{\boldf}{\mathbf{f}}
    \newcommand{\bg}{\mathbf{g}}
    \newcommand{\bh}{\mathbf{h}}
    \newcommand{\bi}{\mathbf{i}}
    \newcommand{\bj}{\mathbf{j}}
    \newcommand{\bk}{\mathbf{k}}
    \newcommand{\bell}{\mathbf{\ell}}
    \newcommand{\bm}{\mathbf{m}}
    \newcommand{\bn}{\mathbf{n}}
    \newcommand{\bo}{\mathbf{o}}
    \newcommand{\bp}{\mathbf{p}}
    \newcommand{\bq}{\mathbf{q}}
    \newcommand{\br}{\mathbf{r}}
    \newcommand{\bs}{\mathbf{s}}
    \newcommand{\bt}{\mathbf{t}}
    \newcommand{\bu}{\mathbf{u}}
    \newcommand{\bv}{\mathbf{v}}
    \newcommand{\bw}{\mathbf{w}}
    \newcommand{\bx}{\mathbf{x}}
    \newcommand{\by}{\mathbf{y}}
    \newcommand{\bz}{\mathbf{z}}
    \newcommand{\bA}{\mathbf{A}}
    \newcommand{\bB}{\mathbf{B}}
    \newcommand{\bC}{\mathbf{C}}
    \newcommand{\bD}{\mathbf{D}}
    \newcommand{\bE}{\mathbf{E}}
    \newcommand{\bF}{\mathbf{F}}
    \newcommand{\bG}{\mathbf{G}}
    \newcommand{\bH}{\mathbf{H}}
    \newcommand{\bI}{\mathbf{I}}
    \newcommand{\bJ}{\mathbf{J}}
    \newcommand{\bK}{\mathbf{K}}
    \newcommand{\bL}{\mathbf{L}}
    \newcommand{\bM}{\mathbf{M}}
    \newcommand{\bN}{\mathbf{N}}
    \newcommand{\bP}{\mathbf{P}}
    \newcommand{\bQ}{\mathbf{Q}}
    \newcommand{\bR}{\mathbf{R}}
    \newcommand{\bS}{\mathbf{S}}
    \newcommand{\bT}{\mathbf{T}}
    \newcommand{\bU}{\mathbf{U}}
    \newcommand{\bV}{\mathbf{V}}
    \newcommand{\bW}{\mathbf{W}}
    \newcommand{\bX}{\mathbf{X}}
    \newcommand{\bY}{\mathbf{Y}}
    \newcommand{\bZ}{\mathbf{Z}}

    \newcommand{\bsa}{\boldsymbol{a}}
    \newcommand{\bsb}{\boldsymbol{b}}
    \newcommand{\bsc}{\boldsymbol{c}}
    \newcommand{\bsd}{\boldsymbol{d}}
    \newcommand{\bse}{\boldsymbol{e}}
    \newcommand{\bsoldf}{\boldsymbol{f}}
    \newcommand{\bsg}{\boldsymbol{g}}
    \newcommand{\bsh}{\boldsymbol{h}}
    \newcommand{\bsi}{\boldsymbol{i}}
    \newcommand{\bsj}{\boldsymbol{j}}
    \newcommand{\bsk}{\boldsymbol{k}}
    \newcommand{\bsell}{\boldsymbol{\ell}}
    \newcommand{\bsm}{\boldsymbol{m}}
    \newcommand{\bsn}{\boldsymbol{n}}
    \newcommand{\bso}{\boldsymbol{o}}
    \newcommand{\bsp}{\boldsymbol{p}}
    \newcommand{\bsq}{\boldsymbol{q}}
    \newcommand{\bsr}{\boldsymbol{r}}
    \newcommand{\bss}{\boldsymbol{s}}
    \newcommand{\bst}{\boldsymbol{t}}
    \newcommand{\bsu}{\boldsymbol{u}}
    \newcommand{\bsv}{\boldsymbol{v}}
    \newcommand{\bsw}{\boldsymbol{w}}
    \newcommand{\bsx}{\boldsymbol{x}}
    \newcommand{\bsy}{\boldsymbol{y}}
    \newcommand{\bsz}{\boldsymbol{z}}
    \newcommand{\bsA}{\boldsymbol{A}}
    \newcommand{\bsB}{\boldsymbol{B}}
    \newcommand{\bsC}{\boldsymbol{C}}
    \newcommand{\bsD}{\boldsymbol{D}}
    \newcommand{\bsE}{\boldsymbol{E}}
    \newcommand{\bsF}{\boldsymbol{F}}
    \newcommand{\bsG}{\boldsymbol{G}}
    \newcommand{\bsH}{\boldsymbol{H}}
    \newcommand{\bsI}{\boldsymbol{I}}
    \newcommand{\bsJ}{\boldsymbol{J}}
    \newcommand{\bsK}{\boldsymbol{K}}
    \newcommand{\bsL}{\boldsymbol{L}}
    \newcommand{\bsM}{\boldsymbol{M}}
    \newcommand{\bsN}{\boldsymbol{N}}
    \newcommand{\bsP}{\boldsymbol{P}}
    \newcommand{\bsQ}{\boldsymbol{Q}}
    \newcommand{\bsR}{\boldsymbol{R}}
    \newcommand{\bsS}{\boldsymbol{S}}
    \newcommand{\bsT}{\boldsymbol{T}}
    \newcommand{\bsU}{\boldsymbol{U}}
    \newcommand{\bsV}{\boldsymbol{V}}
    \newcommand{\bsW}{\boldsymbol{W}}
    \newcommand{\bsX}{\boldsymbol{X}}
    \newcommand{\bsY}{\boldsymbol{Y}}
    \newcommand{\bsZ}{\boldsymbol{Z}}

    \newcommand{\calA}{\mathcal{A}}
    \newcommand{\calB}{\mathcal{B}}
    \newcommand{\calC}{\mathcal{C}}
    \newcommand{\calD}{\mathcal{D}}
    \newcommand{\calE}{\mathcal{E}}
    \newcommand{\calF}{\mathcal{F}}
    \newcommand{\calG}{\mathcal{G}}
    \newcommand{\calH}{\mathcal{H}}
    \newcommand{\calI}{\mathcal{I}}
    \newcommand{\calJ}{\mathcal{J}}
    \newcommand{\calK}{\mathcal{K}}
    \newcommand{\calL}{\mathcal{L}}
    \newcommand{\calM}{\mathcal{M}}
    \newcommand{\calN}{\mathcal{N}}
    \newcommand{\calO}{\mathcal{O}}
    \newcommand{\calP}{\mathcal{P}}
    \newcommand{\calQ}{\mathcal{Q}}
    \newcommand{\calR}{\mathcal{R}}
    \newcommand{\calS}{\mathcal{S}}
    \newcommand{\calT}{\mathcal{T}}
    \newcommand{\calU}{\mathcal{U}}
    \newcommand{\calV}{\mathcal{V}}
    \newcommand{\calW}{\mathcal{W}}
    \newcommand{\calX}{\mathcal{X}}
    \newcommand{\calY}{\mathcal{Y}}
    \newcommand{\calZ}{\mathcal{Z}}

    \newcommand{\R}{\mathbb{R}}
    \newcommand{\C}{\mathbb{C}}
    \newcommand{\N}{\mathbb{N}}
    \newcommand{\Z}{\mathbb{Z}}
    \newcommand{\F}{\mathbb{F}}
    \newcommand{\Q}{\mathbb{Q}}

    \DeclareMathOperator*{\argmax}{arg\,max}
    \DeclareMathOperator*{\argmin}{arg\,min}
    \newcommand{\nnz}[1]{\mbox{nnz}(#1)}
    \newcommand{\dotprod}[2]{\langle #1, #2 \rangle}

    \newcommand{\ignore}[1]{}

    \let\Pr\relax
    \DeclareMathOperator*{\Pr}{\mathbf{Pr}}
    \newcommand{\E}{\mathbb{E}}
    \DeclareMathOperator*{\Ex}{\mathbf{E}}
    \DeclareMathOperator*{\Var}{\mathbf{Var}}
    \DeclareMathOperator*{\Cov}{\mathbf{Cov}}
    \DeclareMathOperator*{\stddev}{\mathbf{stddev}}
    \DeclareMathOperator*{\avg}{avg}

    \DeclareMathOperator{\poly}{poly}
    \DeclareMathOperator{\polylog}{polylog}
    \DeclareMathOperator{\size}{size}
    \DeclareMathOperator{\sgn}{sgn}
    \DeclareMathOperator{\dist}{dist}
    \DeclareMathOperator{\vol}{vol}
    \DeclareMathOperator{\spn}{span}
    \DeclareMathOperator{\supp}{supp}
    \DeclareMathOperator{\tr}{tr}
    \DeclareMathOperator{\Tr}{Tr}
    \DeclareMathOperator{\codim}{codim}
    \DeclareMathOperator{\diag}{diag}

    \newcommand{\PTIME}{\mathsf{P}}
    \newcommand{\LOGSPACE}{\mathsf{L}}
    \newcommand{\ZPP}{\mathsf{ZPP}}
    \newcommand{\RP}{\mathsf{RP}}
    \newcommand{\BPP}{\mathsf{BPP}}
    \newcommand{\P}{\mathsf{P}}
    \newcommand{\NP}{\mathsf{NP}}
    \newcommand{\TC}{\mathsf{TC}}
    \newcommand{\AC}{\mathsf{AC}}
    \newcommand{\SC}{\mathsf{SC}}
    \newcommand{\SZK}{\mathsf{SZK}}
    \newcommand{\AM}{\mathsf{AM}}
    \newcommand{\IP}{\mathsf{IP}}
    \newcommand{\PSPACE}{\mathsf{PSPACE}}
    \newcommand{\EXP}{\mathsf{EXP}}
    \newcommand{\MIP}{\mathsf{MIP}}
    \newcommand{\NEXP}{\mathsf{NEXP}}
    \newcommand{\BQP}{\mathsf{BQP}}
    \newcommand{\distP}{\mathsf{dist\textbf{P}}}
    \newcommand{\distNP}{\mathsf{dist\textbf{NP}}}

    \newcommand{\eps}{\epsilon}
    \newcommand{\lam}{\lambda}
    \newcommand{\dleta}{\delta}
    \newcommand{\simga}{\sigma}
    \newcommand{\vphi}{\varphi}
    \newcommand{\la}{\langle}
    \newcommand{\ra}{\rangle}
    \newcommand{\wt}[1]{\widetilde{#1}}
    \newcommand{\wh}[1]{\widehat{#1}}
    \newcommand{\ol}[1]{\overline{#1}}
    \newcommand{\ul}[1]{\underline{#1}}
    \newcommand{\ot}{\otimes}
    \newcommand{\zo}{\{0,1\}}
    \newcommand{\co}{:} %\newcommand{\co}{\colon}
    \newcommand{\bdry}{\partial}
    \newcommand{\grad}{\nabla}
    \newcommand{\transp}{^\intercal}
    \newcommand{\inv}{^{-1}}
    \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff}
    \newcommand{\half}{\tfrac{1}{2}}
    \newcommand{\bbone}{\mathbbm 1}
    \newcommand{\Id}{\bbone}

    \newcommand{\SAT}{\mathsf{SAT}}

    \newcommand{\bcalG}{\boldsymbol{\calG}}
    \newcommand{\calbG}{\bcalG}
    \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX}
    \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY}
    \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ}
    $$
</div>

  <header class="post-header">
    <h1 class="post-title">A Concise Proof of the Central Limit Theorem, and Its Actually Useful Version, the Berry-Esseen Theorem</h1>
    <p class="post-meta">December 28, 2022• fanpu</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/tag/math">
          <i class="fas fa-hashtag fa-sm"></i> math</a>  
          <a href="/blog/tag/machine-learning">
          <i class="fas fa-hashtag fa-sm"></i> machine-learning</a>  
          

    </p>
  </header>

  
      <figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/maine.webp" class="preview z-depth-1 rounded center" width="100%" height="450px" alt="post.cover" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>
</figure>

    <div class="caption">
        Acadia National Park, Maine, USA
    </div>
  

  <article class="post-content">
    
    <div id="markdown-content">
      <p>The Central Limit Theorem is widely used in statistics and machine learning,
as it allows us to assume that given enough samples, the mean of the samples
will follow a normal distribution. This holds even if the samples come
from a distribution that is not normally distributed.
In this post, we prove the Central Limit Theorem, and then take a look
at the Berry-Esseen Theorem, which actually provides a quantitative bound
on the convergence of the distribution and can therefore be actually used in
deriving theoretical bounds.</p>

<h3 id="the-central-limit-theorem">The Central Limit Theorem</h3>

<p>The Central Limit states that the mean of an appropriately transformed random variable
converges in distribution to a standard normal. We first need to introduce the 
definition of convergence of probability distributions:</p>

<div class="definition">
    <div class="theorem-title">Definition
        
        
            (Convergence in Distribution)
        
    </div>
    <div class="theorem-contents">
        
  Let \( F_{X_n} \) and \( F_{X} \) denotes the cumulative density functions (CDF) of 
  \( X_n \) and \( X \) respectively.

  A sequence \( X_n \) converges to \( X \) in distribution if
  $$ \lim_{n \to \infty } F_{X_n}(t) = F_X (t)$$
  
  for all points \( t \) where \( F_X \) is continuous.
  
    </div>
</div>

<p>Note that the requirement that it only holds for points of continuity is not superfluous, as there
can be distributions that converge but disagree in value at points of discontinuities
(i.e take \(X_n = N(0, 1/n)\) and \(X\) to be the point mass at 0, they converge but their CDF take different values at \(t=0\)).</p>

<p>The Central Limit Theorem can then be stated in the following form (there are many other equivalent statements):</p>

<div class="theorem">
    <div class="theorem-title">Theorem
        
        
            (Central Limit Theorem)
        
    </div>
    <div class="theorem-contents">
        
  Let \( X_1, X_2, \dots, X_n \) be a sequence of independent random variables with mean \( \mu \) and variance \( \sigma^2 \).
  Assume that the moment generating function \( \mathbb{E} \left[ \exp(t X_i) \right] \) is finite for \( t \) in a neighborhood around zero.
  Let \( \overline{X}_n = \frac{1}{n} \sum\limits_{i=1}^n X_i \). Let
  
  $$ Z_n = \frac{\sqrt{n} \left( \overline{X}_n - \mu \right)}{\sigma}. $$
  
  Then \( Z_n \) converges in distribution to \( Z \sim N(0, 1) \).
  
    </div>
</div>

<h3 id="proof-of-the-central-limit-theorem">Proof of the Central Limit Theorem</h3>

<p>There are several ways of proving the Central Limit Theorem. 
The proof that we will explore today relies on the methods of moments.
An alternative measure-theoretic version of the proof relies on Lévy’s
Continuity Theorem, and makes use of convolutions and Fourier transforms.</p>

<p>Our goal is to show that \(Z_n\) converges in distribution to \(Z \sim N(0,
1)\). To do so, we will show that all the moments of \(Z_n\) converges to
the respective moments of \(Z\).</p>

<h4 id="moment-generating-functions">Moment Generating Functions</h4>
<p>The moments of a random variable can be obtained from its moment-generating function (MGF),
defined as follows:</p>

<div class="definition">
    <div class="theorem-title">Definition
        
        
            (Moment Generating Function)
        
    </div>
    <div class="theorem-contents">
        
  The moment generating function of a random variable \( X \) is given by
  
  $$ M_X(t) = \mathbb{E} \left[ e^{tX} \right].$$
  
    </div>
</div>

<p>It is called a moment generating function since the \(k\)th moment of \(X\),
i.e \(\mathbb{E} \left[X^k \right]\), can be obtained by taking the
\(k\)th derivative of its moment-generating function (MGF) at 0:</p>

\[\mathbb{E} \left[X^k \right]  = M^{(k)}(0).\]

<p>This is not too hard to see by induction on the fact that
\(M_X^k(t) = \mathbb{E} \left[ X^k e^{tX} \right]\). The base case is trivial. 
For the inductive case,</p>

\[\begin{align*}
    M_X^{(k)}(t) &amp; = \frac{d^k}{dt^k} \mathbb{E} \left[ e^{tX} \right] \\ 
               &amp; = \frac{d}{dt} \mathbb{E} \left[ X^{k-1} e^{tX} \right] &amp; \text{(by IH)}\\
               &amp; = \frac{d}{dt} \int f(x) x^{k-1} e^{tx} \; dx \\ 
               &amp; = \int \frac{d}{dt} f(x) x^{k-1} e^{tx} \; dx \\
               &amp; = \int f(x) x^{k} e^{tx} \; dx \\
               &amp; = \mathbb{E} \left[ X^{k} e^{tX} \right].
\end{align*}\]

<p>Substituting \(t=0\) gives us the desired result.</p>

<h4 id="normal-distribution-is-determined-by-its-moments">Normal Distribution is Determined by its Moments</h4>
<p>Distributions are determined uniquely by its moments under certain conditions. This is made precise
in the following theorem:</p>

<div class="theorem">
    <div class="theorem-title">Theorem
        
        
            (Sufficient Condition for Distribution to be Determined by Moments)
        
    </div>
    <div class="theorem-contents">
        
  Let \( s_0 &gt; 0 \), and let \( X \) be a random variable with moment generating
  function \( M_X(s) \) which is finite for \( |s| &lt; s_0 \). Then \( f_X \)
  is determined by its moments (and also by \( M_X(s)\)).
  
    </div>
</div>

<p>In words, it means that for some open interval around 0 we have that all moments are finite,
then the moments determine the distribution. This is true for the normal distribution,
where it can be shown that the following recurrence holds for the \(k\)th moment:</p>

\[M^k(t) = \mu M^{k-1}(t) + (k-1) \sigma^2 M^{k-2}(t).\]

<p>This is also not hard to show by induction, and the proof is omitted for brevity. Since the
first two moments of the standard normal distribution are 1 and 0 respectively which are both finite,
and our mean and standard deviation are both finite, then all our moments generated by the
recurrence must also be finite. So our standard normal is determined by its moments.</p>

<h4 id="method-of-moments">Method of Moments</h4>
<p>Now cue the theorem that ties things together:</p>

<div class="theorem">
    <div class="theorem-title">Theorem
        
        
            (Method of Moments)
        
    </div>
    <div class="theorem-contents">
        
  Suppose that \( X \) is determined by its moments. Let \( X_n \) be a sequence of
  distributions, such that \( \int f_{X_n}(x) x^k \; dx \) is finite for all \( n, k \in \N \),
  and such that \( \lim_{n \to \infty} \int f_{X_n}(x) x^k \; dx = \int f_{X}(x) x^k \; dx \)
  for each \( k \in \N \). Then \( X_n \) converges in distribution to \( X \).
  
    </div>
</div>

<p>In words, it states that if the \(k\)th moment of \(X_n\) is finite and converges to the \(k\)th moment
of \(X\) in the limit of \(n\), then \(X_n\) converges to \(X\).</p>

<p>This is great, since now we just have to show that all the moments of 
\(Z_n = \frac{\sqrt{n} \left( \overline{X}_n - \mu \right)}{\sigma}\) converges to
the moments of the standard normal \(Z\).</p>

<h3 id="moment-generating-function-of-z">Moment Generating Function of \(Z\)</h3>
<p>Let’s first find the moment generating function of \(Z\):</p>

\[\begin{align*}
    M_{Z} &amp; = \mathbb{E} \left[ e^{tZ} \right]                                                                                                \\
          &amp; = \int f_Z(x) e^{tx} \; dx                                                                                                \\
          &amp; = \int \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}x^2} e^{tx} \; dx                 &amp; \text{(subst. pdf of standard Gaussian)} \\
          &amp; = \int \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}x^2 + tx} \; dx                                                              \\
          &amp; = \int \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}(x - t)^2 + \frac{1}{2}t^2} \; dx &amp; \text{(completing the square)}           \\
          &amp; = e^{\frac{1}{2}t^2} \int \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}(x - t)^2 } \; dx &amp; \text{($e^{\frac{1}{2}t^2}$ does not depend on $x$)}           \\
          &amp; = e^{\frac{1}{2}t^2} \cdot  1 \\
          &amp; = e^{\frac{1}{2}t^2},
\end{align*}\]

<p>where the second last step comes from the fact that
\(\frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}(x - t)^2 }\) is a probability distribution of a Gaussian with mean \(t\) and variance 1, 
and therefore the integral integrates to 1.</p>

<h3 id="moment-generating-function-of-z_n">Moment Generating Function of \(Z_n\)</h3>

<p>Now we find the moment generating function of \(Z_n\). 
To simplify notation, define
\(A_i = \frac{X_i - \mu}{\sigma}\),
and see that we can write \(Z_n = \frac{1}{\sqrt{n}} \sum\limits_{i=1}^n A_i\), since</p>

\[\begin{align*}
    \frac{1}{\sqrt{n}} \sum\limits_{i=1}^n A_i
    &amp;= \frac{1}{\sqrt{n}} \sum\limits_{i=1}^n \frac{X_i - \mu}{\sigma} \\
    &amp;= \sqrt{n} \sum\limits_{i=1}^n \frac{X_i - \mu}{ n \sigma} \\
    &amp;= \sqrt{n} \frac{\overline{X}_n - \mu}{ \sigma} \\
    &amp;= Z_n.
\end{align*}\]

<p>See that \(\mathbb{E}[A_i] = 0\), and \(\mathbf{Var}(A_i) = 1\).</p>

<p>Then starting from the definition of the moment generating function of \(Z_n\),</p>

\[\begin{align*}
    M_{Z_n}(t) &amp; = \mathbb{E} \left[ e^{t Z_n} \right]                                                                                                   \\
               &amp; = \mathbb{E} \left[ \exp\left(t \frac{1}{\sqrt{n}} \sum\limits_{i=1}^n A_i \right) \right] &amp; \text{(by equivalent definition of $Z_n$)} \\
               &amp; = \prod_{i=1}^n \mathbb{E} \left[ \exp\left( \frac{t}{\sqrt{n}} A_i \right) \right]        &amp; \text{(by independence of $A_i$'s)}        \\
               &amp; = \prod_{i=1}^n M_{A_i}(t/\sqrt{n})                                                &amp; \text{(definition of $M_{A_i}$)}           \\
               &amp; = M_{A_i}(t/\sqrt{n} )^n.
\end{align*}\]

<p>Let’s analyze each individual term \(M_{A_i}(t / \sqrt{n})\) by performing a Taylor expansion around 0.
Recall that the Taylor expansion of a function \(f(x)\) about a point \(a\) is
given by 
\(f(x)= \sum\limits_{n=0}^\infty \frac{f^{(n)(a)}}{n!}(x-a)^n.\). We will expand up to the
second order term, which requires us to find the first three moments of the MGF.</p>

<p>These are:</p>

\[\begin{align*}
    M_{A_i}(0)                 &amp; = \mathbb{E} \left[ e^{t A_i} \right] \Big|_{t=0}                                                                                                                 \\
                               &amp; = \mathbb{E} \left[ 1 \right]                                                                                                                                     \\
                               &amp; = 1,                                                                                                                                                      \\
    M_{A_i}^\prime(0)          &amp; = \mathbb{E} \left[ A_i \right]                                                       &amp; \text{(by the $k$th moment property proved previously)}                   \\
                               &amp; = 0,                                                                                                                                                      \\
    M_{A_i}^{\prime \prime}(0) &amp; = \mathbb{E} \left[ A_i^2 \right]                                                     &amp; \text{(by the $k$th moment property proved previously)}                   \\
                               &amp; = \mathbb{E} \left[ A_i^2 \right] - \mathbb{E} \left[ A_i \right]^2 + \mathbb{E} \left[ A_i \right]^2                                                                             \\
                               &amp; = \mathbf{Var}(A_i) + \mathbb{E} \left[ A_i \right]^2                                         &amp; \text{($\mathbf{Var}(A_i) = \mathbb{E} \left[ A_i^2 \right] - \mathbb{E} \left[ A_i \right]^2 $)} \\
                               &amp; = 1 + 0 \\
                               &amp; = 1.
\end{align*}\]

<p>Taking all terms up to the second order Taylor expansion allows us to approximate \(M_{A_i}\) as</p>

\[\begin{align*}
    M_{A_i}(t/\sqrt{n}) &amp; \approx M_{A_i}(0) + M_{A_i}^\prime(0) + M_{A_i}^{\prime \prime}(0) \frac{t^2}{2n} \\
                        &amp; = 1 + 0 + \frac{t^2}{2n}                                                           \\
                        &amp; = 1 + \frac{t^2}{2n}.
\end{align*}\]

<p>Then now we can write the limit of the MGF of \(Z_n\) as the following:</p>

\[\begin{align*}
    M_{Z_n}(t) &amp; = M_{A_i}(t/\sqrt{n})^n       \\
               &amp; \approx \left( 1 + \frac{t^2}{2n}  \right)^n \\
               &amp; \to e^{t^2/2}, &amp; \text{(by identity $\lim_{n \to \infty} (1 + x/n)^n \to e^x$)}
\end{align*}\]

<p>which shows that it converges to the MGF of \(Z\), as desired. Hooray!</p>

<h3 id="an-uncomfortable-feeling">An Uncomfortable Feeling</h3>
<p>However, there is one thing in this proof that might have bothered you.
Our result came from making use of the Taylor approximation and taking limits, 
but there is no bound on how large \(n\) must be for the distributions to converge
up to a maximum amount of error. This makes it unsuitable for much theoretical analysis,
since usually we would like to know that \(n\) does not have to be too large
for us to obtain a sufficiently good approximation to the standard normal.</p>

<h3 id="the-useful-form-of-the-central-limit-theorem-the-berry-esseen-theorem">The Useful Form of the Central Limit Theorem: The Berry-Esseen Theorem</h3>

<p>The Berry-Esseen theorem solves this limitation by also providing explicit error bounds. 
This was proved independently by Andrew Berry and Carl-Gustav Esseen in the 40s,
and the statement goes as follows:</p>

<div class="theorem">
    <div class="theorem-title">Theorem
        
        
            (Berry-Esseen)
        
    </div>
    <div class="theorem-contents">
        
    Let \( X_1, \dots, X_n \) be independent random variables.

    Assume \( \mathbb{E} [X_i] = 0 \; \forall i \).

    Write \( \sigma_i^2 = \mathbf{Var} [ X_i] = \mathbb{E}[X_i^2] - \mathbb{E}[X_i]^2 = \mathbb{E}[X_i^2] \).

    Assume \( \sum\limits_{i=1}^n \sigma_i^2 = 1 \). 

    Let \( S = \sum\limits_{i=1}^n X_i \). Then \( \forall u \in \mathbb{R}\),
    
    $$
        \lvert \Pr \left[ S \leq u \right] - \Pr \left[ Z \leq u \right] \rvert
        \leq \mbox{const} \cdot \beta,
    $$

    where the exact constant depends on the proof, with the best known constant
    being \(.5600\) proven by Shevtsova in 2010, and 
    \(\beta = \sum\limits_{i=1}^n \mathbb{E} \left[ \lvert X_i \rvert^3 \right]\).

  
    </div>
</div>

<p>In words, the theorem says that the difference between the CDF of the sum of
the mean-0 random variables and the CDF of the standard normal is bounded by a
value proportionate to the third moment. This then becomes useful as a tool in
proving high probability statements if we can show that the third moment is
inversely polynomially small, i.e \(\beta = 1/\text{poly}(n)\).</p>

<p>Another thing to note is that the theorem only provides an absolute bound for all values of \(u\).
Therefore, when \(u\) is very negative and \(\Pr [Z \leq u ] = \Phi(u)\) is very small, the
relative error is actually very large, and therefore is not as helpful.</p>

<p>I hope this article has been helpful!</p>

<p><em>I would like to express my thanks to my friend <a href="https://adbforlife.github.io/" rel="external nofollow noopener" target="_blank">Albert Gao</a>
for reviewing this article and for providing valuable suggestions</em>.</p>

<h3 id="references">References</h3>
<ul>
  <li>Rosenthal, J. S. (2016). A first look at rigorous probability theory. World Scientific.</li>
  <li>Larry Wasserman, CMU 36-705 Intermediate Statistics Lecture Notes. URL: <a href="https://www.stat.cmu.edu/~larry/=stat705/" rel="external nofollow noopener" target="_blank">https://www.stat.cmu.edu/~larry/=stat705/</a>
</li>
  <li>Ryan O’Donnell, CMU 15-751 A Theorist’s Toolkit. URL: <a href="https://www.youtube.com/watch?v=Ig5TuZauhW4" rel="external nofollow noopener" target="_blank">https://www.youtube.com/watch?v=Ig5TuZauhW4</a>
</li>
</ul>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <!-- <p class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</p> -->
    <p class="mb-2">Related Posts:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/setting-up-yuancon-controller-sound-voltex/">Playing Sound Voltex at Home: Setting Up Unnamed SDVX Clone with the Yuancon SDVX Controller</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/creating-trackback-requests/">Creating Trackback Requests for Static Sites</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/high-dimensional-analysis-of-m-estimators/">A Unified Framework for High-Dimensional Analysis of M-Estimators with Decomposable Regularizers: A Guided Walkthrough</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cmu-steam-tunnels/">The CMU Steam Tunnels and Wean 9</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/advanced-operating-systems-course-review/">CMU 15712 Advanced Operating Systems and Distributed Systems Course Review</a>
  </li>

<div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "fanpu/website",
        "data-repo-id": "R_kgDOIpOodA",
        "data-category": "General",
        "data-category-id": "DIC_kwDOIpOodM4CTKDC",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

          </div>
        </div>
        
      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Fan Pu  Zeng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.
Last updated: July 23, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Enable Tooltips -->
  <script type="text/javascript">
  $(function () {$('[data-toggle="tooltip"]').tooltip()})
  </script>
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>
  <!-- Sidebar Table of Contents -->
  <script defer src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>


  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      loader: {load: ['[tex]/mathtools']},
      tex: {
        packages: {'[+]': ['mathtools', 'ams']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          addMenu: []
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
