<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Notes on 'The Llama 3 Herd of Models' | Fan Pu  Zeng</title>
    <meta name="author" content="Fan Pu  Zeng">
    <meta name="description" content="Notes on the new Llama 3.1 technical report. It's a long paper, but one that's well-written with lots of interesting technical details and  design choices.
">
    <meta name="keywords" content="fanpu, fan pu, fanpu zeng, fan pu zeng, zengfanpu, fanpuzeng, fzeng, CMU, cmu, carnegie mellon, cmu courses, school of computer science, scs, machine learning, computer science, ml, theory, courses, course reviews, CS, Jane Street">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    
    <!-- Sidebar Table of Contents -->
    <link href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css" rel="stylesheet">
    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon_new.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fanpu.io/blog/2024/llama-3.1-technical-report-notes/">
    <!-- Dark Mode -->
    

    <!-- Twitter cards -->
    <meta name="twitter:site" content="@FanPu_Zeng">
    <meta name="twitter:creator" content="@fanpu">
    <meta name="og:title" content="Notes on 'The Llama 3 Herd of Models'">

    
    <meta name="twitter:card" content="summary_large_image">
    
    <meta name="og:image" content="https://fanpu.io/assets/img/posts/furano.webp">

    

    
    <meta name="og:description" content="Notes on the new Llama 3.1 technical report. It's a long paper, but one that's well-written with lots of interesting technical details and  design choices.
">
    

    <!-- end of Twitter cards -->
  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Fan Pu </span>Zeng</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/courses/">CMU Course Reviews</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cmu-online/">CMU Online</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/summaries/">ML Paper Summaries</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">Repositories</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        
        <div class="row">
          <!-- sidebar, which will move to the top on a small screen -->
          <div class="col-sm-3">
            <nav id="toc-sidebar" class="sticky-top"></nav>
          </div>
          <!-- main content area -->
          <div class="col-sm-9">
            <!-- _layouts/post.html -->

<div class="post">

  <div style="display:none">
    $$
    \newcommand{\bone}{\mathbf{1}}
    \newcommand{\bbeta}{\mathbf{\beta}}
    \newcommand{\bdelta}{\mathbf{\delta}}
    \newcommand{\bepsilon}{\mathbf{\epsilon}}
    \newcommand{\blambda}{\mathbf{\lambda}}
    \newcommand{\bomega}{\mathbf{\omega}}
    \newcommand{\bpi}{\mathbf{\pi}}
    \newcommand{\bphi}{\mathbf{\phi}}
    \newcommand{\bvphi}{\mathbf{\varphi}}
    \newcommand{\bpsi}{\mathbf{\psi}}
    \newcommand{\bsigma}{\mathbf{\sigma}}
    \newcommand{\btheta}{\mathbf{\theta}}
    \newcommand{\btau}{\mathbf{\tau}}
    \newcommand{\ba}{\mathbf{a}}
    \newcommand{\bb}{\mathbf{b}}
    \newcommand{\bc}{\mathbf{c}}
    \newcommand{\bd}{\mathbf{d}}
    \newcommand{\be}{\mathbf{e}}
    \newcommand{\boldf}{\mathbf{f}}
    \newcommand{\bg}{\mathbf{g}}
    \newcommand{\bh}{\mathbf{h}}
    \newcommand{\bi}{\mathbf{i}}
    \newcommand{\bj}{\mathbf{j}}
    \newcommand{\bk}{\mathbf{k}}
    \newcommand{\bell}{\mathbf{\ell}}
    \newcommand{\bm}{\mathbf{m}}
    \newcommand{\bn}{\mathbf{n}}
    \newcommand{\bo}{\mathbf{o}}
    \newcommand{\bp}{\mathbf{p}}
    \newcommand{\bq}{\mathbf{q}}
    \newcommand{\br}{\mathbf{r}}
    \newcommand{\bs}{\mathbf{s}}
    \newcommand{\bt}{\mathbf{t}}
    \newcommand{\bu}{\mathbf{u}}
    \newcommand{\bv}{\mathbf{v}}
    \newcommand{\bw}{\mathbf{w}}
    \newcommand{\bx}{\mathbf{x}}
    \newcommand{\by}{\mathbf{y}}
    \newcommand{\bz}{\mathbf{z}}
    \newcommand{\bA}{\mathbf{A}}
    \newcommand{\bB}{\mathbf{B}}
    \newcommand{\bC}{\mathbf{C}}
    \newcommand{\bD}{\mathbf{D}}
    \newcommand{\bE}{\mathbf{E}}
    \newcommand{\bF}{\mathbf{F}}
    \newcommand{\bG}{\mathbf{G}}
    \newcommand{\bH}{\mathbf{H}}
    \newcommand{\bI}{\mathbf{I}}
    \newcommand{\bJ}{\mathbf{J}}
    \newcommand{\bK}{\mathbf{K}}
    \newcommand{\bL}{\mathbf{L}}
    \newcommand{\bM}{\mathbf{M}}
    \newcommand{\bN}{\mathbf{N}}
    \newcommand{\bP}{\mathbf{P}}
    \newcommand{\bQ}{\mathbf{Q}}
    \newcommand{\bR}{\mathbf{R}}
    \newcommand{\bS}{\mathbf{S}}
    \newcommand{\bT}{\mathbf{T}}
    \newcommand{\bU}{\mathbf{U}}
    \newcommand{\bV}{\mathbf{V}}
    \newcommand{\bW}{\mathbf{W}}
    \newcommand{\bX}{\mathbf{X}}
    \newcommand{\bY}{\mathbf{Y}}
    \newcommand{\bZ}{\mathbf{Z}}

    \newcommand{\bsa}{\boldsymbol{a}}
    \newcommand{\bsb}{\boldsymbol{b}}
    \newcommand{\bsc}{\boldsymbol{c}}
    \newcommand{\bsd}{\boldsymbol{d}}
    \newcommand{\bse}{\boldsymbol{e}}
    \newcommand{\bsoldf}{\boldsymbol{f}}
    \newcommand{\bsg}{\boldsymbol{g}}
    \newcommand{\bsh}{\boldsymbol{h}}
    \newcommand{\bsi}{\boldsymbol{i}}
    \newcommand{\bsj}{\boldsymbol{j}}
    \newcommand{\bsk}{\boldsymbol{k}}
    \newcommand{\bsell}{\boldsymbol{\ell}}
    \newcommand{\bsm}{\boldsymbol{m}}
    \newcommand{\bsn}{\boldsymbol{n}}
    \newcommand{\bso}{\boldsymbol{o}}
    \newcommand{\bsp}{\boldsymbol{p}}
    \newcommand{\bsq}{\boldsymbol{q}}
    \newcommand{\bsr}{\boldsymbol{r}}
    \newcommand{\bss}{\boldsymbol{s}}
    \newcommand{\bst}{\boldsymbol{t}}
    \newcommand{\bsu}{\boldsymbol{u}}
    \newcommand{\bsv}{\boldsymbol{v}}
    \newcommand{\bsw}{\boldsymbol{w}}
    \newcommand{\bsx}{\boldsymbol{x}}
    \newcommand{\bsy}{\boldsymbol{y}}
    \newcommand{\bsz}{\boldsymbol{z}}
    \newcommand{\bsA}{\boldsymbol{A}}
    \newcommand{\bsB}{\boldsymbol{B}}
    \newcommand{\bsC}{\boldsymbol{C}}
    \newcommand{\bsD}{\boldsymbol{D}}
    \newcommand{\bsE}{\boldsymbol{E}}
    \newcommand{\bsF}{\boldsymbol{F}}
    \newcommand{\bsG}{\boldsymbol{G}}
    \newcommand{\bsH}{\boldsymbol{H}}
    \newcommand{\bsI}{\boldsymbol{I}}
    \newcommand{\bsJ}{\boldsymbol{J}}
    \newcommand{\bsK}{\boldsymbol{K}}
    \newcommand{\bsL}{\boldsymbol{L}}
    \newcommand{\bsM}{\boldsymbol{M}}
    \newcommand{\bsN}{\boldsymbol{N}}
    \newcommand{\bsP}{\boldsymbol{P}}
    \newcommand{\bsQ}{\boldsymbol{Q}}
    \newcommand{\bsR}{\boldsymbol{R}}
    \newcommand{\bsS}{\boldsymbol{S}}
    \newcommand{\bsT}{\boldsymbol{T}}
    \newcommand{\bsU}{\boldsymbol{U}}
    \newcommand{\bsV}{\boldsymbol{V}}
    \newcommand{\bsW}{\boldsymbol{W}}
    \newcommand{\bsX}{\boldsymbol{X}}
    \newcommand{\bsY}{\boldsymbol{Y}}
    \newcommand{\bsZ}{\boldsymbol{Z}}

    \newcommand{\calA}{\mathcal{A}}
    \newcommand{\calB}{\mathcal{B}}
    \newcommand{\calC}{\mathcal{C}}
    \newcommand{\calD}{\mathcal{D}}
    \newcommand{\calE}{\mathcal{E}}
    \newcommand{\calF}{\mathcal{F}}
    \newcommand{\calG}{\mathcal{G}}
    \newcommand{\calH}{\mathcal{H}}
    \newcommand{\calI}{\mathcal{I}}
    \newcommand{\calJ}{\mathcal{J}}
    \newcommand{\calK}{\mathcal{K}}
    \newcommand{\calL}{\mathcal{L}}
    \newcommand{\calM}{\mathcal{M}}
    \newcommand{\calN}{\mathcal{N}}
    \newcommand{\calO}{\mathcal{O}}
    \newcommand{\calP}{\mathcal{P}}
    \newcommand{\calQ}{\mathcal{Q}}
    \newcommand{\calR}{\mathcal{R}}
    \newcommand{\calS}{\mathcal{S}}
    \newcommand{\calT}{\mathcal{T}}
    \newcommand{\calU}{\mathcal{U}}
    \newcommand{\calV}{\mathcal{V}}
    \newcommand{\calW}{\mathcal{W}}
    \newcommand{\calX}{\mathcal{X}}
    \newcommand{\calY}{\mathcal{Y}}
    \newcommand{\calZ}{\mathcal{Z}}

    \newcommand{\R}{\mathbb{R}}
    \newcommand{\C}{\mathbb{C}}
    \newcommand{\N}{\mathbb{N}}
    \newcommand{\Z}{\mathbb{Z}}
    \newcommand{\F}{\mathbb{F}}
    \newcommand{\Q}{\mathbb{Q}}

    \DeclareMathOperator*{\argmax}{arg\,max}
    \DeclareMathOperator*{\argmin}{arg\,min}
    \newcommand{\nnz}[1]{\mbox{nnz}(#1)}
    \newcommand{\dotprod}[2]{\langle #1, #2 \rangle}

    \newcommand{\ignore}[1]{}

    \let\Pr\relax
    \DeclareMathOperator*{\Pr}{\mathbf{Pr}}
    \newcommand{\E}{\mathbb{E}}
    \DeclareMathOperator*{\Ex}{\mathbf{E}}
    \DeclareMathOperator*{\Var}{\mathbf{Var}}
    \DeclareMathOperator*{\Cov}{\mathbf{Cov}}
    \DeclareMathOperator*{\stddev}{\mathbf{stddev}}
    \DeclareMathOperator*{\avg}{avg}

    \DeclareMathOperator{\poly}{poly}
    \DeclareMathOperator{\polylog}{polylog}
    \DeclareMathOperator{\size}{size}
    \DeclareMathOperator{\sgn}{sgn}
    \DeclareMathOperator{\dist}{dist}
    \DeclareMathOperator{\vol}{vol}
    \DeclareMathOperator{\spn}{span}
    \DeclareMathOperator{\supp}{supp}
    \DeclareMathOperator{\tr}{tr}
    \DeclareMathOperator{\Tr}{Tr}
    \DeclareMathOperator{\codim}{codim}
    \DeclareMathOperator{\diag}{diag}

    \newcommand{\PTIME}{\mathsf{P}}
    \newcommand{\LOGSPACE}{\mathsf{L}}
    \newcommand{\ZPP}{\mathsf{ZPP}}
    \newcommand{\RP}{\mathsf{RP}}
    \newcommand{\BPP}{\mathsf{BPP}}
    \newcommand{\P}{\mathsf{P}}
    \newcommand{\NP}{\mathsf{NP}}
    \newcommand{\TC}{\mathsf{TC}}
    \newcommand{\AC}{\mathsf{AC}}
    \newcommand{\SC}{\mathsf{SC}}
    \newcommand{\SZK}{\mathsf{SZK}}
    \newcommand{\AM}{\mathsf{AM}}
    \newcommand{\IP}{\mathsf{IP}}
    \newcommand{\PSPACE}{\mathsf{PSPACE}}
    \newcommand{\EXP}{\mathsf{EXP}}
    \newcommand{\MIP}{\mathsf{MIP}}
    \newcommand{\NEXP}{\mathsf{NEXP}}
    \newcommand{\BQP}{\mathsf{BQP}}
    \newcommand{\distP}{\mathsf{dist\textbf{P}}}
    \newcommand{\distNP}{\mathsf{dist\textbf{NP}}}

    \newcommand{\eps}{\epsilon}
    \newcommand{\lam}{\lambda}
    \newcommand{\dleta}{\delta}
    \newcommand{\simga}{\sigma}
    \newcommand{\vphi}{\varphi}
    \newcommand{\la}{\langle}
    \newcommand{\ra}{\rangle}
    \newcommand{\wt}[1]{\widetilde{#1}}
    \newcommand{\wh}[1]{\widehat{#1}}
    \newcommand{\ol}[1]{\overline{#1}}
    \newcommand{\ul}[1]{\underline{#1}}
    \newcommand{\ot}{\otimes}
    \newcommand{\zo}{\{0,1\}}
    \newcommand{\co}{:} %\newcommand{\co}{\colon}
    \newcommand{\bdry}{\partial}
    \newcommand{\grad}{\nabla}
    \newcommand{\transp}{^\intercal}
    \newcommand{\inv}{^{-1}}
    \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff}
    \newcommand{\half}{\tfrac{1}{2}}
    \newcommand{\bbone}{\mathbbm 1}
    \newcommand{\Id}{\bbone}

    \newcommand{\SAT}{\mathsf{SAT}}

    \newcommand{\bcalG}{\boldsymbol{\calG}}
    \newcommand{\calbG}{\bcalG}
    \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX}
    \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY}
    \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ}
    $$
</div>

  <header class="post-header">
    <h1 class="post-title">Notes on 'The Llama 3 Herd of Models'</h1>
    <p class="post-meta">August 7, 2024• fanpu</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
        ·  
        <a href="/blog/tag/machine-learning">
          <i class="fas fa-hashtag fa-sm"></i> machine-learning</a>  
          

    </p>
  </header>

  
      <figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/furano.webp" class="preview z-depth-1 rounded center" width="100%" height="450px" alt="post.cover" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>
</figure>

    <div class="caption">
        Lavender Fields in Biei, Kamikawa Subprefecture, Hokkaido, Japan
    </div>
  

  <article class="post-content">
    
    <div id="markdown-content">
      <h1 id="reading-recommendations">Reading Recommendations</h1>

<p>This is a long paper, but it’s full of gems. Here’s a reading recommendation guide:</p>

<ul>
  <li>Strapped on time: sections 1 (Introduction), 2 (General Overview). It’s just a couple of pages and provides a good overview.</li>
  <li>Love ML systems: 3.3 (Infrastructure, Scaling, Efficiency). Talks about on hardware, architecture, training challenges, parallelism optimizations</li>
  <li>How to train a coding model: 4.3.1 (Code). Covers how they targeted specific coding abilities and generated synthetic datasets to bootstrap the model</li>
  <li>Training model to perform tool use: 4.3.5 (Tool Use)</li>
  <li>Post-training framework: 4.1 (Modeling). Covers their pipeline for reward modeling, supervised finetuning, and direct preference optimization</li>
  <li>Extending to 128K context: 3.4.2 (Long Context Pre-Training) and 4.3.4 (Long Context)</li>
  <li>Why a 405B model: 3.2.1 (Scaling Laws)</li>
  <li>Optimizations for inference: 6 (Inference) on both pipeline parallelism and FP8 quantization, this is a short section</li>
  <li>Results and benchmarks: 5.1, 5.2 (Pre and Post-trained Language Model), 5.3 (Human Evaluations)</li>
  <li>Red teaming: 5.4.6 (Red Teaming)</li>
  <li>Multi-modality: 7 (Vision Experiments), 8 (Speech Experiments), 9.2 (Multimodality)</li>
</ul>

<h1 id="introduction">Introduction</h1>

<p>Introduces new set of models (8/70/405 B) that supports:</p>

<ul>
  <li>multilinguality</li>
  <li>coding</li>
  <li>reasoning</li>
  <li>tool usage</li>
</ul>

<p>Largest model:</p>

<ul>
  <li>405B parameters</li>
  <li>128k context window</li>
  <li>Has instruction fine-tuned version</li>
  <li>Pre-trained on 3.8 x \(10^{25}\) FLOPS</li>
</ul>

<p>Also introduced Llama Guard 3 model for input/output safety.</p>

<h1 id="pre-training">Pre-training</h1>

<h2 id="pre-training-data">Pre-Training Data</h2>

<h3 id="data-cleaning">Data Cleaning</h3>

<p>Knowledge cutoff end of 2023. To ensure high-quality tokens, performed:
de-duplication, data cleaning, removed domains known to contain large amounts of
PII, adult content.</p>

<p>Data cleaning:</p>

<ul>
  <li>extracts HTML content from web documents</li>
  <li>done carefully
for pages with math &amp; code content to preserve structure</li>
  <li>Markdown markers also removed</li>
</ul>

<p>De-duplication:</p>

<ul>
  <li>on the URL, duplication across documents, line-level de-duplication (common in boilerplate)</li>
</ul>

<p>Used heuristics to filter other low-quality documents: logs/error messages,
other adult websites, websites with excessive numbers of outlier tokens</p>

<p>Built a model-based classifier to sub-select high-quality tokens.</p>

<p>Built domain-specific pipelines to extract code &amp; math-relevant web pages,
including pages containing math deduction, pages containing code interleaved
with natural language.</p>

<p>Used similar approaches as the above for other languages.</p>

<h3 id="data-mix">Data Mix</h3>

<p>This ensures they have the right proportion of different data sources.
They ended up with:</p>

<ul>
  <li>50% general knowledge</li>
  <li>25% math &amp; reasoning</li>
  <li>17% code</li>
  <li>8% multilingual</li>
</ul>

<p>Knowledge classification: categorizes data to determine the data mix.
Used this to downsample data over-represented on the web like arts &amp; entertainment.</p>

<p>Scaling laws for data mix: trained several small models on data mix &amp; use that to predict the performance of large models on mix</p>

<p>Overview</p>

<ul>
  <li>15.6T multilingual tokens (compare 1.8T for Llama 2)</li>
  <li>Use 8K token context window initially, followed by continued pre-training
stage which increases supported context window to 128K tokens</li>
</ul>

<h3 id="multi-modality">Multi-modality</h3>

<h4 id="encoders">Encoders</h4>

<p>Separate encoders trained for images and speech.</p>

<p>Image encoder:</p>

<ul>
  <li>Trained on image-text pairs</li>
</ul>

<p>Speech encoder:</p>

<ul>
  <li>Self-supervised learning via masking</li>
  <li>Masked part reconstructed by discrete-token representation</li>
</ul>

<h4 id="adapters">Adapters</h4>

<p>TBD</p>

<h3 id="annealing-data">Annealing Data</h3>

<p>Performed annealing on small amounts of high-quality code and mathematical data.
Annealing here means increasingly upsampling these high-quality data over time.</p>

<p>Found improvements for Llama 3 8B on GSM8k and MATH, but not 405B.</p>

<h2 id="model-architecture">Model Architecture</h2>

<p>Architecture</p>

<ul>
  <li>Uses dense Transformer architecture instead of MoE for training stability</li>
  <li>Similar to Llama and Llama 2, performance gains mostly
from improvements in data quality &amp; diversity, and training scale</li>
  <li>Grouped query attention with 8 KV heads: improves inference speed, reduce size of KV cache during decoding</li>
  <li>Attention mask to prevent self-attention between different
documents (why not just put them in different sequences? maybe to take advantage of parallelism?). Limited impact during pre-training,
helpful for continued pre-training on long sequences</li>
  <li>RoPE for positional embeddings (500,000 base frequency hyperparameter instead of 10k in original paper, this is helpful for longer context), SwiGLU activation</li>
  <li>128K token vocabulary, based off <code class="language-plaintext highlighter-rouge">tiktoken</code> tokenizer and extra 28K non-English tokens. Tokenizer improves compression rate from 3.17 to 3.94 characters per token compared to Llama 2 tokenizer.</li>
  <li>Llama 3 405B: 126 layers (!!), model dimension 16,382, 128 attention heads</li>
</ul>

<h3 id="scaling-laws">Scaling Laws</h3>

<p>Scaling laws are nice for predicting loss, but not helpful
for understanding impact on downstream task performance.</p>

<p>To find relationship with downstream task performance they did:</p>

<ol>
  <li>Find correlation between compute-optimal model’s loss on downstream tasks and training FLOPs</li>
  <li>Find correlation between loss and downstream task accuracy,
using scaling law models</li>
</ol>

<p>The scaling laws suggest that given their compute
budget of \(3.8 \times 10^{25}\) FLOPs, a 402B
model with 16.55T tokens is optimal, which led to their 405B model.</p>

<p>They also found their predictions to be quite accurate
for the final downstream performance of their models.</p>

<h3 id="infrastructure-scaling-and-efficiency">Infrastructure, Scaling, and Efficiency</h3>

<p>Compute:</p>

<ul>
  <li>16K H100 GPUs, 700W TDP (thermal design power) with 80GB HBM3 (high bandwidth memory that allows for faster data transfer between CPU and GPU)</li>
  <li>Trained on Meta’s Grand Teton AI server platform, scheduling using MAST (Meta’s global-scale training scheduler)</li>
  <li>Each server: 8 GPUs connected by NVLink, 2 CPUs</li>
</ul>

<p>Storage:</p>

<ul>
  <li>Tectonic, Meta’s distributed file system</li>
  <li>240 PB storage over 7500 servers, 2TB/s sustainable throughput, 7TB/s peak throughput</li>
</ul>

<p>Network:</p>

<ul>
  <li>Llama 3 405B used RDMA over Converged Ethernet (RoCE) fabric</li>
  <li>Smaller models uses Nvidia Quantum2 Infiniband</li>
  <li>Both 400 Gbps interconnect</li>
</ul>

<h3 id="parallelism-for-model-scaling">Parallelism for Model Scaling</h3>

<p>Scaled parallelism as much as possible, so all of GPU’s model parameters,
optimizer states, gradients, and activations fit in HBM.</p>

<p>4D parallelism:</p>

<ul>
  <li>tensor parallelism</li>
  <li>pipeline parallelism</li>
  <li>context parallelism</li>
  <li>data parallelism</li>
</ul>

<p>Parallelism achieved BF16 Model FLOPs Utilization (MFU)
of 38-43%</p>

<h3 id="reliability-and-operational-challenges">Reliability and Operational Challenges</h3>

<ul>
  <li>
    <blockquote>
      <p>90% effective training time, even while supporting automated cluster maintenance (i.e Linux kernel upgrades)</p>
    </blockquote>
  </li>
  <li>At least one training interruption daily</li>
</ul>

<p>466 job interruptions</p>

<ul>
  <li>47 planned interruptions (i.e maintenance)</li>
  <li>419 unexpected: mostly GPU/host component failures,
suspected data corruption, unplanned maintenance</li>
  <li>Significant manual intervention only required 3 times, rest handled
by automation</li>
</ul>

<p>Debugging</p>

<ul>
  <li>PyTorch’s built-in NCCL flight recorder helped diagnose issues quickly at scale</li>
  <li>Mixed use of NVLink and RoCE complicated things</li>
</ul>

<p>Others</p>

<ul>
  <li>Higher mid-day temperatures impacted GPU dynamic voltage and frequency
scaling, causing diurnal 1-2% throughput variation throughout the day</li>
  <li>~10ks of GPUs with correlated increase/decrease in power consumption (i.e waiting for checkpointing) causes fluctuation
of power consumption on the order of ~10s megawatts, stretching limits of power grid</li>
</ul>

<h2 id="training-recipe">Training Recipe</h2>

<p>Initial pre-training:</p>

<ul>
  <li>AdamW optimizer</li>
  <li>Linear warm up, cosine LR schedule</li>
  <li>Start with lower batch size for training stability, increase
subsequently for efficiency</li>
  <li>Few loss spikes, no interventions needed to correct for training divergence</li>
  <li>Upsampled non-English and math data, downsampled low-quality data</li>
  <li>Added recent web data in final stages of pre-training
to advance model knowledge cut-off</li>
</ul>

<p>Long context pre-training:</p>

<ul>
  <li>To support 128K context window</li>
  <li>Don’t do long-context training earlier because of quadratic self-attention, too expensive</li>
  <li>Increased context length by successive adaptation over 6 stages from 8K to 128K, 800B training tokens</li>
</ul>

<p>Annealing:</p>

<ul>
  <li>On final 40M tokens, linearly annealed LR to 0, kept 128K context window</li>
  <li>Upsampled data source of very high quality</li>
  <li>Averaged model checkpoints during annealing to get final pre-trained model</li>
</ul>

<h1 id="post-training">Post-Training</h1>

<ul>
  <li>Several rounds of post-training, each starts with SFT followed by DPO</li>
  <li>Examples collected by human annotations or generated synthetically</li>
  <li>Custom</li>
</ul>

<h2 id="modeling">Modeling</h2>

<ul>
  <li>Uses reward model (RM) and language model (LM)</li>
  <li>RM trained by human-annotated preference data</li>
  <li>
    <p>Checkpoints aligned with DPO</p>
  </li>
  <li>Model supports tool use, which required designing multi-message chat protocol with special header and termination tokens</li>
</ul>

<h3 id="reward-modeling">Reward Modeling</h3>

<ul>
  <li>RM trained on top of pre-trained checkpoint</li>
  <li>Preference pairs of either (chosen, rejected) or (chosen, rejected, edited), where edited &gt; chosen &gt; rejected</li>
  <li>Filtered out preference data with similar responses</li>
</ul>

<h3 id="supervised-finetuning">Supervised Finetuning</h3>

<ul>
  <li>RM performs rejection sampling on human annotation prompts</li>
  <li>Fine-tune pre-trained LM on the model-generated samples that are accepted</li>
</ul>

<h3 id="direct-preference-optimization">Direct Preference Optimization</h3>

<ul>
  <li>Why not on-policy algorithms like PPO? DPO required less compute, performed better on instruction-following benchmarks</li>
  <li>Used most recent batches of preference data from best-performing
models during previous alignment rounds, ensures training data
conforms better to distribution of policy model being optimized</li>
</ul>

<p>Modified DPO:</p>

<ul>
  <li>Masked out formatting tokens (including header and termination tokens) in DPO loss, helps with stability. These tokens caused tail repetition or random termination tokens. Hypothesized due to these tokens being common in both chosen and rejected responses causes conflicting optimization objectives</li>
  <li>Added regularization with negative log-likelihood (NLL) loss</li>
</ul>

<h2 id="post-training-data">Post-training Data</h2>

<h3 id="preference-data">Preference Data</h3>

<ul>
  <li>Sample two responses from two different models for each user prompt, labelled by human annotators</li>
  <li>Annotators state strength of preference by 4 levels: significantly better, better, slightly better, marginally better</li>
  <li>Allow editing step after annotation to further improve response</li>
  <li>Only used responses significantly better or better for training</li>
</ul>

<h3 id="sft-data">SFT Data</h3>

<p>Finetuning data contains:</p>

<ul>
  <li>Prompts from human annotation collection with rejection-sampled (RS) responses</li>
  <li>Synthetic data targeting specific capabilities</li>
  <li>Small amounts of human-curated data</li>
</ul>

<p>Datasets:</p>

<ul>
  <li>General English</li>
  <li>Code</li>
  <li>Multilingual</li>
  <li>Exam-like</li>
  <li>Reasoning and tools</li>
  <li>Long context</li>
</ul>

<p>Rejection sampling:</p>

<ul>
  <li>Choose prompt from human annotation collection</li>
  <li>Sample 10-30 outputs from latest chat model policy</li>
  <li>Use RM to choose best candidate</li>
  <li>For later rounds of post-training, use system prompt to steer RS responses to conform with tone/style/formatting</li>
  <li>Uses PagedAttention to make RS efficient</li>
</ul>

<h3 id="data-processing-and-quality-control">Data Processing and Quality Control</h3>

<p>Most of training data is model-generated, requires careful cleaning and quality control</p>

<p>Data cleaning:</p>

<ul>
  <li>Rule-based data removal or modification strategies</li>
  <li>Balance proportion of such samples in dataset</li>
</ul>

<p>Data pruning:</p>

<ul>
  <li>Topic classification: Fine-tuned Llama 3 8B to a topic classifier</li>
  <li>Quality scoring: Use RM and Llama 3 checkpoint to rate content, keep examples marked as high quality by either RM or Llama. Both signals have high disagreement rates, and combining signals gives best recall on test set.</li>
  <li>Difficulty scoring: used Llama 3 70B to perform intention-tagging, where more intentions implies more complexity. Also used it to measure difficulty of dialogs</li>
  <li>Semantic deduplication: clustering using RoBERTa, sort by
quality score \(\times\) difficulty score, go through sorted examples by best and take only ones with maximum cosine similarity less than threshold</li>
</ul>

<h2 id="capabilities">Capabilities</h2>

<h3 id="code">Code</h3>

<p>Capabilities:</p>

<ul>
  <li>Code generation</li>
  <li>Documentation</li>
  <li>Debugging</li>
  <li>Review</li>
</ul>

<p>Targeted languages: Python, Java, Javascript, C/C++, Typescript, Rust, PHP, HTML/CSS, SQL, bash/shell</p>

<p>Improved capabilities via:</p>

<ul>
  <li>Training a code expert</li>
  <li>Generate synthetic data for SFT</li>
  <li>Improve formatting with system prompt steering</li>
  <li>Create quality filters to remove bad examples</li>
</ul>

<p>Expert training:</p>

<ul>
  <li>Train code expert to obtain high quality human annotations for code</li>
  <li>Approach similar to CodeLlama (scant on details, should probably check this paper)</li>
</ul>

<p>Synthetic data generation:</p>

<ul>
  <li>Faced issues in code generation: following instructions, code syntax errors, incorrect code generation, difficulty in fixing bugs</li>
  <li>Use Llama 3 and code expert to generate synthetic 2.7M dialogs for SFT</li>
</ul>

<p>During RS, used code specific system prompts to improve:</p>

<ul>
  <li>code readability</li>
  <li>documentation</li>
  <li>thoroughness</li>
  <li>specificity</li>
</ul>

<h4 id="synthetic-data-generation-execution-feedback">Synthetic data generation: execution feedback</h4>

<ul>
  <li>Distillation to smaller models helped, but not for 405B on its
own inputs</li>
  <li>Use execution feedback as source of truth, allow model to learn from own mistakes
    <ol>
      <li>Problem description generation: generate programming problem descriptions, use random code snippets as inspiration</li>
      <li>Solution generation: Prompt Llama 3 to solve problem, use CoT in comments, add programming guidelines in system prompt</li>
      <li>Correctness analysis: use static analysis (parser and linters),
and unit test generation (also by the model) and execution</li>
      <li>Error feedback and iterative self-correction: prompt model to revise solutions that fail, includes feedback from parser/linter/tester. Can modify code and unit test to accomodate new code. 20% of solutions that were incorrect could be self-corrected this way.</li>
      <li>Fine-tuning and iterative improvement: process iterated over multiple rounds, higher-quality synthetic data generated in each subsequent rounds</li>
    </ol>
  </li>
</ul>

<h4 id="synthetic-data-generation-programming-language-translation">Synthetic data generation: programming language translation</h4>

<ul>
  <li>Noted performance gap between popular vs less common programming languages, due to difference in dataset size</li>
  <li>Translate data from more common to less common languages</li>
  <li>Ensure quality via syntax parsing, compilation, execution</li>
</ul>

<h4 id="synthetic-data-generation-backtranslation">Synthetic data generation: backtranslation</h4>

<ul>
  <li>Some coding capabilities don’t benefit as much from execution feedback, i.e documentation &amp; explanation</li>
  <li>Generated 1.2M synthetic dialogs for code explanation, generation, documentation, debugging</li>
  <li>Done as follows:
    <ol>
      <li>Generate: Prompt Llama 3 to generate data corresponding to desired capability (i.e add comments to code)</li>
      <li>Backtranslate: Ask model to backtranslate synthetically generated data to original code (i.e generate code based on only comments)</li>
      <li>Filter: ask Llama 3 to determine quality of generated code with original code as reference. This self-verification step acts as a filter for good examples, only those with high scores are used for SFT</li>
    </ol>
  </li>
</ul>

<h3 id="tool-use">Tool Use</h3>

<p>Trained Llama 3 to use search engine (Brave), Python interpreter, Wolfram Alpha
API.</p>

<p>To train on tool use:</p>

<ul>
  <li>Start with training single-turn tool use, then tool use in dialog, and then multi-step tool use and data analysis</li>
  <li>All synthetically generated: first synthetic user prompts which require
calling out to tools, then the corresponding tool calls which are then executed,
and then the final answer to user prompt</li>
  <li>Multi-step tool use trained in a similar way synthetically</li>
  <li>User prompts are based on a provided file, and ask to summarize the contents of
the file, find and fix bugs, optimize a piece of code, perform data analysis or
visualization</li>
  <li>Augmented synthetic data with different system prompts to teach model to use tools
only when activated</li>
  <li>To avoid model using tools for simple queries, added dataset containing
queries of simple math/reasoning questions with tool use activated but without
using tools in response</li>
</ul>

<h3 id="factuality">Factuality</h3>

<p>To train the model to guard against hallucinations, they used a knowledge probe to find out what the model knows, and to generate training data of refusals for the things it doesn’t:</p>

<ol>
  <li>Extract a data snippet from the pre-training data.</li>
  <li>Generate a factual question about these snippets (context) by prompting Llama 3.</li>
  <li>Sample responses from Llama 3 to the question.</li>
  <li>Score the correctness of the generations using the original context as a reference and Llama 3 as a judge.</li>
  <li>Score the informativeness of the generations using Llama 3 as a judge.</li>
  <li>Generate a refusal for responses which are consistently informative and incorrect across the generations,
using Llama 3.</li>
</ol>

<p>But because pre-training data is not always factually correct, they also did this for
sensitive topics where contradictory/incorrect statements are prevalent</p>

<h3 id="steerability">Steerability</h3>

<p>Remainder to be continued…</p>

<!--
- Got human annotators to come up with different system prompts,

-->

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <!-- <p class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</p> -->
    <p class="mb-2">Related Posts:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/setting-up-yuancon-controller-sound-voltex/">Playing Sound Voltex at Home: Setting Up Unnamed SDVX Clone with the Yuancon SDVX Controller</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/creating-trackback-requests/">Creating Trackback Requests for Static Sites</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/high-dimensional-analysis-of-m-estimators/">A Unified Framework for High-Dimensional Analysis of M-Estimators with Decomposable Regularizers: A Guided Walkthrough</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cmu-steam-tunnels/">The CMU Steam Tunnels and Wean 9</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/advanced-operating-systems-course-review/">CMU 15712 Advanced Operating Systems and Distributed Systems Course Review</a>
  </li>

<div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "fanpu/website",
        "data-repo-id": "R_kgDOIpOodA",
        "data-category": "General",
        "data-category-id": "DIC_kwDOIpOodM4CTKDC",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

          </div>
        </div>
        
      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Fan Pu  Zeng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.
Last updated: October 04, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Enable Tooltips -->
  <script type="text/javascript">
  $(function () {$('[data-toggle="tooltip"]').tooltip()})
  </script>
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>
  <!-- Sidebar Table of Contents -->
  <script defer src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>


  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      loader: {load: ['[tex]/mathtools']},
      tex: {
        packages: {'[+]': ['mathtools', 'ams']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          addMenu: []
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
