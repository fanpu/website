<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://fanpu.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fanpu.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-01T03:13:49+00:00</updated><id>https://fanpu.io/feed.xml</id><title type="html">blank</title><subtitle>Homepage </subtitle><entry><title type="html">Notes on ‚ÄòThe Llama 3 Herd of Models‚Äô</title><link href="https://fanpu.io/blog/2024/llama-3.1-technical-report-notes/" rel="alternate" type="text/html" title="Notes on ‚ÄòThe Llama 3 Herd of Models‚Äô"/><published>2024-08-07T00:00:00+00:00</published><updated>2024-08-07T00:00:00+00:00</updated><id>https://fanpu.io/blog/2024/llama-3.1-technical-report-notes</id><content type="html" xml:base="https://fanpu.io/blog/2024/llama-3.1-technical-report-notes/"><![CDATA[<h1 id="reading-recommendations">Reading Recommendations</h1> <p>This is a long paper, but it‚Äôs full of gems. Here‚Äôs a reading recommendation guide:</p> <ul> <li>Strapped on time: sections 1 (Introduction), 2 (General Overview). It‚Äôs just a couple of pages and provides a good overview.</li> <li>Love ML systems: 3.3 (Infrastructure, Scaling, Efficiency). Talks about on hardware, architecture, training challenges, parallelism optimizations</li> <li>How to train a coding model: 4.3.1 (Code). Covers how they targeted specific coding abilities and generated synthetic datasets to bootstrap the model</li> <li>Training model to perform tool use: 4.3.5 (Tool Use)</li> <li>Post-training framework: 4.1 (Modeling). Covers their pipeline for reward modeling, supervised finetuning, and direct preference optimization</li> <li>Extending to 128K context: 3.4.2 (Long Context Pre-Training) and 4.3.4 (Long Context)</li> <li>Why a 405B model: 3.2.1 (Scaling Laws)</li> <li>Optimizations for inference: 6 (Inference) on both pipeline parallelism and FP8 quantization, this is a short section</li> <li>Results and benchmarks: 5.1, 5.2 (Pre and Post-trained Language Model), 5.3 (Human Evaluations)</li> <li>Red teaming: 5.4.6 (Red Teaming)</li> <li>Multi-modality: 7 (Vision Experiments), 8 (Speech Experiments), 9.2 (Multimodality)</li> </ul> <h1 id="introduction">Introduction</h1> <p>Introduces new set of models (8/70/405 B) that supports:</p> <ul> <li>multilinguality</li> <li>coding</li> <li>reasoning</li> <li>tool usage</li> </ul> <p>Largest model:</p> <ul> <li>405B parameters</li> <li>128k context window</li> <li>Has instruction fine-tuned version</li> <li>Pre-trained on 3.8 x \(10^{25}\) FLOPS</li> </ul> <p>Also introduced Llama Guard 3 model for input/output safety.</p> <h1 id="pre-training">Pre-training</h1> <h2 id="pre-training-data">Pre-Training Data</h2> <h3 id="data-cleaning">Data Cleaning</h3> <p>Knowledge cutoff end of 2023. To ensure high-quality tokens, performed: de-duplication, data cleaning, removed domains known to contain large amounts of PII, adult content.</p> <p>Data cleaning:</p> <ul> <li>extracts HTML content from web documents</li> <li>done carefully for pages with math &amp; code content to preserve structure</li> <li>Markdown markers also removed</li> </ul> <p>De-duplication:</p> <ul> <li>on the URL, duplication across documents, line-level de-duplication (common in boilerplate)</li> </ul> <p>Used heuristics to filter other low-quality documents: logs/error messages, other adult websites, websites with excessive numbers of outlier tokens</p> <p>Built a model-based classifier to sub-select high-quality tokens.</p> <p>Built domain-specific pipelines to extract code &amp; math-relevant web pages, including pages containing math deduction, pages containing code interleaved with natural language.</p> <p>Used similar approaches as the above for other languages.</p> <h3 id="data-mix">Data Mix</h3> <p>This ensures they have the right proportion of different data sources. They ended up with:</p> <ul> <li>50% general knowledge</li> <li>25% math &amp; reasoning</li> <li>17% code</li> <li>8% multilingual</li> </ul> <p>Knowledge classification: categorizes data to determine the data mix. Used this to downsample data over-represented on the web like arts &amp; entertainment.</p> <p>Scaling laws for data mix: trained several small models on data mix &amp; use that to predict the performance of large models on mix</p> <p>Overview</p> <ul> <li>15.6T multilingual tokens (compare 1.8T for Llama 2)</li> <li>Use 8K token context window initially, followed by continued pre-training stage which increases supported context window to 128K tokens</li> </ul> <h3 id="multi-modality">Multi-modality</h3> <h4 id="encoders">Encoders</h4> <p>Separate encoders trained for images and speech.</p> <p>Image encoder:</p> <ul> <li>Trained on image-text pairs</li> </ul> <p>Speech encoder:</p> <ul> <li>Self-supervised learning via masking</li> <li>Masked part reconstructed by discrete-token representation</li> </ul> <h4 id="adapters">Adapters</h4> <p>TBD</p> <h3 id="annealing-data">Annealing Data</h3> <p>Performed annealing on small amounts of high-quality code and mathematical data. Annealing here means increasingly upsampling these high-quality data over time.</p> <p>Found improvements for Llama 3 8B on GSM8k and MATH, but not 405B.</p> <h2 id="model-architecture">Model Architecture</h2> <p>Architecture</p> <ul> <li>Uses dense Transformer architecture instead of MoE for training stability</li> <li>Similar to Llama and Llama 2, performance gains mostly from improvements in data quality &amp; diversity, and training scale</li> <li>Grouped query attention with 8 KV heads: improves inference speed, reduce size of KV cache during decoding</li> <li>Attention mask to prevent self-attention between different documents (why not just put them in different sequences? maybe to take advantage of parallelism?). Limited impact during pre-training, helpful for continued pre-training on long sequences</li> <li>RoPE for positional embeddings (500,000 base frequency hyperparameter instead of 10k in original paper, this is helpful for longer context), SwiGLU activation</li> <li>128K token vocabulary, based off <code class="language-plaintext highlighter-rouge">tiktoken</code> tokenizer and extra 28K non-English tokens. Tokenizer improves compression rate from 3.17 to 3.94 characters per token compared to Llama 2 tokenizer.</li> <li>Llama 3 405B: 126 layers (!!), model dimension 16,382, 128 attention heads</li> </ul> <h3 id="scaling-laws">Scaling Laws</h3> <p>Scaling laws are nice for predicting loss, but not helpful for understanding impact on downstream task performance.</p> <p>To find relationship with downstream task performance they did:</p> <ol> <li>Find correlation between compute-optimal model‚Äôs loss on downstream tasks and training FLOPs</li> <li>Find correlation between loss and downstream task accuracy, using scaling law models</li> </ol> <p>The scaling laws suggest that given their compute budget of \(3.8 \times 10^{25}\) FLOPs, a 402B model with 16.55T tokens is optimal, which led to their 405B model.</p> <p>They also found their predictions to be quite accurate for the final downstream performance of their models.</p> <h3 id="infrastructure-scaling-and-efficiency">Infrastructure, Scaling, and Efficiency</h3> <p>Compute:</p> <ul> <li>16K H100 GPUs, 700W TDP (thermal design power) with 80GB HBM3 (high bandwidth memory that allows for faster data transfer between CPU and GPU)</li> <li>Trained on Meta‚Äôs Grand Teton AI server platform, scheduling using MAST (Meta‚Äôs global-scale training scheduler)</li> <li>Each server: 8 GPUs connected by NVLink, 2 CPUs</li> </ul> <p>Storage:</p> <ul> <li>Tectonic, Meta‚Äôs distributed file system</li> <li>240 PB storage over 7500 servers, 2TB/s sustainable throughput, 7TB/s peak throughput</li> </ul> <p>Network:</p> <ul> <li>Llama 3 405B used RDMA over Converged Ethernet (RoCE) fabric</li> <li>Smaller models uses Nvidia Quantum2 Infiniband</li> <li>Both 400 Gbps interconnect</li> </ul> <h3 id="parallelism-for-model-scaling">Parallelism for Model Scaling</h3> <p>Scaled parallelism as much as possible, so all of GPU‚Äôs model parameters, optimizer states, gradients, and activations fit in HBM.</p> <p>4D parallelism:</p> <ul> <li>tensor parallelism</li> <li>pipeline parallelism</li> <li>context parallelism</li> <li>data parallelism</li> </ul> <p>Parallelism achieved BF16 Model FLOPs Utilization (MFU) of 38-43%</p> <h3 id="reliability-and-operational-challenges">Reliability and Operational Challenges</h3> <ul> <li> <blockquote> <p>90% effective training time, even while supporting automated cluster maintenance (i.e Linux kernel upgrades)</p> </blockquote> </li> <li>At least one training interruption daily</li> </ul> <p>466 job interruptions</p> <ul> <li>47 planned interruptions (i.e maintenance)</li> <li>419 unexpected: mostly GPU/host component failures, suspected data corruption, unplanned maintenance</li> <li>Significant manual intervention only required 3 times, rest handled by automation</li> </ul> <p>Debugging</p> <ul> <li>PyTorch‚Äôs built-in NCCL flight recorder helped diagnose issues quickly at scale</li> <li>Mixed use of NVLink and RoCE complicated things</li> </ul> <p>Others</p> <ul> <li>Higher mid-day temperatures impacted GPU dynamic voltage and frequency scaling, causing diurnal 1-2% throughput variation throughout the day</li> <li>~10ks of GPUs with correlated increase/decrease in power consumption (i.e waiting for checkpointing) causes fluctuation of power consumption on the order of ~10s megawatts, stretching limits of power grid</li> </ul> <h2 id="training-recipe">Training Recipe</h2> <p>Initial pre-training:</p> <ul> <li>AdamW optimizer</li> <li>Linear warm up, cosine LR schedule</li> <li>Start with lower batch size for training stability, increase subsequently for efficiency</li> <li>Few loss spikes, no interventions needed to correct for training divergence</li> <li>Upsampled non-English and math data, downsampled low-quality data</li> <li>Added recent web data in final stages of pre-training to advance model knowledge cut-off</li> </ul> <p>Long context pre-training:</p> <ul> <li>To support 128K context window</li> <li>Don‚Äôt do long-context training earlier because of quadratic self-attention, too expensive</li> <li>Increased context length by successive adaptation over 6 stages from 8K to 128K, 800B training tokens</li> </ul> <p>Annealing:</p> <ul> <li>On final 40M tokens, linearly annealed LR to 0, kept 128K context window</li> <li>Upsampled data source of very high quality</li> <li>Averaged model checkpoints during annealing to get final pre-trained model</li> </ul> <h1 id="post-training">Post-Training</h1> <ul> <li>Several rounds of post-training, each starts with SFT followed by DPO</li> <li>Examples collected by human annotations or generated synthetically</li> <li>Custom</li> </ul> <h2 id="modeling">Modeling</h2> <ul> <li>Uses reward model (RM) and language model (LM)</li> <li>RM trained by human-annotated preference data</li> <li> <p>Checkpoints aligned with DPO</p> </li> <li>Model supports tool use, which required designing multi-message chat protocol with special header and termination tokens</li> </ul> <h3 id="reward-modeling">Reward Modeling</h3> <ul> <li>RM trained on top of pre-trained checkpoint</li> <li>Preference pairs of either (chosen, rejected) or (chosen, rejected, edited), where edited &gt; chosen &gt; rejected</li> <li>Filtered out preference data with similar responses</li> </ul> <h3 id="supervised-finetuning">Supervised Finetuning</h3> <ul> <li>RM performs rejection sampling on human annotation prompts</li> <li>Fine-tune pre-trained LM on the model-generated samples that are accepted</li> </ul> <h3 id="direct-preference-optimization">Direct Preference Optimization</h3> <ul> <li>Why not on-policy algorithms like PPO? DPO required less compute, performed better on instruction-following benchmarks</li> <li>Used most recent batches of preference data from best-performing models during previous alignment rounds, ensures training data conforms better to distribution of policy model being optimized</li> </ul> <p>Modified DPO:</p> <ul> <li>Masked out formatting tokens (including header and termination tokens) in DPO loss, helps with stability. These tokens caused tail repetition or random termination tokens. Hypothesized due to these tokens being common in both chosen and rejected responses causes conflicting optimization objectives</li> <li>Added regularization with negative log-likelihood (NLL) loss</li> </ul> <h2 id="post-training-data">Post-training Data</h2> <h3 id="preference-data">Preference Data</h3> <ul> <li>Sample two responses from two different models for each user prompt, labelled by human annotators</li> <li>Annotators state strength of preference by 4 levels: significantly better, better, slightly better, marginally better</li> <li>Allow editing step after annotation to further improve response</li> <li>Only used responses significantly better or better for training</li> </ul> <h3 id="sft-data">SFT Data</h3> <p>Finetuning data contains:</p> <ul> <li>Prompts from human annotation collection with rejection-sampled (RS) responses</li> <li>Synthetic data targeting specific capabilities</li> <li>Small amounts of human-curated data</li> </ul> <p>Datasets:</p> <ul> <li>General English</li> <li>Code</li> <li>Multilingual</li> <li>Exam-like</li> <li>Reasoning and tools</li> <li>Long context</li> </ul> <p>Rejection sampling:</p> <ul> <li>Choose prompt from human annotation collection</li> <li>Sample 10-30 outputs from latest chat model policy</li> <li>Use RM to choose best candidate</li> <li>For later rounds of post-training, use system prompt to steer RS responses to conform with tone/style/formatting</li> <li>Uses PagedAttention to make RS efficient</li> </ul> <h3 id="data-processing-and-quality-control">Data Processing and Quality Control</h3> <p>Most of training data is model-generated, requires careful cleaning and quality control</p> <p>Data cleaning:</p> <ul> <li>Rule-based data removal or modification strategies</li> <li>Balance proportion of such samples in dataset</li> </ul> <p>Data pruning:</p> <ul> <li>Topic classification: Fine-tuned Llama 3 8B to a topic classifier</li> <li>Quality scoring: Use RM and Llama 3 checkpoint to rate content, keep examples marked as high quality by either RM or Llama. Both signals have high disagreement rates, and combining signals gives best recall on test set.</li> <li>Difficulty scoring: used Llama 3 70B to perform intention-tagging, where more intentions implies more complexity. Also used it to measure difficulty of dialogs</li> <li>Semantic deduplication: clustering using RoBERTa, sort by quality score \(\times\) difficulty score, go through sorted examples by best and take only ones with maximum cosine similarity less than threshold</li> </ul> <h2 id="capabilities">Capabilities</h2> <h3 id="code">Code</h3> <p>Capabilities:</p> <ul> <li>Code generation</li> <li>Documentation</li> <li>Debugging</li> <li>Review</li> </ul> <p>Targeted languages: Python, Java, Javascript, C/C++, Typescript, Rust, PHP, HTML/CSS, SQL, bash/shell</p> <p>Improved capabilities via:</p> <ul> <li>Training a code expert</li> <li>Generate synthetic data for SFT</li> <li>Improve formatting with system prompt steering</li> <li>Create quality filters to remove bad examples</li> </ul> <p>Expert training:</p> <ul> <li>Train code expert to obtain high quality human annotations for code</li> <li>Approach similar to CodeLlama (scant on details, should probably check this paper)</li> </ul> <p>Synthetic data generation:</p> <ul> <li>Faced issues in code generation: following instructions, code syntax errors, incorrect code generation, difficulty in fixing bugs</li> <li>Use Llama 3 and code expert to generate synthetic 2.7M dialogs for SFT</li> </ul> <p>During RS, used code specific system prompts to improve:</p> <ul> <li>code readability</li> <li>documentation</li> <li>thoroughness</li> <li>specificity</li> </ul> <h4 id="synthetic-data-generation-execution-feedback">Synthetic data generation: execution feedback</h4> <ul> <li>Distillation to smaller models helped, but not for 405B on its own inputs</li> <li>Use execution feedback as source of truth, allow model to learn from own mistakes <ol> <li>Problem description generation: generate programming problem descriptions, use random code snippets as inspiration</li> <li>Solution generation: Prompt Llama 3 to solve problem, use CoT in comments, add programming guidelines in system prompt</li> <li>Correctness analysis: use static analysis (parser and linters), and unit test generation (also by the model) and execution</li> <li>Error feedback and iterative self-correction: prompt model to revise solutions that fail, includes feedback from parser/linter/tester. Can modify code and unit test to accomodate new code. 20% of solutions that were incorrect could be self-corrected this way.</li> <li>Fine-tuning and iterative improvement: process iterated over multiple rounds, higher-quality synthetic data generated in each subsequent rounds</li> </ol> </li> </ul> <h4 id="synthetic-data-generation-programming-language-translation">Synthetic data generation: programming language translation</h4> <ul> <li>Noted performance gap between popular vs less common programming languages, due to difference in dataset size</li> <li>Translate data from more common to less common languages</li> <li>Ensure quality via syntax parsing, compilation, execution</li> </ul> <h4 id="synthetic-data-generation-backtranslation">Synthetic data generation: backtranslation</h4> <ul> <li>Some coding capabilities don‚Äôt benefit as much from execution feedback, i.e documentation &amp; explanation</li> <li>Generated 1.2M synthetic dialogs for code explanation, generation, documentation, debugging</li> <li>Done as follows: <ol> <li>Generate: Prompt Llama 3 to generate data corresponding to desired capability (i.e add comments to code)</li> <li>Backtranslate: Ask model to backtranslate synthetically generated data to original code (i.e generate code based on only comments)</li> <li>Filter: ask Llama 3 to determine quality of generated code with original code as reference. This self-verification step acts as a filter for good examples, only those with high scores are used for SFT</li> </ol> </li> </ul> <h3 id="tool-use">Tool Use</h3> <p>Trained Llama 3 to use search engine (Brave), Python interpreter, Wolfram Alpha API.</p> <p>To train on tool use:</p> <ul> <li>Start with training single-turn tool use, then tool use in dialog, and then multi-step tool use and data analysis</li> <li>All synthetically generated: first synthetic user prompts which require calling out to tools, then the corresponding tool calls which are then executed, and then the final answer to user prompt</li> <li>Multi-step tool use trained in a similar way synthetically</li> <li>User prompts are based on a provided file, and ask to summarize the contents of the file, find and fix bugs, optimize a piece of code, perform data analysis or visualization</li> <li>Augmented synthetic data with different system prompts to teach model to use tools only when activated</li> <li>To avoid model using tools for simple queries, added dataset containing queries of simple math/reasoning questions with tool use activated but without using tools in response</li> </ul> <h3 id="factuality">Factuality</h3> <p>To train the model to guard against hallucinations, they used a knowledge probe to find out what the model knows, and to generate training data of refusals for the things it doesn‚Äôt:</p> <ol> <li>Extract a data snippet from the pre-training data.</li> <li>Generate a factual question about these snippets (context) by prompting Llama 3.</li> <li>Sample responses from Llama 3 to the question.</li> <li>Score the correctness of the generations using the original context as a reference and Llama 3 as a judge.</li> <li>Score the informativeness of the generations using Llama 3 as a judge.</li> <li>Generate a refusal for responses which are consistently informative and incorrect across the generations, using Llama 3.</li> </ol> <p>But because pre-training data is not always factually correct, they also did this for sensitive topics where contradictory/incorrect statements are prevalent</p> <h3 id="steerability">Steerability</h3> <p>Remainder to be continued‚Ä¶</p> ]]></content><author><name>fanpu</name></author><category term="machine-learning"/><summary type="html"><![CDATA[Notes on the new Llama 3.1 technical report. It's a long paper, but one that's well-written with lots of interesting technical details and design choices.]]></summary></entry><entry><title type="html">Playing Sound Voltex at Home: Setting Up Unnamed SDVX Clone with the Yuancon SDVX Controller</title><link href="https://fanpu.io/blog/2023/setting-up-yuancon-controller-sound-voltex/" rel="alternate" type="text/html" title="Playing Sound Voltex at Home: Setting Up Unnamed SDVX Clone with the Yuancon SDVX Controller"/><published>2023-09-02T00:00:00+00:00</published><updated>2023-09-02T00:00:00+00:00</updated><id>https://fanpu.io/blog/2023/setting-up-yuancon-controller-sound-voltex</id><content type="html" xml:base="https://fanpu.io/blog/2023/setting-up-yuancon-controller-sound-voltex/"><![CDATA[<p>Rhythm is just a $200 controller and some hopefully-not-too-complicated open source software setup away!</p> <p>This beginner‚Äôs guide will help to demystify the process of setting up Sound Voltex at home using a custom SDVX controller using Unnamed SDVX Clone.</p> <hr/> <p>My foray into rhythm games started way back with <a href="https://lovelive-sif-global.bushimo.jp/">Love Live! School Idol Festival</a>. While the game has sadly since shut down, other titles I‚Äôve played include <a href="https://bang-dream-gbp-en.bushiroad.com/">BanG Dream!</a> and <a href="https://projectsekai.fandom.com/wiki/Project_SEKAI_COLORFUL_STAGE!">Project SEKAI</a>.</p> <p>When I visited Japan a few months ago in the summer, I discovered <a href="https://p.eagate.573.jp/game/sdvx/sv/p/index.html">Sound Voltex</a>, and instantly fell in love with its unique control system and beautiful flashy graphics:</p> <figure> <picture> <img src="/assets/img/posts/sdvx/sdvx_and_me.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In Japan, you pay 100 yen (~$0.68 USD) to play two to three songs. You get to play three songs if you don‚Äôt crash (i.e fail) any tracks, and two if you fail on either of the first two guaranteed plays.</p> <p>Rhythm games in general are sadly not as mainstream outside of Japan. For instance, in Singapore I was only aware of a single arcade that had Sound Voltex cabs, even though arcades are quite popular in general. Similarly, in NYC, there‚Äôs only a single small arcade called <a href="http://www.chinatownfair.biz/">Chinatown Fair</a> that has Sound Voltex. So naturally I wanted to see if I could set it up at home to continue enjoying the game.</p> <p>(Apparently, if you have a lot of disposable income and space in your living room, you can also just buy an entire <a href="https://www.hadouken-arcade.com/products/sound-voltex-vivid-wave">previous-generation Sound Voltex cabinet</a> for a few thousand dollars)</p> <h2 id="why-this-guide">Why This Guide</h2> <p>I decided to write this guide since the setup process could seem somewhat daunting for people who are interested in rhythm games but are not developers. Hopefully now more people will also be able to play and enjoy this game.</p> <p>The setup process is very straightforward on Windows, but has a few subtle points on macOS and Linux that I‚Äôll point out.</p> <h2 id="setup-specification">Setup Specification</h2> <p>This guide will use the following setup:</p> <ul> <li>Game: <a href="https://github.com/Drewol/unnamed-sdvx-clone">unnamed-sdvx-clone</a>, commonly abbreviated USC</li> <li>Controller: <a href="https://yuancon.store/controller/sdvxblack">Yuancon SDVX controller</a>.</li> </ul> <p>While I performed the setup on macOS, the instructions are largely the same for Linux based systems as well. In fact, if you are already regardless of which controller or OS you use.</p> <h2 id="installing-unnamed-sdvx-clone">Installing Unnamed SDVX Clone</h2> <h3 id="windows">Windows</h3> <p>The setup process for Windows is very straightforward. You should just download the <a href="https://drewol.me/Downloads/Game.zip">latest Windows build</a> as linked on the <a href="https://drewol.me/Downloads/Game.zip">Github page</a>, and run <code class="language-plaintext highlighter-rouge">usc-game.exe</code> to start the game.</p> <h3 id="macos">macOS</h3> <p>This is mostly just from the <a href="https://github.com/Drewol/unnamed-sdvx-clone#macos">official instructions</a>, but with implicit points made explicit:</p> <ol> <li>If you don‚Äôt have <a href="https://docs.brew.sh/">Homebrew</a> on your machine yet, install it by <a href="https://brew.sh/">following the instructions here</a>. Homebrew is a package management software.</li> <li> <p>If you don‚Äôt have <a href="https://git-scm.com/">git</a> yet, install it with Homebrew:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>brew <span class="nb">install </span>git
</code></pre></div> </div> <p>Git is a version control system (normally used for code). In our case, we use it mainly to obtain the project dependencies.</p> </li> <li> <p>Clone the <code class="language-plaintext highlighter-rouge">unnamed-sdvx-clone</code> with <code class="language-plaintext highlighter-rouge">git</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>git clone https://github.com/Drewol/unnamed-sdvx-clone
</code></pre></div> </div> <p>This will result in the game being downloaded to a <code class="language-plaintext highlighter-rouge">unnamed-sdvx-clone</code> folder in your current working directory.</p> </li> <li> <p>Navigate into the new folder, and download the submodules of the project:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cd </span>unnamed-sdvx-clone
<span class="nv">$ </span>git submodule update <span class="nt">--init</span> <span class="nt">--recursive</span>
</code></pre></div> </div> <p>This is necessary because the game has third-party dependencies, which are tracked as <a href="https://github.com/Drewol/unnamed-sdvx-clone/blob/develop/.gitmodules">other Github repositories</a>.</p> </li> <li> <p>Install more dependencies required to build the project with Homebrew:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>brew <span class="nb">install </span>cmake freetype libvorbis sdl2 libpng jpeg libarchive libiconv
</code></pre></div> </div> <p>These are all open source libraries required for the following reasons:</p> <ul> <li><a href="https://cmake.org/"><code class="language-plaintext highlighter-rouge">cmake</code></a>: a popular build system used to compile the project</li> <li><a href="https://freetype.org/"><code class="language-plaintext highlighter-rouge">freetype</code></a>: for rendering fonts</li> <li><a href="https://xiph.org/vorbis/"><code class="language-plaintext highlighter-rouge">libvorbis</code></a>: audio compression</li> <li><a href="https://www.libsdl.org/"><code class="language-plaintext highlighter-rouge">sdl2</code></a>: get access to hardware inputs like keyboard, mouse, controller, etc</li> <li><a href="https://github.com/glennrp/libpng"><code class="language-plaintext highlighter-rouge">libpng</code></a>: for using/manipulating PNG images</li> <li><a href="https://www.ijg.org/"><code class="language-plaintext highlighter-rouge">jpeg</code></a>: for using/manipulating JPEG images</li> <li><a href="https://www.libarchive.org/"><code class="language-plaintext highlighter-rouge">libarchive</code></a>: compression library</li> <li><a href="https://www.gnu.org/software/libiconv/"><code class="language-plaintext highlighter-rouge">libiconv</code></a>: convert between different character encodings (i.e <a href="https://en.wikipedia.org/wiki/ISO/IEC_8859-1">ISO-8859-1</a> to <a href="https://en.wikipedia.org/wiki/UTF-8">UTF-8</a>)</li> </ul> </li> <li> <p>Configure the project using <code class="language-plaintext highlighter-rouge">cmake</code>. In this case, the project author already kindly wrapped a script around this command, so we only have to run the script:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./mac-cmake.sh
</code></pre></div> </div> </li> <li> <p>Compile and build the project:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>make
</code></pre></div> </div> <p>This step could take a while. If you want to speed it up, you can run it and specify the <code class="language-plaintext highlighter-rouge">-j</code> argument parallelize the compilation based on the number of cores you have (use one less than your total number of cores):</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>make <span class="nt">-j</span> 9
</code></pre></div> </div> </li> <li> <p>Run the game from the <code class="language-plaintext highlighter-rouge">bin</code> folder:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cd </span>bin
<span class="nv">$ </span>./usc-game
</code></pre></div> </div> <p>It is important to run it from the <code class="language-plaintext highlighter-rouge">./bin</code> folder and not the root of the project directory, as some skins search for file dependencies in a relative manner and will hence not be able to find it.</p> </li> </ol> <h3 id="linux">Linux</h3> <p>Honestly if you‚Äôre on Linux, you should be able to figure it out by yourself üòä</p> <h2 id="first-startup">First Startup</h2> <p>On first startup, you should see this:</p> <figure> <video src="/assets/img/posts/sdvx/sdvx-initial.webm" class="z-depth-1" width="400px" height="auto" autoplay="" loop="" muted=""/> </figure> <p>For now, you can just use your mouse to interact with the game menu.</p> <h2 id="configuring-the-controller">Configuring The Controller</h2> <p>Let‚Äôs now setup our Yuancon controller!</p> <ol> <li> <p>Quit the game, and unplug your SDVX controller if it is plugged in</p> </li> <li> <p>Hold the <code class="language-plaintext highlighter-rouge">START</code> and <code class="language-plaintext highlighter-rouge">BT-C</code> button simultaneously. The <code class="language-plaintext highlighter-rouge">START</code> button is the diamond-shaped button at the top, while the BT-C is the third white button from the left (it should also be labelled on the controller board).</p> <p>Then while still holding down both buttons, connect it to your computer. This will put it in <a href="https://oniichan.wtf/help/index.html">Controller HID Mode</a>, where the controller inputs as a gamepad.</p> </li> <li> <p>Start up the game again, and navigate to the <code class="language-plaintext highlighter-rouge">Settings</code> page. Here, you want to do the following:</p> <ul> <li>Set <code class="language-plaintext highlighter-rouge">Button input mode</code> to <code class="language-plaintext highlighter-rouge">Controller</code></li> <li>Set <code class="language-plaintext highlighter-rouge">Laser input mode</code> to <code class="language-plaintext highlighter-rouge">Controller</code></li> <li>Adjust laser sensitivity accordingly (I like <code class="language-plaintext highlighter-rouge">1.875</code>)</li> <li>Click on each of the key bindings, and hit the corresponding keys on your controller. I used the button on the right side of the controller panel as my back button.</li> </ul> <p>You should have something that looks similar to this:</p> <p><img src="/assets/img/posts/sdvx/sdvx-settings.webp" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover"/></p> </li> <li> <p>Restart the game. You should now be able to use the knobs to cycle through the menus, and the buttons to activate them!</p> </li> </ol> <h2 id="getting-songs">Getting Songs</h2> <p>Right after setup, there are no songs to play yet. USC uses the same chart format as <a href="https://www.kshootmania.com/en/">K-Shoot MANIA (KSM)</a></p> <p>There are a few places you can get songs:</p> <ul> <li><a href="https://ksm.dev/">Nautica</a> hosts community-created KSM charts. There is also a menu option to download these directly from within USC.</li> <li><a href="https://oniichan.wtf/help/songs.html">Converted SDVX Charts</a>: probably somewhat questionable legally because of copyright and whatnot but a lot of people recommend and use it</li> <li><a href="https://www.reddit.com/r/kshootmania/wiki/gettingsongs/#wiki_getting_more_songs">This KSM FAQ page on Reddit</a> provides many useful links for downloading new songs</li> </ul> <p>Once you have downloaded the songs, unzip and extract them if necessary, and copy them into <code class="language-plaintext highlighter-rouge">./bin/songs</code>.</p> <h2 id="skinning-the-game">Skinning The Game</h2> <p>The default skin works, but it is not very impressive:</p> <figure> <video src="/assets/img/posts/sdvx/sdvx-default.webm" class="z-depth-1" width="400px" height="auto" autoplay="" loop="" muted=""/> </figure> <p>Let‚Äôs try to re-create the original SDVX arcade experience with skins. You can get skins for the game <a href="https://oniichan.wtf/help/skins.html">here</a>. These are really high-effort and well-made, and huge thanks to the developers and artists for making them.</p> <p>Once you have downloaded the skin, extract and move them to <code class="language-plaintext highlighter-rouge">./bin/skins</code>. You should then be able to select the skin under the <code class="language-plaintext highlighter-rouge">Skins</code> tab of the game settings.</p> <p>The UI of the skin for the game may change depending on whether your monitor is in portrait or landscape mode. Orienting it vertically is recommended for the best SDVX-like experience - the spaceship(?) at the bottom only shows up when it‚Äôs vertical.</p> <p>Some examples of the different skins are shown below. I know, they‚Äôre pretty!</p> <p>(<em>Why are the previews so low-res? <a href="https://www.digitalocean.com/blog/its-all-about-the-bandwidth-why-many-network-intensive-services-select-digitalocean-as-their-cloud">Bandwidth costs add up!</a></em>)</p> <h3 id="liqidwave">LiqidWave</h3> <figure> <video src="/assets/img/posts/sdvx/vivid.webm" class="z-depth-1" width="400px" height="auto" autoplay="" loop="" muted=""/> </figure> <p>If you run into errors about shaders when trying to play a song, see the <a href="#common-errors">Common Errors</a> section below.</p> <h3 id="experimentalgear">ExperimentalGear</h3> <figure> <video src="/assets/img/posts/sdvx/experimental-gear.webm" class="z-depth-1" width="400px" height="auto" autoplay="" loop="" muted=""/> </figure> <p>As a side note, if you find the default menu text for this skin too casual/unprofessional, you can change it in <code class="language-plaintext highlighter-rouge">./bin/skins/ExperimentalGear/scripts/language/EN.lua</code>.</p> <h3 id="heavenly-express">Heavenly Express</h3> <figure> <video src="/assets/img/posts/sdvx/heavenly-express.webm" class="z-depth-1" width="400px" height="auto" autoplay="" loop="" muted=""/> </figure> <h2 id="crew">Crew</h2> <p>Not all skins come with a cast of crews, like the ExperimentalGear skin which only comes with a boring empty <code class="language-plaintext highlighter-rouge">nothing</code> skin in <code class="language-plaintext highlighter-rouge">./bin/skins/ExperimentalGear/textures/crew/anim</code>.</p> <p>As crews are very important for our psychological safety and well-being, fortunately we can just copy over the animations from other skins. In HeavenlyExpress, you can find it in <code class="language-plaintext highlighter-rouge">./bin/skins/HeavenlyExpress-1.3.0/textures/_shared/crew</code>. Similarly, in LiqidWave they are stored in <code class="language-plaintext highlighter-rouge">./bin/skins/LiqidWave-1.5.0/textures/_shared/crew</code>.</p> <figure> <video src="/assets/img/posts/sdvx/rasis.webm" class="z-depth-1" width="400px" height="auto" autoplay="" loop="" muted=""/> </figure> <h2 id="conclusion">Conclusion</h2> <p>If you‚Äôve made it this far, congrats and thanks for reading! I hope you‚Äôll enjoy the game as much as I do. If you have any questions or run into problems, feel free to ask in the comments section below.</p> <h2 id="extras">Extras</h2> <h3 id="aside-ksm-chart-formats">Aside: KSM Chart Formats</h3> <p>KSM charts have a <code class="language-plaintext highlighter-rouge">.ksh</code> extensions. This can be a useful check to ensure that any charts that you download are actually for this game.</p> <p>The following is a snippet of the <code class="language-plaintext highlighter-rouge">ADV.ksh</code> (i.e advanced beatmap) file for YOASOBI‚Äôs Idol („Ç¢„Ç§„Éâ„É´):</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">title</span><span class="o">=</span>„Ç¢„Ç§„Éâ„É´
<span class="nv">artist</span><span class="o">=</span>YOASOBI /„ÄåÊé®„Åó„ÅÆÂ≠ê„Äç„Çà„Çä
<span class="nv">effect</span><span class="o">=</span>AS
<span class="nv">jacket</span><span class="o">=</span>jk.jpg
<span class="nv">illustrator</span><span class="o">=</span>-
<span class="nv">difficulty</span><span class="o">=</span>challenge
<span class="nv">level</span><span class="o">=</span>10
<span class="nv">t</span><span class="o">=</span>166
<span class="nv">m</span><span class="o">=</span>music.ogg
<span class="nv">o</span><span class="o">=</span>0
<span class="nb">bg</span><span class="o">=</span>desert
<span class="nv">layer</span><span class="o">=</span>smoke
<span class="nv">po</span><span class="o">=</span>56024
<span class="nv">plength</span><span class="o">=</span>15000
<span class="nv">pfiltergain</span><span class="o">=</span>50
<span class="nv">filtertype</span><span class="o">=</span>peak
<span class="nv">chokkakuautovol</span><span class="o">=</span>0
<span class="nv">chokkakuvol</span><span class="o">=</span>50
<span class="nv">ver</span><span class="o">=</span>171
<span class="nt">--</span>
<span class="nv">beat</span><span class="o">=</span>4/4
0000|00|--
<span class="nt">--</span>
0000|00|0-
0000|00|:-
0000|00|o-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|o-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|P-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|P-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
0000|00|:-
<span class="nt">--</span>
0000|00|0-
0000|00|:-
<span class="nv">filtertype</span><span class="o">=</span>lpf1
0000|00|0o
0000|00|::
0000|00|o0
0000|00|::
</code></pre></div></div> <h2 id="possible-errors-and-how-to-resolve">Possible Errors And How To Resolve</h2> <p>Some errors I faced when trying to setup and configure the game.</p> <h3 id="module-commonshared-not-found">module <code class="language-plaintext highlighter-rouge">commonShared</code> not found</h3> <p>If you get a Lua error about not being able to load a <code class="language-plaintext highlighter-rouge">commonShared</code> package, such as when using a custom skin:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>14:45:10][Error] Lua error: ...clone/bin/skins/HeavenlyExpress-1.3.0/scripts/common.lua:2: module <span class="s1">'commonShared'</span> not found:
	no field package.preload[<span class="s1">'commonShared'</span><span class="o">]</span>
	no file <span class="s1">'/usr/local/share/lua/5.3/commonShared.lua'</span>
	no file <span class="s1">'/usr/local/share/lua/5.3/commonShared/init.lua'</span>
	no file <span class="s1">'/usr/local/lib/lua/5.3/commonShared.lua'</span>
	no file <span class="s1">'/usr/local/lib/lua/5.3/commonShared/init.lua'</span>
	no file <span class="s1">'./commonShared.lua'</span>
	no file <span class="s1">'./commonShared/init.lua'</span>
	no file <span class="s1">'/Users/fanpu/unnamed-sdvx-clone/bin/skins/HeavenlyExpress-1.3.0/scripts/commonShared.lua'</span>
	no file <span class="s1">'skins/HeavenlyExpress-1.3.0/textures/_shared/scripts/commonShared.lua'</span>
	no file <span class="s1">'/usr/local/lib/lua/5.3/commonShared.so'</span>
	no file <span class="s1">'/usr/local/lib/lua/5.3/loadall.so'</span>
	no file <span class="s1">'./commonShared.so'</span>

</code></pre></div></div> <p>You are likely running the game from the root of the project directory (i.e <code class="language-plaintext highlighter-rouge">./bin/usc-game</code>), instead of from within the <code class="language-plaintext highlighter-rouge">./bin</code> directory itself.</p> <h3 id="heavenlyexpress-skin-could-not-load-shaders">HeavenlyExpress Skin: Could not load shaders</h3> <p>If you are using the HeavenlyExpress skin, you may run into the following error after selecting a track to play:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Shader Error: 
Could not load shaders skins/HeavenlyExpress-1.3.0/shaders/holdbutton.vs 
and skins/HeavenlyExpress-1.3.0/shaders/holdbutton.fs
</code></pre></div></div> <p>You may also get logs like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>14:58:37][Error] Shader program compile log <span class="k">for</span> /Users/fanpu/unnamed-sdvx-clone/bin/skins/HeavenlyExpress-1.3.0/shaders/holdbutton.vs: ERROR: 0:6: <span class="s1">'varying'</span> : syntax error: syntax error

<span class="o">[</span>14:58:37][Error] Shader program compile log <span class="k">for</span> /Users/fanpu/unnamed-sdvx-clone/bin/skins/HeavenlyExpress-1.3.0/shaders/holdbutton.fs: ERROR: 0:10: <span class="s1">'varying'</span> : syntax error: syntax error

<span class="o">[</span>14:58:37][Error] Failed to load vertex shader <span class="k">for </span>material from /Users/fanpu/unnamed-sdvx-clone/bin/skins/HeavenlyExpress-1.3.0/shaders/holdbutton.vs
</code></pre></div></div> <p>The shaders were probably written a long time ago, since the <code class="language-plaintext highlighter-rouge">varying</code> keyword has been deprecated since OpenGL 3.3. It was previously used as a qualifier for variables that communicate between the vertex shader and the fragment shader, that is now replaced by the <code class="language-plaintext highlighter-rouge">in</code> and <code class="language-plaintext highlighter-rouge">out</code> qualifiers to provide a more clear distinction of data flow between shaders.</p> <p>To fix this, modify the two files and change the <code class="language-plaintext highlighter-rouge">varying</code> keyword to <code class="language-plaintext highlighter-rouge">out</code> in both files:</p> <p>In file <code class="language-plaintext highlighter-rouge">bin/skins/HeavenlyExpress-1.3.0/shaders/holdbutton.vs</code>:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#version 330
#extension GL_ARB_separate_shader_objects : enable
</span><span class="n">layout</span><span class="p">(</span><span class="n">location</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="n">in</span> <span class="n">vec2</span> <span class="n">inPos</span><span class="p">;</span>
<span class="n">layout</span><span class="p">(</span><span class="n">location</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="n">in</span> <span class="n">vec2</span> <span class="n">inTex</span><span class="p">;</span>

<span class="n">out</span> <span class="n">vec4</span> <span class="n">position</span><span class="p">;</span> <span class="c1">// update here</span>

<span class="n">out</span> <span class="n">gl_PerVertex</span>
<span class="p">{</span>
        <span class="n">vec4</span> <span class="n">gl_Position</span><span class="p">;</span>
<span class="p">};</span>

<span class="p">...</span><span class="n">rest</span> <span class="n">of</span> <span class="n">file</span> <span class="n">omitted</span><span class="p">...</span>
</code></pre></div></div> <p>In file <code class="language-plaintext highlighter-rouge">bin/skins/HeavenlyExpress-1.3.0/shaders/holdbutton.fs</code>:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#version 330
#extension GL_ARB_separate_shader_objects : enable
</span>
<span class="n">layout</span><span class="p">(</span><span class="n">location</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="n">in</span> <span class="n">vec2</span> <span class="n">fsTex</span><span class="p">;</span>
<span class="n">layout</span><span class="p">(</span><span class="n">location</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="n">out</span> <span class="n">vec4</span> <span class="n">target</span><span class="p">;</span>

<span class="n">uniform</span> <span class="n">sampler2D</span> <span class="n">mainTex</span><span class="p">;</span>
<span class="n">uniform</span> <span class="kt">float</span> <span class="n">objectGlow</span><span class="p">;</span>

<span class="n">out</span> <span class="n">vec4</span> <span class="n">position</span><span class="p">;</span> <span class="c1">// update here</span>

<span class="p">...</span><span class="n">rest</span> <span class="n">of</span> <span class="n">file</span> <span class="n">omitted</span><span class="p">...</span>
</code></pre></div></div> <p>Restart the game and you should be good now.</p> <h3 id="experimentalgear-custom-skin-does-not-change">ExperimentalGear Custom Skin Does Not Change</h3> <p>I faced issues where it appeared that the value that I set in the settings page for the skin to use was not being saved. I resolved this by manually editing the config file in <code class="language-plaintext highlighter-rouge">./bin/skins/ExperimentalGear/skin.cfg</code>.</p>]]></content><author><name>fanpu</name></author><category term="general"/><category term="rhythm-games"/><summary type="html"><![CDATA[Rhythm is just a $200 controller and some hopefully-not-too-complicated open source software setup away! This beginner's guide will help to demystify the process of setting up Sound Voltex at home using a custom SDVX controller using Unnamed SDVX Clone.]]></summary></entry><entry><title type="html">Creating Trackback Requests for Static Sites</title><link href="https://fanpu.io/blog/2023/creating-trackback-requests/" rel="alternate" type="text/html" title="Creating Trackback Requests for Static Sites"/><published>2023-09-01T00:00:00+00:00</published><updated>2023-09-01T00:00:00+00:00</updated><id>https://fanpu.io/blog/2023/creating-trackback-requests</id><content type="html" xml:base="https://fanpu.io/blog/2023/creating-trackback-requests/"><![CDATA[<p>In this article, I will show you how you can generate trackback requests to external websites to link back to your static site like <a href="https://jekyllrb.com/">Jekyll</a> or <a href="https://gohugo.io/">Hugo</a>. I decided to write this article after realizing how there is almost no information online about how to make DIY trackback requests when I was trying to set it up.</p> <h2 id="what-is-trackback">What is Trackback?</h2> <p>From <a href="">Wikipedia</a>:</p> <blockquote> <p>A trackback allows one website to notify another about an update. It is one of four types of linkback methods for website authors to request notification when somebody links to one of their documents. This enables authors to keep track of who is linking to their articles. Some weblog software, such as SilverStripe, WordPress, Drupal, and Movable Type, supports automatic pingbacks where all the links in a published article can be pinged when the article is published. The term is used colloquially for any kind of linkback.</p> </blockquote> <p>Essentially, it is a mechanism for other websites to know that you mentioned them, with the hope that they‚Äôll notice you and possibly mention you as well. It helps to increase the visibility and discoverability of your website.</p> <h2 id="use-case">Use Case</h2> <p>My use case was to send trackbacks to <a href="https://info.arxiv.org/help/trackback.html">arXiv</a>, so that specific arXiv papers will know that my blog post mentioned them, and readers can also check it out as an additional resource. In particular, each of my <a href="summaries">paper summary</a> posts is based around a paper, and it would be nice if they could be linked from the respective arXiv paper abstract pages.</p> <p>In arXiv, there is a blog link section that will track websites that made trackback requests for a given paper:</p> <figure> <picture> <img src="/assets/img/posts/trackback_post/trackback_blog_link.webp" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Unfortunately, if you try to search for anything about trackbacks and/or pingbacks, most of what you‚Äôll get are articles about how to disable them on popular blogging platforms like WordPress due to <a href="https://blog.hubspot.com/website/trackback-spam">widespread misuse and spam</a>, or otherwise how to configure them.</p> <p>There was also a 7-year old <a href="https://superuser.com/questions/1098682/how-to-send-trackback-to-arxiv-papers-from-a-jekyll-blog">StackOverflow post</a> about how to create trackback requests for arXiv, essentially the same problem I was facing. Sadly, it currently has a grand total of 0 answers and 0 comments. I hope this article might be useful if the author is still facing the issue.</p> <h2 id="manually-creating-trackback-requests">Manually Creating Trackback Requests</h2> <p>The convenience of CMS blogging software like <a href="https://wordpress.org/documentation/article/trackbacks-and-pingbacks/">WordPress</a> is that it supports features like automated trackbacks and pingbacks for content that you create. Static site generators are not capable of this, since by design they are static and stateless. This means that we have to make such requests manually, which is fortunately not too difficult!</p> <p>Here‚Äôs a very simple script for doing it. In this example, the target URL is for the arXiv trackback endpoint.</p> <p>Before reading or running the code, please note that you <strong>SHOULD NOT</strong> test or experiment on this with trackback listener URLs and spam them. You should only make requests if they are legitimate and you have a genuine reason for letting them know about your blog post. Trackback spam is a serious issue and part of why they have become so unpopular and unmanageable is due to the high volumes of spam.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">requests</span>

<span class="c1"># Replace with your own data
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">My Awesome Blog Post</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">url</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">https://my-blog.com/post/</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">blog_name</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">My Awesome Blog</span><span class="sh">'</span>
<span class="p">}</span>

<span class="c1"># Replace with actual Trackback destination URL
</span><span class="n">trackback_url</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">https://foo.bar/trackback/post_id</span><span class="sh">'</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">trackback_url</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

<span class="k">if</span> <span class="n">response</span><span class="p">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Trackback successful!</span><span class="sh">"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Trackback failed with status code: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">content</span><span class="p">.</span><span class="nf">decode</span><span class="p">())</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>A successful response has the <code class="language-plaintext highlighter-rouge">error</code> field set to <code class="language-plaintext highlighter-rouge">0</code>:</p> <figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="cp">&lt;?xml version="1.0" encoding="utf-8"?&gt;</span>
<span class="nt">&lt;response&gt;</span>
  <span class="nt">&lt;error&gt;</span>0<span class="nt">&lt;/error&gt;</span>
<span class="nt">&lt;/response&gt;</span></code></pre></figure> <p>If an error occured, the <code class="language-plaintext highlighter-rouge">error</code> field is set to <code class="language-plaintext highlighter-rouge">1</code>:</p> <figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="cp">&lt;?xml version="1.0" encoding="utf-8"?&gt;</span>
<span class="nt">&lt;response&gt;</span>
  <span class="nt">&lt;error&gt;</span>1<span class="nt">&lt;/error&gt;</span>
  <span class="nt">&lt;message&gt;</span>(some error message)<span class="nt">&lt;/message&gt;</span>
<span class="nt">&lt;/response&gt;</span></code></pre></figure> <h2 id="conclusion">Conclusion</h2> <p>And that‚Äôs all there is to creating Trackback requests! It‚Äôs actually quite simple, and is just not terribly well-documented.</p> <p>As a final parting word, a reminder again to please use it <em>responsibly</em> and <em>stay away from any behavior that could be constituted as spamming</em>.</p>]]></content><author><name>fanpu</name></author><category term="code"/><category term="general"/><summary type="html"><![CDATA[A simple guide on creating manual Trackback requests for static sites to increase visibility and discoverability]]></summary></entry><entry><title type="html">A Unified Framework for High-Dimensional Analysis of M-Estimators with Decomposable Regularizers: A Guided Walkthrough</title><link href="https://fanpu.io/blog/2023/high-dimensional-analysis-of-m-estimators/" rel="alternate" type="text/html" title="A Unified Framework for High-Dimensional Analysis of M-Estimators with Decomposable Regularizers: A Guided Walkthrough"/><published>2023-07-14T00:00:00+00:00</published><updated>2023-07-14T00:00:00+00:00</updated><id>https://fanpu.io/blog/2023/high-dimensional-analysis-of-m-estimators</id><content type="html" xml:base="https://fanpu.io/blog/2023/high-dimensional-analysis-of-m-estimators/"><![CDATA[\[\newcommand{\rcal}{\mathcal{R}} \newcommand{\lcal}{\mathcal{L}} \newcommand{\mcal}{\mathcal{M}} \newcommand{\mocal}{\overline{\mathcal{M}}} \newcommand{\mocalp}{\overline{\mathcal{M}}^\perp} \newcommand{\mcalp}{\mathcal{M}^\perp} \newcommand{\sse}{\subseteq} \newcommand{\kl}{\kappa_{\lcal}} \newcommand{\tl}{\tau_{\lcal}} \newcommand{\ts}{\theta^*} \newcommand{\hd}{\widehat{\Delta}} \newcommand{\thatn}{\hat{\theta}_n} \newcommand{\that}{\hat{\theta}} \newcommand{\thatlambda}{\widehat{\theta}_{\lambda_n}} \newcommand{\thatl}{\thatlambda} \newcommand{\rs}{\rcal^*} \newcommand{\ctriplet}{ \C(\mcal, \mocalp; \ts) } \newcommand{\fcal}{\mathcal{F}} \newcommand{\kbb}{\mathbb{K}} \newcommand{\dotprod}[2]{\langle #1, #2 \rangle} \DeclareMathOperator*{\argmin}{arg\,min}\] <h1 id="introduction">Introduction</h1> <p>In high-dimensional statistical inference, it is common for the number of parameters \(p\) to be comparable to or greater than the sample size \(n\). However, for an estimator \(\thatn\) to be consistent in such a regime, meaning that it converges to the true parameter \(\theta\), it is necessary to make additional low-dimensional assumptions on the model. Examples of such constraints that have been well-studied include linear regression with sparsity constraints, estimation of structured covariance or inverse covariance matrices, graphical model selection, sparse principal component analysis (PCA), low-rank matrix estimation, matrix decomposition problems and estimation of sparse additive nonparametric models <a href="https://arxiv.org/abs/1010.2731">(Negahban et al., 2009)</a>.</p> <p>In recent years, there has been a flurry of work on each of these individual specific cases. However, the authors of the paper in discussion poses the question of whether there is a way of unifying these analysis to understand all of such estimators in a common framework, and answers it in the affirmative. They showed that it is possible to bound the squared difference between any regularized \(M\)-estimator and its true parameter by (1) the decomposability of the regularization function, and (2) restricted strong convexity of the loss function. We will call this the ‚Äúmain theorem‚Äù in the remainder of the blog post, and this is referred to as ‚ÄúTheorem 1‚Äù in <a href="https://arxiv.org/abs/1010.2731">(Negahban et al., 2009)</a>.</p> <p>In the remainder of the paper, we will develop the tools necessary to deeply understand and prove the result. Notation used will be consistent with the original paper for expositional clarity.</p> <h1 id="background">Background</h1> <p>In this section, we develop some of the necessary background and notation to build up to the proof.</p> <h2 id="regularized-m-estimators">Regularized \(M\)-estimators</h2> <p>\(M\)-estimators (\(M\) for ‚Äúmaximum likelihood-type‚Äù) are solutions that minimize the sum of loss functions \(\rho\): \begin{align} \that \in \argmin_\theta \sum_{i=1}^n \rho(x_i, \theta). \end{align}</p> <p>If we add a regularization term \(\rcal\) to penalize complexity of the model, scaled by weights \(\lambda\), the method is known as a regularized \(M\)-estimator: \begin{align} \that \in \argmin_\theta \sum_{i=1}^n \rho(x_i, \theta) + \lambda \rcal(\theta). \end{align}</p> <div class="example"> <div class="theorem-title">Example (Lasso Program) </div> <div class="theorem-contents"> The Lasso program is an example of a regularized \( M \)-estimator, where a \( \ell_1 \) regularization penalty is applied: $$ \that \in \argmin_{\theta \in \mathbb{R}^d} \left\{ \frac{1}{2n} \| y - \bX \theta \|_2^2 + \lambda_n \| \theta \|_1 \right\}. $$ </div> </div> <h2 id="dual-norms">Dual Norms</h2> <div class="definition"> <div class="theorem-title">Definition (Dual Norms) </div> <div class="theorem-contents"> Let \(\rcal\) be a norm induced by an inner product \(\dotprod{\cdot}{\cdot}\). Then the dual norm of \(\rcal\) is defined as $$ \rs(v) \coloneqq \sup_{u \in \mathbb{R}^p \setminus \left\{ 0 \right\}} \frac{ \dotprod{u}{v} }{\rcal (u)} = \sup_{\rcal(u) \leq 1} \dotprod{u}{v}. $$ </div> </div> <div class="example"> <div class="theorem-title">Example (\(\ell_1\) and \(\ell_\infty\) norms are dual norms) </div> <div class="theorem-contents"> We will show that the dual of the \( \ell_1 \) norm is the \( \ell_\infty \) norm. Well, to see that \( \rs(v) \leq \| v \|_\infty \), observe that \begin{align*} \rs(v) &amp; = \sup_{\| u \|_1 \leq 1} \dotprod{u}{v} \\ &amp; = \sup_{\| u \|_1 \leq 1} \sum_{k=1}^p | u_k | | v_k | \\ &amp; \leq \sup_{\| u \|_1 \leq 1} \left( \sum_{k=1}^p | u_k | \right) \| v \|_\infty \\ &amp; = | v |_\infty \tag{since \( \| u \|_1 \leq 1 \) }. \end{align*} For the opposite direction, \begin{align*} \sup_{\| u \|_1 \leq 1} \dotprod{u}{v} &amp; = \sup_{\| u \|_1 \leq 1} \sum_{k=1}^p |u_k| |v_k| \\ &amp; \geq 1 \cdot |v_j| \tag{ set \( j = \argmax_j |v_j|, u = \be_j \) } \\ &amp; = \| v \|_\infty, \end{align*} hence we have equality. </div> </div> <h2 id="subspace-compatibility-constant">Subspace Compatibility Constant</h2> <p>The subspace compatibility constant measures how much the regularizer \(\rcal\) can change with respect to the error norm \(\| \cdot \|\) restricted to the subspace \(\mcal\). This concept will show up later in showing that the restricted strong convexity condition will hold with certain parameters.</p> <p>The subspace compatibility constant is defined as follows:</p> <div class="definition"> <div class="theorem-title">Definition (Subspace Compatibility Constant) </div> <div class="theorem-contents"> For any subspace \( \mcal \) of \( \mathbb{R}^p \), the <i>subspace compatibility constant</i> with respect to the pair \( (\rcal, \| \cdot \|) \) is given by $$ \varPsi (\mcal) \coloneqq \sup_{u \in \mcal \setminus \left\{ 0 \right\}} \frac{\rcal(u)}{\| u \|}. $$ </div> </div> <p>It can be thought of as the Lipschitz constant of the regularizer with respect to the error norm restricted to values in \(\mcal\), by considering the point where it can vary the most.</p> <h2 id="projections">Projections</h2> <p>Define the projection operator \begin{align} \Pi_{\mcal}(u) \coloneqq \argmin_{v \in \mcal} | u - v | \end{align} to be the projection of \(u\) onto the subspace \(\mcal\). For notational brevity, we will use the shorthand \(u_{\mcal} = \Pi_{\mcal}(u)\).</p> <p>One property of the projection operator is that it is non-expansive, meaning that \begin{align} | \Pi(u) - \Pi(v) | \leq | u - v | \label{eq:non-expansive} \end{align} for some error norm \(\| \cdot \|\). In other words, it has Lipschitz constant 1.</p> <h1 id="problem-formulation">Problem Formulation</h1> <p>In our setup, we define the following quantities:</p> <ul> <li>\(Z_1^n \coloneqq \left\{ Z_1, \cdots, Z_n \right\}\) \(n\) i.i.d observations drawn from distribution \(\mathbb{P}\) with some parameter \(\theta^*\),</li> <li>\(\mathcal{L}: \mathbb{R}^p \times \mathcal{Z}^n \to \mathbb{R}\) a convex and differentiable loss function, such that \(\mathcal{L}(\theta; Z_1^n)\) returns the loss of \(\theta\) on observations \(Z_1^n\),</li> <li>\(\lambda_n &gt; 0\): a user-defined regularization penalty,</li> <li>\(\mathcal{R} : \mathbb{R}^p \to \mathbb{R}_+\) a norm-based regularizer.</li> </ul> <p>The purpose of the regularized \(M\)-estimator is then to solve for the convex optimization problem</p> \[\begin{align} \label{eq:opt} \widehat{\theta}_{\lambda_n} \in \argmin_{\theta \in \mathbb{R}^p} \left\{ \mathcal{L}(\theta; Z_1^n) + \lambda_n \mathcal{R} (\theta) \right\}, \end{align}\] <p>and we are interested in deriving bounds on \(\begin{align} \| \thatlambda - \theta^* \| \end{align}\) for some error norm \(\| \cdot \|\) induced by an inner product \(\langle \cdot, \cdot \rangle\) in \(\mathbb{R}^p\).</p> <h1 id="decomposability-of-the-regularizer-mathcalr">Decomposability of the Regularizer \(\mathcal{R}\)</h1> <p>The first key property in the result is decomposability of our norm-based regularizer \(\rcal\). Working in the ambient \(\mathbb{R}^p\), define \(\mcal \sse \mathbb{R}^p\) to be the model subspace that captures the constraints of the model that we are working with (i.e \(k\)-sparse vectors), and denote \(\mocal\) to be its closure, i.e the union of \(\mcal\) and all of its limit points. In addition, denote \(\mocalp\) to be the orthogonal complement of \(\mocal\), namely</p> \[\begin{align} \mocalp \coloneqq \left\{ v \in \mathbb{R}^p \mid \langle u, v \rangle = 0 \text{ for all \( u \in \mocal \) } \right\}. \end{align}\] <p>We call this the perturbation subspace, as they represent perturbations away from the model subspace \(\mocal\). The reason why we need to consider \(\mocal\) instead of \(\mcal\) is because there are some special cases of low-rank matrices and nuclear norms where it could be possible that \(\mcal\) is strictly contained in \(\mocal\).</p> <p>Now we can introduce the property of decomposability:</p> <div class="definition"> <div class="theorem-title">Definition (Regularizer Decomposability) </div> <div class="theorem-contents"> Given a pair of subspaces \( \mcal \sse \mocal \), a norm-based regularizer \( \rcal \) is <i>decomposable</i> with respect to \( (\mocal, \mocalp) \) if $$ \rcal(\theta + \gamma) = \rcal(\theta) + \rcal(\gamma) $$ for all \( \theta \in \mcal \) and \( \gamma \in \mocalp \). </div> </div> <p>Since \(\rcal\) is a norm-based regularizer, by the triangle inequality property of norms we know that always \begin{align} \rcal(\theta + \gamma) \leq \rcal(\theta) + \rcal(\gamma), \end{align} and hence this is a stronger condition which requires tightness in the inequality when we are specifically considering elements in the closure of the model subspace and its orthogonal complement.</p> <p>Decomposability of the regularizer is important as it allows us to penalize deviations \(\gamma\) away from the model subspace in \(\mcal\) to the maximum extent possible. We are usually interested to find model subspaces that are small, with a large orthogonal complement. We will see in the main theorem that when this is the case, we will obtain better rates for estimating \(\theta^*\).</p> <p>There are many natural contexts that admit regularizers which are decomposable with respect to subspaces, and the following example highlights one such case.</p> <div class="example"> <div class="theorem-title">Example (\( s \)-sparse Vectors) </div> <div class="theorem-contents"> Consider estimating the parameters \( \that \) with \( \ell_1 \)-regularization in \( \mathbb{R}^p \) where we assume that the model is \( s \)-sparse. Then for any set \( S \sse [p] \) where \( |S| = s \), we can define our model subspace \( \mcal \) as \[ \begin{align*} \mcal(S) \coloneqq \left\{ \theta \in \mathbb{R}^p \mid \theta_j = 0 \quad \forall j \not\in S \right\}, \end{align*} \] i.e all the vectors in \( \mathbb{R}^p \) that only has support in \( S \). In this case, \( \mcal = \mocal \), and our orthogonal complement \( \mocalp \) is just \[ \begin{align*} \mocalp(S) \coloneqq \left\{ \gamma \in \mathbb{R}^p \mid \gamma_j = 0 \quad \forall j \in S \right\}. \end{align*} \] Then this setup is decomposable: \[ \begin{align*} \| \theta + \gamma \|_1 = \| \theta_S + \gamma_{S^c} \|_1 = \| \theta_S \|_1 + \| \gamma_{S^c} \| = \| \theta \|_1 + \| \gamma \|_1 \end{align*} \] by the Pythagorean theorem. </div> </div> <h2 id="role-of-decomposability">Role of Decomposability</h2> <figure id="fig-1"> <picture> <img src="/assets/img/posts/high-dimensional-analysis-of-m-estimators/c_illust.webp" class="z-depth-1 center" width="500px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"><i>Figure 1.</i> A visualization of \( \ctriplet \). The shaded area represents the set \( \ctriplet \), i.e all values of \( \theta \) that satisfies the inequality of the set in Lemma 1. </figcaption> </figure> <p>Decomposability is important because it allows us to bound the error of the estimator. This is given in the following result, which is known as Lemma 1 in <a href="https://arxiv.org/abs/1010.2731">(Negahban et al., 2009)</a>:</p> <div class="lemma" id="lemma-1"> <div class="theorem-title">Lemma (Lemma 1 in <a href="https://arxiv.org/abs/1010.2731"> (Negahban et al., 2009) </a>) </div> <div class="theorem-contents"> Suppose that \( \lcal \) is a convex and differentiable function, and consider any optimal solution \( \that \) to the optimization problem with a strictly positive regularization parameter satisfying $$ \begin{align*} \lambda_n \geq 2 \rcal^* (\nabla \lcal (\ts; Z_1^n)). \end{align*} $$ Then for any pair \( (\mcal, \mocalp) \) over which \( \rcal \) is decomposable, the error \( \hd = \thatlambda - \ts \) belongs to the set $$ \begin{align*} \label{eq:c} \C(\mcal, \mocalp; \ts) \coloneqq \left\{ \Delta \in \mathbb{R}^p \mid \rcal(\Delta_{\mocalp}) \leq 3 \rcal (\Delta_{\mocal}) + 4 \rcal (\ts_{\mcalp}) \right\}. \end{align*} $$ </div> </div> <p>Recall from the <a href="#projections">Projections Section</a> that \(\Delta_{\mocalp}\) represents the projection of \(\Delta\) onto \(\mocalp\), and similarly for the other quantities. Due to space constraints, we are unable to prove Lemma <a href="#lemma-1">Lemma 1</a> in this survey, but it is very important in the formulation of restricted strong convexity, and in proving <a href="#thm-1">Theorem 1</a>.</p> <p><a href="#fig-1">Figure 1</a> provides a visualization of \(\ctriplet\) in \(\mathbb{R}^3\) in the sparse vectors setting. In this case, \(S = \left\{ 3 \right\}\) with \(|S|=1\), and so the projection of \(\Delta\) onto the model subspace only has non-zero values on the third coordinate, and its orthogonal complement is where the third coordinate is zero. Formally,</p> \[\begin{align} \mcal(S) = \mocal(S) &amp; = \left\{ \Delta \in \mathbb{R}^3 \mid \Delta_1 = \Delta_2 = 0 \right\}, \\ \mocalp(S) &amp; = \left\{ \Delta \in \mathbb{R}^3 \mid \Delta_3 = 0 \right\}. \end{align}\] <p>The vertical axis of <a href="#fig-1">Figure 1</a> denotes the third coordinate, and the horizontal plane denotes the first two coordinates. The shaded area represents the set \(\ctriplet\), i.e all values of \(\theta\) that satisfies the inequality of the set in <a href="#lemma-1">Lemma 1</a>.</p> <p><a href="#fig-1">Figure 1(a)</a> shows the special case when \(\ts \in \mcal\). In this scenario, \(\rcal (\ts_{\mcalp}) = 0\), and so</p> \[\begin{align*} \C(\mcal, \mocalp; \ts) = \left\{ \Delta \in \mathbb{R}^p \mid \rcal(\Delta_{\mocalp}) \leq 3 \rcal (\Delta_{\mocal}) \right\}, \end{align*}\] <p>which is a cone.</p> <p>However, in the general setting where \(\ts \not\in \mcal\), then \(\rcal (\ts_{\mcalp}) &gt; 0\), and the set \(\ctriplet\) will become a star-shaped set like what is shown in <a href="#fig-1">Figure 1(b)</a>.</p> <h1 id="restricted-strong-convexity-rsc-of-the-loss-function">Restricted Strong Convexity (RSC) of the Loss Function</h1> <figure id="fig-2"> <picture> <img src="/assets/img/posts/high-dimensional-analysis-of-m-estimators/curvature.webp" class="z-depth-1 center" width="500px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"><i>Figure 2.</i> An illustration of the role of curvature in guaranteeing that \( \hd = \thatlambda - \ts \) is small when \( \lcal(\thatlambda) - \lcal(\ts) \) is small. </figcaption> </figure> <p>In a classical setting, as the number of samples \(n\) increases, the difference in loss \(d \lcal = |\lcal(\thatlambda) - \lcal(\ts)|\) will converge to zero. However, the convergence in loss by itself is insufficient to also ensure the convergence in parameters, \(\hd = \thatlambda - \ts\). Instead, it also depends on the curvature of the loss function \(\lcal\).</p> <p><a href="#fig-2">Figure 2</a> illustrates the importance of curvature. In <a href="#fig-2">Figure 2(a)</a>, \(\lcal\) has high curvature, and so having a small \(d\lcal\) also implies a small \(\hd\). On the other hand, in <a href="#fig-2">Figure 2(b)</a>, \(\lcal\) has an almost flat landscape near \(\thatlambda\), and hence even when \(d \lcal\) is small, \(\hd\) could still be large.</p> <p>Consider performing a Taylor expansion of \(\lcal\) around \(\ts\):</p> \[\begin{align} \lcal(\ts + \Delta) &amp; = \lcal(\ts) + \dotprod{\nabla \lcal(\ts)}{\Delta} + \underbrace{\frac{1}{2} \Delta^T \nabla^2 \lcal(\ts) \Delta + \dots}_{\delta \lcal(\Delta, \ts)}. \end{align}\] <p>Then we can rearrange and write the error of the first-order Taylor series expansion at \(\ts\) as</p> \[\begin{align*} \delta \lcal(\Delta, \ts) = \lcal(\ts + \Delta) - \lcal(\ts) - \dotprod{\nabla \lcal(\ts)}{\Delta}. \end{align*}\] <p>The first-order Taylor approximation is a linear approximation, and hence the error \(\delta \lcal(\Delta, \ts)\), which is dominated by the quadratic term, can capture the curvature about \(\ts\).</p> <p>As such, one way to show that \(\lcal\) has good curvature about \(\ts\) is to show that \(\delta \lcal(\Delta, \ts) \geq \kappa \|\Delta \|^2\) holds for all \(\Delta\) in a neighborhood of \(\ts\). This is because we are enforcing a lower bound on its quadratic growth.</p> <p>This leads us to the definition of restricted strong convexity:</p> <div class="definition"> <div class="theorem-title">Definition (Restricted Strong Convexity) </div> <div class="theorem-contents"> The loss function satisfies a <i>restricted strong convexity</i> (RSC) condition with curvature \( \kl &gt; 0 \) and tolerance function \( \tl \) if \begin{align*} \delta \lcal(\Delta, \ts) \geq \kl \| \Delta \|^2 - \tl^2(\ts) \end{align*} for all \( \Delta \in \ctriplet \). </div> </div> <p>We only need to consider error terms \(\Delta \in \ctriplet\), since Lemma \ref{lemma:1} guarantees us that the error term will only lie in that set.</p> <p>In many statistical models, restricted strong convexity holds with \(\tl = 0\), however, it is required in more general settings, such as generalized linear models.</p> <h1 id="proof-of-theorem-1">Proof of Theorem 1</h1> <p>We can now state and prove the main result of the paper. This will hold under the decomposability of the regularizer (G1), and the restricted strong convexity of the loss function (G2).</p> <ul> <li> <p><strong>(G1)</strong> The regularizer \(\rcal\) is a norm and is decomposable with respect to the subspace pair \((\mcal, \mocalp)\), where \(\mcal \sse \mocalp\).</p> </li> <li> <p><strong>(G2)</strong> The loss function \(\lcal\) is convex and differentiable, and satisfies restricted strong convexity with curvature \(\kl\) and tolerance \(\tl\).</p> </li> </ul> <div class="theorem" id="thm-1"> <div class="theorem-title">Theorem 1 in (Negahban et al., 2009) (Bounds for General Models) </div> <div class="theorem-contents"> Under conditions (G1) and (G2), consider the convex optimization problem (\ref{eq:opt}) based on a strictly positive positive regularization constant \( \lambda_n \geq 2 \rs (\nabla \lcal (\ts)) \). Then any optimal solution \( \thatlambda \) to the convex program (\ref{eq:opt}) satisfies the bound \begin{align*} \| \thatlambda - \ts \|^2 \leq 9 \frac{\lambda_n^2}{\kl^2} \Psi^2(\mocal) + \frac{\lambda_n}{\kl} \left( 2 \tl^2 (\ts) + 4 \rcal (\ts_{\mcal^{\perp}}) \right). \end{align*} </div> </div> <p>We will rely on the following lemmas that will be stated without proof due to space constraints:</p> <div class="lemma"> <div class="theorem-title">Lemma 3 in (Negahban et al., 2009) (Deviation Inequalities) </div> <div class="theorem-contents"> For any decomposable regularizer and \( p \)-dimensional vectors \( \ts \) and \( \Delta \), we have \begin{align*} \rcal(\ts + \Delta) - \rcal(\ts) \geq \rcal(\Delta_{\mocalp}) - \rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}}). \end{align*} Moreover, as long as \( \lambda_n \geq 2 \rs (\nabla \lcal(\ts)) \) and \( \lcal \) is convex, we have \begin{align*} \lcal(\ts + \Delta) - \lcal(\ts) \geq - \frac{\lambda_n}{2} [\rcal(\Delta_{\mocal}) + \rcal(\Delta_{\mocalp})]. \end{align*} </div> </div> <div class="lemma"> <div class="theorem-title">Lemma 4 in (Negahban et al., 2009) </div> <div class="theorem-contents"> If \( \fcal(\Delta) &gt; 0 \) for all vectors \( \Delta \in \mathbb{K}(\delta) \), then \( \| \hd \| \leq \delta \). </div> </div> <p>Note that this was similar to our previous analysis on restricted strong convexity where we only really need to consider error terms restricted to \(\ctriplet\) due to <a href="#lemma-1">Lemma 1</a>. Therefore, it suffices to show \(\fcal(\Delta) &gt; 0\) to obtain a bound on \(\| \hd \| = \| \thatlambda - \ts\|\), which completes the proof of Theorem 1.</p> <p>Define \(\fcal : \mathbb{R}^p \to \mathbb{R}\) by</p> \[\begin{align} \fcal(\Delta) \coloneqq \lcal(\ts + \Delta) - \lcal(\ts) + \lambda_n \left\{ \rcal(\ts + \Delta) - \rcal(\ts) \right\}, \end{align}\] <p>and define the set</p> \[\begin{align} \mathbb{K}(\delta) \coloneqq \ctriplet \cap \left\{ \| \Delta \| = \delta \right\}. \end{align}\] <p>Take any \(\Delta \in \kbb\). Then</p> \[\begin{align} \fcal(\Delta) = &amp; \lcal(\ts + \Delta) - \lcal(\ts) + \lambda_n \left\{ \rcal(\ts + \Delta) - \rcal(\ts) \right\} \tag{by definition} \\ \geq &amp; \langle \nabla \lcal (\ts), \Delta \rangle + \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{ \rcal(\ts + \Delta) - \rcal(\ts) \right\} \\ &amp; \qquad \text{(by restricted strong convexity: \(\delta \lcal(\Delta, \ts) \geq \kl \| \Delta \|^2 - \tl^2(\ts)\),} \\ &amp; \qquad \text{ and \( \delta \lcal(\Delta, \ts) = \lcal(\ts + \Delta) - \lcal(\ts) - \dotprod{\nabla \lcal(\ts)}{\Delta} \) ) } \\ \geq &amp; \langle \nabla \lcal (\ts), \Delta \rangle + \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{ \rcal(\Delta_{\mocalp}) - \rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}}) \right\} \\ &amp; \qquad \text{(by Lemma 3)}. \label{thm-deriv:1} \end{align}\] <p>We lower bound the first term as \(\langle \nabla \lcal (\ts), \Delta \rangle \geq - \frac{\lambda_n}{2} \rcal(\Delta)\):</p> \[\begin{align} | \langle \nabla \lcal (\ts), \Delta \rangle | \leq &amp; \rs(\nabla \lcal(\ts)) \rcal(\Delta) &amp; \text{(Cauchy-Schwarz using dual norms \( \rcal \) and \( \rs \))} \\ \leq &amp; \frac{\lambda_n}{2} \rcal(\Delta) &amp; \text{Theorem 1 assumption: \( \lambda_n \geq 2 \rs (\nabla \lcal(\ts)) \))}, \end{align}\] <p>and hence,</p> \[\begin{align} \langle \nabla \lcal (\ts), \Delta \rangle \geq &amp; - \frac{\lambda_n}{2} \rcal(\Delta). \end{align}\] <p>So applying to (\ref{thm-deriv:1}),</p> \[\begin{align} \fcal(\Delta) \geq &amp; \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{ \rcal(\Delta_{\mocalp}) - \rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}}) \right\} - \frac{\lambda_n}{2} \rcal(\Delta) \\ \geq &amp; \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{ \rcal(\Delta_{\mocalp}) - \rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}}) \right\} - \frac{\lambda_n}{2} (\rcal(\Delta_{\mocalp}) + \rcal(\Delta_{\mocal})) \\ &amp; \qquad \text{(Triangle inequality: \( \rcal(\Delta) \leq \rcal(\Delta_{\mocalp}) + \rcal(\Delta_{\mocal}) \))} \\ = &amp; \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{ \frac{1}{2}\rcal(\Delta_{\mocalp}) - \frac{3}{2}\rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}}) \right\} \\ &amp; \qquad \text{(Moving terms in)} \\ \geq &amp; \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{ - \frac{3}{2}\rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}}) \right\} \\ &amp; \qquad \text{(Norms always non-negative)} \\ = &amp; \kl \| \Delta \|^2 - \tl^2(\ts) - \frac{\lambda_n }{2} \left\{ 3 \rcal(\Delta_{\mocal}) + 4 \rcal(\ts_{\mcal^{\perp}}) \right\} \label{eq:r-delta-lb} . \end{align}\] <p>To bound the term \(\rcal(\Delta_{\mocal})\), recall the definition of subspace compatibility:</p> \[\begin{align} \varPsi (\mcal) \coloneqq \sup_{u \in \mcal \setminus \left\{ 0 \right\}} \frac{\rcal(u)}{\| u \|}, \label{eq:r-delta-ub} \end{align}\] <p>and hence</p> \[\begin{align} \rcal(\Delta_{\mocal}) \leq \varPsi(\mocal) \| \Delta_{\mocal} \|. \end{align}\] <p>To upper bound \(\| \Delta_{\mocal} \|\), we have</p> \[\begin{align} \| \Delta_{\mocal} \| &amp; = \| \Pi_{\mocal} (\Delta) - \Pi_{\mocal}(0) \| &amp; \text{(Since \(0 \in \mocal \), \( \Pi_{\mocal}(0) = 0 \)) } \\ &amp; \leq \| \Delta - 0 \| &amp; \text{(Projection operator is non-expansive, see Equation \ref{eq:non-expansive})} \\ &amp; = \| \Delta \|, \end{align}\] <p>which substituting into Equation (\ref{eq:r-delta-ub}) gives</p> \[\begin{align} \rcal(\Delta_{\mocal}) \leq \varPsi(\mocal) \| \Delta \|. \end{align}\] <p>Now we can use this result to lower bound Equation \ref{eq:r-delta-lb}:</p> \[\begin{align} \fcal (\Delta) \geq &amp; \kl \| \Delta \|^2 - \tl^2(\ts) - \frac{\lambda_n }{2} \left\{ 3 \varPsi(\mocal) \| \Delta \| + 4 \rcal(\ts_{\mcal^{\perp}}) \right\}. \label{eq:strict-psd} \end{align}\] <p>The RHS of the inequality in Equation \ref{eq:strict-psd} has a strictly positive definite quadratic form in \(\| \Delta \|\), and hence by taking \(\| \Delta \|\) large, it will be strictly positive. To find such a sufficiently large \(\| \Delta \|\), write</p> \[\begin{align} a &amp; = \kl, \\ b &amp; = \frac{3\lambda_n}{2} \varPsi (\mocal), \\ c &amp; = \tau_{\lcal}^2 (\ts) + 2 \lambda_n \rcal(\ts_{\mcalp}), \\ \end{align}\] <p>such that we have</p> \[\begin{align} \fcal (\Delta) &amp; \geq a \| \Delta \|^2 - b \| \Delta \| - c. \end{align}\] <p>Then the square of the rightmost intercept is given by the squared quadratic formula</p> \[\begin{align} \| \Delta \|^2 &amp; = \left( \frac{-(-b) + \sqrt{b^2 - 4a(-c)}}{2a} \right)^2 \\ &amp; = \left( \frac{b + \sqrt{b^2 + 4ac}}{2a} \right)^2 \\ &amp; \leq \left( \frac{\sqrt{b^2 + 4ac}}{a} \right)^2 &amp; \text{($b \leq \sqrt{b^2 + 4ac}$)} \label{eq:coarse-bound} \\ &amp; = \frac{b^2 + 4ac}{a^2} \\ &amp; = \frac{9 \lambda_n^2 \varPsi^2 (\mocal)}{4 \kl^2} + \frac{ 4 \tau_{\lcal}^2 (\ts) + 8 \lambda_n \rcal(\ts_{\mcalp}) }{\kl}. &amp; \text{(Substituting in \(a, b, c\))} \\ \end{align}\] <p>In <a href="https://arxiv.org/abs/1010.2731">(Negahban et al., 2009)</a>, they were able to show an upper bound of</p> \[\begin{align} \| \Delta \|^2 &amp; \leq \frac{9 \lambda_n^2 \varPsi^2 (\mocal)}{\kl^2} + \frac{\lambda_n}{\kl} \left\{ 2\tau_{\lcal}^2 (\ts) + 4 \rcal(\ts_{\mcalp}) \right\}, \label{eq:ub} \end{align}\] <p>but I did not manage to figure out how they managed to produce a \(\lambda_n\) term beside the \(\tl^2(\ts)\) term. All other differences are just constant factors. It may be due to an overly coarse bound on my end applied in Equation \ref{eq:coarse-bound}, but it is unclear to me how the \(\lambda_n\) term can be applied on only the \(\tl^2(\ts)\) term without affecting the \(\rcal(\ts_{\mcalp})\) term.</p> <p>With Equation \ref{eq:ub}, we can hence apply Lemma 4 in <a href="https://arxiv.org/abs/1010.2731">(Negahban et al., 2009)</a> to obtain the desired result that</p> \[\begin{align} \| \thatlambda - \ts \|^2 \leq 9 \frac{\lambda_n^2}{\kl^2} \Psi^2(\mocal) + \frac{\lambda_n}{\kl} \left( 2 \tl^2 (\ts) + 4 \rcal (\ts_{\mcal^{\perp}}) \right). \end{align}\] <p>This concludes the proof.</p> <h1 id="conclusion">Conclusion</h1> <p>In the <a href="#proof-of-theorem-1">proof of Theorem 1</a>, we saw how the bound is derived from the two key ingredients of the decomposability of the regularizer, and restricted strong convexity of the loss function. The decomposability of the regularizer allowed us to ensure that the error vector \(\hd\) will stay in the set \(\ctriplet\). This condition is then required in Lemma 4 of <a href="https://arxiv.org/abs/1010.2731">(Negahban et al., 2009)</a>, which allows us to bound \(\| \hd \|\) given that \(\fcal(\Delta) &gt; 0\). In one of the steps where we were lower bounding \(\fcal(\Delta)\) in the proof, we made use of the properties of restricted strong convexity.</p> <p><a href="#thm-1">Theorem 1</a> provides a family of bounds for each decomposable regularizer under the choice of \((\mcal, \mocalp)\). The authors of <a href="https://arxiv.org/abs/1010.2731">(Negahban et al., 2009)</a> were able to use <a href="#thm-1">Theorem 1</a> to rederive both existing known results, and also derive new results on low-rank matrix estimation using the nuclear norm, minimax-optimal rates for noisy matrix completion, and noisy matrix decomposition. The reader is encouraged to refer to <a href="https://arxiv.org/abs/1010.2731">(Negahban et al., 2009)</a> for more details on the large number of corrollaries of <a href="#thm-1">Theorem 1</a>.</p> <h1 id="acknowledgments">Acknowledgments</h1> <p>I would like to thank my dear friend <a href="https://www.linkedin.com/in/josh-abrams-78a4a6134/">Josh Abrams</a> for helping to review and provide valuable suggestions for this post!</p> <h1 id="citations">Citations</h1> <ol> <li>Negahban, S., Yu, B., Wainwright, M. J., and Ravikumar, P. <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf">A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers</a>. In Bengio, Y., Schuurmans, D., Lafferty, J., Williams, C., and Culotta, A. (eds.), Advances in Neural Information Processing Systems, volume 22. Curran Associates, Inc., 2009. URL https://proceedings.neurips.cc/paper_files/paper/2009/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf.</li> </ol>]]></content><author><name>fanpu</name></author><category term="statistics"/><category term="machine-learning"/><summary type="html"><![CDATA[Imagine doing high-dimensional statistical inference, but instead of repeatedly studying different settings with specific low-dimensional constraints (such as linear regression with sparsity constraints, or estimation of structured covariance matrices), there is a method for performing a unified analysis using appropriate notions. Well, you're in luck! 'A Unified Framework for High-Dimensional Analysis of \( M \)-Estimators with Decomposable Regularizers' by Negahban, Ravikumar, Wainwright, and Yu shows that the \( \ell_2 \) difference between any regularized \(M\)-estimator and its true parameter can be bounded if the regularization function is decomposable, and the loss function satisfies restricted strong convexity. The goal of this post is to provide intuition for the result and develop sufficient background for understanding the proof of this result, followed by a walkthrough of the proof itself.]]></summary></entry><entry><title type="html">a post with bibliography</title><link href="https://fanpu.io/blog/2023/post-bibliography/" rel="alternate" type="text/html" title="a post with bibliography"/><published>2023-07-12T13:56:00+00:00</published><updated>2023-07-12T13:56:00+00:00</updated><id>https://fanpu.io/blog/2023/post-bibliography</id><content type="html" xml:base="https://fanpu.io/blog/2023/post-bibliography/"><![CDATA[<p>This post shows how to add bibliography to simple blog posts. If you would like something more academic, check the <a href="/blog/2021/distill/">distill style post</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="bib"/><summary type="html"><![CDATA[an example of a blog post with bibliography]]></summary></entry><entry><title type="html">a post with jupyter notebook</title><link href="https://fanpu.io/blog/2023/jupyter-notebook/" rel="alternate" type="text/html" title="a post with jupyter notebook"/><published>2023-07-04T12:57:00+00:00</published><updated>2023-07-04T12:57:00+00:00</updated><id>https://fanpu.io/blog/2023/jupyter-notebook</id><content type="html" xml:base="https://fanpu.io/blog/2023/jupyter-notebook/"><![CDATA[<p>To include a jupyter notebook in a post, you can use the following code:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{::nomarkdown}
{% assign jupyter_path = "assets/jupyter/blog.ipynb" | relative_url %}
{% capture notebook_exists %}{% file_exists assets/jupyter/blog.ipynb %}{% endcapture %}
{% if notebook_exists == "true" %}
    {% jupyter_notebook jupyter_path %}
{% else %}
    <span class="nt">&lt;p&gt;</span>Sorry, the notebook you are looking for does not exist.<span class="nt">&lt;/p&gt;</span>
{% endif %}
{:/nomarkdown}
</code></pre></div></div> <p>Let‚Äôs break it down: this is possible thanks to <a href="https://github.com/red-data-tools/jekyll-jupyter-notebook">Jekyll Jupyter Notebook plugin</a> that allows you to embed jupyter notebooks in your posts. It basically calls <a href="https://nbconvert.readthedocs.io/en/latest/usage.html#convert-html"><code class="language-plaintext highlighter-rouge">jupyter nbconvert --to html</code></a> to convert the notebook to an html page and then includes it in the post. Since <a href="https://jekyllrb.com/docs/configuration/markdown/">Kramdown</a> is the default Markdown renderer for Jekyll, we need to surround the call to the plugin with the <a href="https://kramdown.gettalong.org/syntax.html#extensions">::nomarkdown</a> tag so that it stops processing this part with Kramdown and outputs the content as-is.</p> <p>The plugin takes as input the path to the notebook, but it assumes the file exists. If you want to check if the file exists before calling the plugin, you can use the <code class="language-plaintext highlighter-rouge">file_exists</code> filter. This avoids getting a 404 error from the plugin and ending up displaying the main page inside of it instead. If the file does not exist, you can output a message to the user. The code displayed above outputs the following:</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/blog.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Note that the jupyter notebook supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="jupyter"/><summary type="html"><![CDATA[an example of a blog post with jupyter notebook]]></summary></entry><entry><title type="html">The CMU Steam Tunnels and Wean 9</title><link href="https://fanpu.io/blog/2023/cmu-steam-tunnels/" rel="alternate" type="text/html" title="The CMU Steam Tunnels and Wean 9"/><published>2023-06-16T00:00:00+00:00</published><updated>2023-06-16T00:00:00+00:00</updated><id>https://fanpu.io/blog/2023/cmu-steam-tunnels</id><content type="html" xml:base="https://fanpu.io/blog/2023/cmu-steam-tunnels/"><![CDATA[<p>If you‚Äôre curious about the infamous steam tunnels at CMU, or what the views from the roof of Wean Hall looks like, this is for you! The week after course finals concluded, <a href="https://www.cmu.edu/student-affairs/slice/">CMU SLICE</a> (Student Leadership, Involvement, and Civic Engagement) organized an Underground Steam Tunnels Tour as a Senior Week event. I was technically not a senior as I am a graduate student, but they were nice enough to let me join.</p> <p>Before I continue, let me warn readers that you are not allowed to enter the steam tunnels by yourself. Please sign up for an official tour by SLICE that will be led by a facilities engineer. From <a href="https://www.cmu.edu/student-affairs/theword/community-policies/steam-tunnels.html">The Word Student Handbook</a>:</p> <blockquote class="block-danger"> <h4 id="steam-tunnels">Steam Tunnels</h4> <p>Because of the danger to all who enter them, the steam tunnels are locked and anyone found in the tunnels will be subject to serious disciplinary action and/or criminal action. The University Police are responsible for keeping the tunnels locked and apprehending anyone who trespasses in them.</p> </blockquote> <h1 id="the-steam-tunnels">The Steam Tunnels</h1> <p>We met at the fence, and all of us had to sign a waiver and don a helmet (the same white helmet that was used by builders during Spring Carnival booth).</p> <p>We proceeded to the basement of Margaret Morrison Hall, and the engineer guiding the expedition shared some history about how the Margaret Morrison basement was enhanced to be flood-resistant after a flooding incident a few decades ago caused water to also flood into the steam tunnels.</p> <p>We were warned that the tunnels will be hot and claustrophobic, that we should not touch any pipes as they will be extremely hot, and to not poke at any asbestos for obvious reasons. He then unlocked an unmarked door, and let us into the tunnels proper:</p> <figure> <picture> <img src="/assets/img/posts/cmu_steam_tunnels/main_tunnel.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> Inside the steam tunnel! </figcaption> </figure> <p>It was initially still quite cool near the door, but the temperature began rising as we went further in. Some sections of the pipes were hissing and you could really feel the warmth emanating from them. At some point I was slightly afraid that a pipe beside me might burst.</p> <figure> <picture> <img src="/assets/img/posts/cmu_steam_tunnels/phone_box.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> A phone box in the tunnel. Hopefully no one ever had to use it. </figcaption> </figure> <p>At some point, there was a fork where the tunnel on the left fork became very short and narrow. We took the right fork.</p> <figure> <picture> <img src="/assets/img/posts/cmu_steam_tunnels/side_tunnel.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> The short and narrow left fork of the tunnel </figcaption> </figure> <p>It was generally quite well-lit, until we were brought to a section of the tunnel where we had to ascend a rusty ladder to reach a cavern, which was unlit. It was known as the CFA cavern as it is located right under the steps of CFA:</p> <figure> <picture> <img src="/assets/img/posts/cmu_steam_tunnels/cfa_cavern.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> The CFA cavern, with flash photography </figcaption> </figure> <p>Apparently at some point in the past, a very resourceful CFA student decided to make this their home. Not only was rent cheap (free!), but it was also very close to the CFA building! However, they were found by campus police and booted out.</p> <p>I would not say that it was the most ideal living arrangement. There were plastic bottles strewn everywhere, and stalactites growing down from the ceiling. The air was very damp and musty, and would probably do something bad to your lungs if you stayed in there long enough. It was surprisingly much cooler than the steam tunnels right below it though.</p> <p>We then went back down into the tunnels, and continued on:</p> <figure> <picture> <img src="/assets/img/posts/cmu_steam_tunnels/another_tunnel.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> More tunnels! </figcaption> </figure> <p>As it got close to the end of the tunnels, we were each handed chalk that can be used for leaving our mark in the tunnel. Since public vandalism is <a href="https://en.wikipedia.org/wiki/Vandalism_Act#Michael_Fay_(1994)">punishable by caning</a> in my home country, of course I was not going to pass on this wonderful opportunity to defile the steam tunnels to my heart‚Äôs content:</p> <figure> <picture> <img src="/assets/img/posts/cmu_steam_tunnels/graffiti.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> SLICE-sanctioned vandalism </figcaption> </figure> <p>We then emerged from an exit deep inside Doherty Hall, which was nice as it was beginning to get rather uncomfortable and claustrophobic. Whew!</p> <h1 id="roof-of-wean">Roof of Wean</h1> <p>If you thought there were only 8 levels in Wean, then you will learn something new today. We took the freight elevator from the corner of Wean to floor PH (penthouse?), AKA Wean 9.</p> <p>Wean 9 was essentially a huge storeroom for CMU FMS (Facility Management Services). There were all sorts of supplies and tools, and even spare doors for classrooms. It made me realize just how much maintenance it took to operate a campus.</p> <figure> <picture> <img src="/assets/img/posts/cmu_steam_tunnels/wean_9.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> Our guide leading us through Wean 9 </figcaption> </figure> <p>We were finally brought to a door that led to the roof of Wean Hall, and had to adjust our eyes for a few seconds to the new blinding sunlight. It was beautiful!</p> <figure> <picture> <img src="/assets/img/posts/cmu_steam_tunnels/hammerschlag.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> A majestic view of Hammerschlag Hall. I wish I could take my graduation pictures from here. </figcaption> </figure> <p>Everyone got busy snapping photos, myself included. To my knowledge this is the only place on campus where you can take a side-by-side photo with the Hammerschlag radio tower:</p> <figure> <picture> <img src="/assets/img/posts/cmu_steam_tunnels/roof_photo.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> The Hammerschlag tower, which houses the Carnegie Tech Radio Club (W3VC) and contains both repeaters and transmitters. </figcaption> </figure> <figure> <picture> <img src="/assets/img/posts/cmu_steam_tunnels/cic.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> An interesting view of CIC and Tepper </figcaption> </figure> <figure> <picture> <img src="/assets/img/posts/cmu_steam_tunnels/gates.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> Gates, which is usually perceived to tower over all other campus buildings in its vicinity, look short from here </figcaption> </figure> <p>There was some open space on the roof, and I thought it would be pretty cool if they opened an open-air cafe here. It has pretty nice panoramic views of the entire campus.</p> <p>And that‚Äôs it for the tour!</p> <h1 id="acknowledgments">Acknowledgments</h1> <p>I would like to thank <a href="https://www.cmu.edu/student-affairs/slice/">SLICE</a> for organizing this trip, my friend <a href="https://www.linkedin.com/in/justin-sun-92b691169/">Justin Sun</a> who also went on this little adventure with me for proofreading this post, and <a href="https://www.linkedin.com/in/joseph-x-li/">Joey Li</a> for pointing out a mistake in the post, where I previously erroneously claimed that WRCT 88.3FM was also broadcast from Hammerschlag tower.</p>]]></content><author><name>fanpu</name></author><category term="general"/><category term="cmu"/><summary type="html"><![CDATA[If you're curious about the infamous steam tunnels at CMU, or what the views from the roof of Wean Hall looks like, this post is for you!]]></summary></entry><entry><title type="html">CMU 15712 Advanced Operating Systems and Distributed Systems Course Review</title><link href="https://fanpu.io/blog/2023/advanced-operating-systems-course-review/" rel="alternate" type="text/html" title="CMU 15712 Advanced Operating Systems and Distributed Systems Course Review"/><published>2023-06-09T00:00:00+00:00</published><updated>2023-06-09T00:00:00+00:00</updated><id>https://fanpu.io/blog/2023/advanced-operating-systems-course-review</id><content type="html" xml:base="https://fanpu.io/blog/2023/advanced-operating-systems-course-review/"><![CDATA[<p>This semester (Spring 2023), I took <a href="https://www.cs.cmu.edu/~15712/">15-712 Advanced Operating Systems and Distributed Systems</a> under professor <a href="http://www.cs.cmu.edu/~gibbons/">Phil Gibbons</a> and his TA and also PhD advisee <a href="http://nicebowlofsoup.com/">Val Choung</a>.</p> <p>This class exceeded my expectations significantly. I found it especially meaningful and apt since this was my last systems class before I graduate, and the topics and discussions from class helped to unify all the systems concepts that I had learnt from previous classes into a nice package informed by common underlying principles: from distributed systems, to networking, databases, filesystems, operating systems, and even machine learning systems.</p> <h1 id="the-first-lecture">The First Lecture</h1> <p>The first lecture went through 2 Wisdom Papers, which no one was expected to have read yet as it was the first class. You can refer to the <a href="https://www.cs.cmu.edu/~15712/lectures/01-intro.pdf">slides here</a> if you are curious.</p> <p>The first paper, <a href="https://www.cs.cmu.edu/~15712/papers/mythicalmanmonth00fred.pdf">Mythical Man-Month: Essays on Software Engineering</a> is a book by Turing-award winner Fred Brooks. It is about many of his observations and principles on software engineering based on his own vast experiences. What really brought it home to me was that a couple of them were also things that I had some suspicions about previously, but never really thought it was universally applicable, and thought they were simply artifacts of the way I approached things.</p> <p>For instance, one of the principles is ‚ÄúPlan to Throw One Away‚Äù, meaning that one should first build a worthwhile system in a short amount of time, and then re-build a better second version with the benefit of hindsight. This is because one would end up having to re-build the system anyway after being confronted with change and feedback, and also due to the following observation on program maintenance:</p> <blockquote> <p>‚ÄúProgram maintenance is an entropy-increasing process, and even its most skillful execution only delays the subsidence of the system into unfixable obsolescence‚Äù</p> </blockquote> <p>This had many parallels with my own experiences. For instance, my group ended up having 4 major re-writes of our kernel during 15-410, and I also did a complete re-write of my CloudFS filesystem for my 18-746 project. Similarly, many of my internship projects were also re-writes and improvements on design of existing systems that had accumulated too much technical debt. It does seem a lot more reasonable to plan for this eventual change to begin with.</p> <p>The paper also contained a lot of other great advice, such as the importance of conceptual integrity to separate architecture from implementation, structuring a team in a ‚Äúsurgical‚Äù fashion to drive software development where the best programmer leads the most critical development work like a surgeon and directs others on the other aspects, and of course the famous Brook‚Äôs law:</p> <blockquote> <p>‚ÄúAdding manpower to a late software project makes it later‚Äù</p> </blockquote> <p>The second paper, <a href="https://www.cs.cmu.edu/~15712/papers/hamming86.pdf">You and Your Research</a> by Richard Hamming (of Hamming code fame), talks about how to become a great scientist. The following two slides gives a good sense of the spirit of the paper:</p> <figure> <picture> <img src="/assets/img/posts/adv_os/slide_1.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> How to be a Great Scientist (1) </figcaption> </figure> <figure> <picture> <img src="/assets/img/posts/adv_os/slide_2.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> How to be a Great Scientist (2) </figcaption> </figure> <p>I mention the first lecture and the two papers that were discussed here not simply because they were interesting, but because they helped to set the tone and expectations for the rest of the semester going forward. The message is clear: this is going to be a practical and useful class that will help you on your journey to becoming great systems designers and researchers.</p> <h1 id="course-content">Course Content</h1> <p>The class took us on a whirlwind tour through many <a href="https://www.sigops.org/awards/hof/">SIGOPS Hall of Fame papers</a>, which the award description states was ‚Äúinstituted in 2005 to recognize the most influential Operating Systems papers that were published at least ten years in the past‚Äù. Reading through the papers helped to consolidate a lot of the knowledge that I learned in previous systems classes, and it was cool to see how decades ago many of these ideas that were once unappreciated or heavily criticized now form the bedrock of many of the systems that we use today.</p> <p>In addition to the Hall of Fame papers, there were also several relatively recent papers that the course staff thought were conceptually interesting and promising.</p> <p>The following sections will go through each of the modules and the required papers that you will read (refer to the <a href="https://www.cs.cmu.edu/~15712/syllabus.html">course website</a> if you are also interested in the optional papers), and a short description of what the paper is about so you can get a pretty good sense of what is covered. A cool thing to note is that the scope of all the papers will touch almost all the systems classes offered at CMU.</p> <h2 id="part-1-concurrency-ordering-races">Part 1: Concurrency, Ordering, Races</h2> <ul> <li><a href="https://www.cs.cmu.edu/~15712/papers/birrell84.pdf">Implementing Remote Procedure Calls (Birrell‚Äô84), SigOps HoF paper</a> - introduced the now-standard design of RPC calls with interfaces and caller/callee stubs for distributed communication.</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/lamport78.pdf">Time, Clocks, and the Ordering of Events in a Distributed System (Lamport‚Äô78), SigOps HoF paper</a> - introduced Lamport clocks, the foundation for ordering distributed systems today</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/chandy85.pdf">Distributed Snapshots: Determining Global States of Distributed Systems (Chandy‚Äô85), SigOps HoF paper</a> - how to snapshot a consistent global state in a distributed system</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/li19.pdf">Efficient Scalable Thread-Safety-Violation Detection (Li‚Äô19), SOSP‚Äô19 best paper</a> - using active testing to discover and reproduce concurrency bugs</li> </ul> <h2 id="part-2-file-systems-and-disks">Part 2: File Systems and Disks</h2> <ul> <li><a href="https://www.cs.cmu.edu/~15712/papers/mckusick84.pdf">A Fast File System for UNIX (McKusick‚Äô84), SigOps HoF paper</a> - addresses many of the problems of the original Unix filesystem by introducing the Fast File System (FFS), and many of its ideas are now staple in modern filesystems like the Linux <code class="language-plaintext highlighter-rouge">ext*</code> filesystems</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/howard88.pdf">Scale and Performance in a Distributed File System (Howard‚Äô88), SigOps HoF paper</a> - introduces the Andrew File System (AFS) that significantly improved on NFS in terms of scalability. AFS was developed at CMU and is still widely used today.</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/rosenblum92.pdf">The Design and Implementation of a Log-Structured File System (Rosenblum‚Äô92), SigOps HoF paper</a> - introduced log-structured filesystems that support high write throughput which addresses the problem of disk traffic being dominated by slow writes on traditional filesystems</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/patterson88.pdf">A Case for Redundant Arrays of Inexpensive Disks (RAID) (Patterson‚Äô88), SigOps HoF paper</a> - introduced RAID, solved problem of how to get cheap fault tolerance</li> </ul> <h2 id="part-3-transactions-and-databases">Part 3: Transactions and Databases</h2> <ul> <li><a href="https://www.cs.cmu.edu/~15712/papers/kung81.pdf">On Optimistic Methods for Concurrency Control (Kung‚Äô81), SigOps HoF paper</a> - introduced optimistic concurrency control, now standard in many database environments with low contention and high throughput requirements</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/franklin97.pdf">Concurrency Control and Recovery (Franklin‚Äô97)</a> - survey paper on concurrency control and recovering from crashes in database systems (write-ahead logging, ARIES)</li> </ul> <h2 id="part-4-fault-tolerance">Part 4: Fault Tolerance</h2> <ul> <li><a href="https://www.cs.cmu.edu/~15712/papers/paxos-simple.pdf">Paxos (Lamport‚Äô01), SigOps HoF paper</a> - simplified version of his original theatrical <a href="https://www.cs.cmu.edu/~15712/papers/part-time-parliament.pdf">The Part-Time Parliament</a> paper that was mostly ignored, introduced how to get replicated logs among unreliable (but not Byzantine) nodes</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/castro99.pdf">Practical Byzantine Fault Tolerance (Castro‚Äô99)</a> - distributed consensus but in the presence of Byzantine faults (i.e a fraction of the nodes can collude and behave maliciously)</li> </ul> <h2 id="part-5-os-kernels-and-virtual-machines">Part 5: OS Kernels and Virtual Machines</h2> <ul> <li><a href="https://www.cs.cmu.edu/~15712/papers/liedtke95.pdf">Microkernels (Liedtke‚Äô95), SigOps HoF paper</a> - the first demonstration of an efficient microkernel designed with a very extreme minimality principle, where as much OS functionality is moved outside of the microkernel as possible, including even its memory manager, pagers, device drivers, software TLBs, etc.</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/klein09.pdf">seL4: Formal Verification of an OS Kernel (Klein‚Äô09), SigOps HoF paper</a> - first paper to perform a formal, machine-checked verification of a microkernel using the Isabelle/HOL theorem prover</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/waldspurger02.pdf">Memory Resource Management in VMware ESX Server (Waldspurger‚Äô02), SigOps HoF paper</a> - introduced many great ideas for hypervisor memory management, like memory ballooning, idle tax, and transparent page sharing that are now commonplace in modern virtual machines</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/clements15.pdf">The Scalable Commutativity Rule: Designing Scalable Software for Multicore Processors (Clements‚Äô15), SOSP‚Äô13 best paper</a> - addresses the problem of deciding whether there exists a scalable implementation (with respect to number of processors) of a program based on a restricted form of commutativity of the interfaces that it uses called SIM commutativity (<strong>S</strong>tate-dependent, <strong>I</strong>nterface-based, <strong>M</strong>onotonic)</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/baumann09.pdf">The Multikernel: A new OS architecture for scalable multicore systems (Baumann‚Äô09), SigOps HoF paper</a> - solves the problem of modern operating systems not being able to take advantage of the current trend of increasing core counts due to the inherent limitations of the shared-memory kernel design, by instead re-framing the OS as a distributed system split among different cores with event-driven execution, replicated state, and a hardware-neutral structure</li> </ul> <h2 id="part-6-big-data-systems">Part 6: Big Data Systems</h2> <ul> <li><a href="https://www.cs.cmu.edu/~15712/papers/decandia07.pdf">Dynamo: Amazon‚Äôs Highly Available Key-value Store (DeCandia‚Äô07), SigOps HoF paper</a> - the secret behind how Amazon can support very high availability with eventual consistency due to extreme business requirements (i.e users should never fail to add an item to their shopping carts), by incorporating established techniques like consistent hashing, sloppy quorums, gossip-based membership protocols, etc</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/corbett12.pdf">Spanner: Google‚Äôs Globally-Distributed Database (Corbett‚Äô12), SigOps HoF paper</a> - built using lessons from <a href="https://cloud.google.com/bigtable">Google Bigtable</a>, Spanner powers Google as their globally replicated transactional database system with 5 nines of availability with several novel ideas like TrueTime to quantify uncertainty in wall clock time and distributed transactions</li> <li><a href="https://www.cs.cmu.edu/~15712/papers/qiao21.pdf">Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning (Qiao‚Äô21), OSDI‚Äô21 best paper</a> - introduces a new notion of goodput that combines throughput and statistical efficiency to evaluate the performance of schedulers for training deep learning systems, with a practical implementation that optimizes both cluster-wide and per-job parameters called Pollux</li> </ul> <h2 id="part-7-emerging-platforms">Part 7: Emerging Platforms</h2> <ul> <li><a href="https://www.cs.cmu.edu/~15712/papers/kim21.pdf">LineFS: Efficient SmartNIC Offload of a Distributed File System with Pipeline Parallelism (Kim‚Äô21), SOSP‚Äô21 best paper</a> - optimizing the performance of distributed file systems (DFS) by decomposing DFS operations into pipelined stages and offloading networked stages to a SmartNIC asynchronously</li> </ul> <h1 id="takeaways-from-the-class">Takeaways From The Class</h1> <p>Here are my thoughts on the key takeaways from the class.</p> <h2 id="class-discussion">Class Discussion</h2> <p>As a seminar-based class, one of the most surprising things for me was how fun and valuable the class discussions were. It was especially enlightening to hear the comments of Ph.D. students who are working in systems and other fields in computer science, who often had very different critiques and opinions of the papers than what I had come up with, which often led me to wonder how they got their perspectives and what their background is like. This was particularly true when someone mentioned glaring deficiencies and problems with the paper that I had completely not even thought of.</p> <p>However, one thing that made me sad was that attendance in class started to fall after the halfway point of the semester. This included quite a few of the students who used to give very insightful and interesting responses and so the diversity of perspectives of the discussions as a whole suffered.</p> <p>While attendance is not strictly enforced, actively participating in the discussions and being engaged in lectures is one of the most valuable takeaways from this class, and positively impacts not just you but also your classmates, and so I would strongly encourage anyone interested in the class to attend all the lectures that you can.</p> <h2 id="tribal-knowledge">Tribal Knowledge</h2> <p>Another aspect of the class that I really appreciated was how Phil taught us a lot of the spirit and tribal knowledge of doing CS research during his lively lectures. These were often presented as off-hand remarks while presenting the context or background of a paper, and provided insight into the zeitgeist of the time, the motivations and challenges that the paper authors faced, and what the authors went on to do in the future based on the impact (or lack of impact at that time) of their work.</p> <p>As someone who has not done a long-term research project with a faculty member but am thinking about possibly doing a Ph.D. in the future, all of these were very valuable wisdom which are not things that you can pick up easily yourself from reading past papers or books. In fact, it almost felt as if I had my own advisor at times.</p> <h2 id="witness-the-evolution-of-systems-research">Witness the Evolution of Systems Research</h2> <p>As you read through the papers, you almost feel as if you are being put into the driver‚Äôs seat and can see how systems research has matured and evolved over the past few decades. Seminal papers of the past tackled the most general problems, although many of them lacked implementations or proper benchmarks that would surely be grounds to be red-flagged and rejected from any systems conference today. Many of the more recent papers strive to anticipate and build for future changes in the computation landscape, have solid replicable implementations and evaluations, and are a lot more careful about anticipating and providing rebuttals for criticisms.</p> <h2 id="personal-attention-for-projects">Personal Attention for Projects</h2> <p>I also really appreciated the personal attention that Phil and Val gave to us by meeting with us every other week for our course projects. This is especially so if you consider that many advisors already have trouble meeting their own Ph.D. students for an hour a week, whereas in this case the course staff dedicated half an hour every two weeks for every single group in the class (there were around 10), which I thought was some real dedication. I will admit that I did not live up to my end of the bargain by spending as much time on the project as I would have wanted to (compared to when I took 15-410). One could always give excuses for anything so you don‚Äôt have to listen to mine, but if I had to reflect on it, it was due to a combination of high workloads from other classes, the fact that this was not the highest priority for the members in our group (my project partners were both quite busy with their own research and I was busy with other classes), and some unexpected obstacles in our project that forced us back to the drawing boards a few times (our project interim report was drastically different from our initial proposal).</p> <h2 id="fantastic-course-staff">Fantastic Course Staff</h2> <p>Phil is a really good lecturer. He is very clear, the class pacing is great, and the lecture slides are polished. He is very approachable and respectful towards students, and puts in great effort to give a good and satisfying answer to every question.</p> <p>Feedback for projects is prompt (there was no feedback for the paper summaries), and the midterms were graded fairly quickly.</p> <p>Overall it is clear that the class is pedagogically mature and has benefited from many rounds of feedback during past iterations. It is rich in content, is accessible and yet challenging to students from a wide range of backgrounds, and will prepare one well for building systems in the future, be it in academia or industry.</p> <h1 id="course-structure">Course Structure</h1> <p>There are three main components to the class: paper summaries, projects, and exams.</p> <h2 id="1-paper-summaries">1. Paper Summaries</h2> <p>Before each lecture, the class is assigned a required reading and an optional reading. A paper summary of the required reading must be submitted before the class, which will discuss both readings.</p> <p>The paper summary will contain 3 things:</p> <ol> <li>The 3 most important things in the paper,</li> <li>1 most glaring deficiency of the paper (even highly celebrated papers have faults!),</li> <li>A conclusion on how you will use lessons from this paper to inform you on how you will build systems in the future.</li> </ol> <p>It took me on average 2-4 hours to read each paper and around 15 minutes for the summary.</p> <p>The lectures for this class are front-loaded, meaning that during the first two-thirds of the semester, you will meet 3 times a week for 80 minutes each, while there will be no lectures at all during the final third of the semester, and so ‚Äúon average‚Äù throughout the semester you will meet twice a week. This is so that students have enough knowledge and content to begin working on their course projects early on in the semester.</p> <p>There will be 3 short breaks in each lecture, where all students will get into breakout groups and share and discuss among themselves one of the prompts for the paper based on their paper summaries. Afterwards, all groups are invited to share what they thought.</p> <p>Reading and writing the paper summaries are the only ‚Äúhomework‚Äù you will get in this class.</p> <h2 id="2-course-project">2. Course Project</h2> <p>There is also a semester-long course project with a significant systems component in groups of three. This will begin in earnest after a third of the semester, and all the project groups met with Phil and the TA Val once every two weeks. The deliverables include a project proposal, an interim report, a final presentation, and a final report. The course project will be the largest constituent of your final grade.</p> <h2 id="3-midterms">3. Midterms</h2> <p>Finally, there are two midterm exams, which are taken during class time. The first is taken in the middle of the semester, and the second is taken after all lectures have concluded.</p> <p>Each midterm will cover content from a shortlisted selection of 10 of the required readings. There will be 9 questions on the midterm, which covers 9 of the 10 papers, and you are only required to answer 7 of the problems.</p> <p>The course staff will also provide two past year exams to practice on, though some of the readings may have changed since.</p> <p>It admittedly does seem quite daunting to have to study and be familiar with 10 papers spanning very different topics. I did not have time to actually re-read all 10 papers to prepare for the midterm, and so the way I prepared was to go through all the lecture slides again, re-read the most important sections of the paper, and skim through the rest. Afterward, I attempted the past exams to fill in any gaps that I may have missed. This strategy allowed me to do fairly well on the exam.</p> <h1 id="workload">Workload</h1> <p>The class has a moderate workload for a systems class. Expect to spend 10-12 hours a week on the readings and paper summaries while lectures are ongoing, probably a couple more hours once the projects get into motion midway through the semester, and for it to consume a significant portion of your existence in the last two weeks before the final presentations.</p> <p>It is a far less demanding and stressful class than the legendary <a href="https://www.cs.cmu.edu/~410/">15-410/605 Operating System Design and Implementation</a> class, so don‚Äôt let the ‚Äúadvanced‚Äù in the course title scare you off from taking this class. After all, most people taking this class are Ph.D. students who have their own research to work on and can‚Äôt exactly spend all their time on courses, unlike undergraduates.</p> <h1 id="our-course-project-and-reflections">Our Course Project, and Reflections</h1> <p>Our course project was on the automated optimal scheduling of data in dynamic neural networks over heterogeneous GPUs for inference tasks in a pipeline-parallelism fashion. This means that when a model is too large to fit on a single GPU but instead has to be distributed across multiple GPUs, we aim to solve the problem of finding the optimal way to perform this split in the presence of dynamism in the network. In our case we focused on input dynamism, meaning that the sizes of the inputs can vary, which can result in different execution times in different segments of the network. We built a system called <code class="language-plaintext highlighter-rouge">DynPartition</code>, a reinforcement-learning based scheduler that uses Deep-Q Learning to learn the optimal way of performing this split.</p> <figure> <picture> <img src="/assets/img/posts/adv_os/presentation.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> Giving our final project presentation in the Panther Hollow conference room at CIC </figcaption> </figure> <p>We had some positive empirical results on our benchmarks, but will require additional future work to verify the generality of these results. Overall, I thought it was a great experience working with PhD students and to learn from their working styles and approach to solving problems. It was also really cool to see the breadth and depth of projects presented by the other teams during the final presentation, which was structured like a conference.</p> <h1 id="is-this-class-suitable-for-me">Is This Class Suitable For Me?</h1> <p>I cannot recommend this course enough to anyone who has sufficient background and have an interest in building systems, or systems research.</p> <p>You should be sufficiently prepared for the class if you have taken <a href="https://www.cs.cmu.edu/~410/">15-410 Operating System Design and Implementation</a>, or any other equivalent rigorous operating systems design class in your undergraduate college. Most papers draw heavily on low-level concepts from operating systems and assume that the reader is familiar with them, and therefore familiarity with these ideas is critical to understanding the papers.</p> <p>I don‚Äôt feel any of the other classes are as critical, as any new concepts can be picked up relatively easily. For instance, a good grasp of considerations involved in operating systems design means that it‚Äôs not too hard to also understand the challenges involved in filesystems or virtual machine design. Having taken other classes would definitely still help to make the papers more approachable though. For instance, the Pollux paper was not very approachable for people who did not have prior exposure to machine learning systems, which led to the course staff deciding not to include that as one of the papers tested for the second midterm.</p> <p>When I took the class, all the students were either Masters or Ph.D. students. Strong undergraduates with sufficient background would also definitely do well in the class.</p> <h1 id="why-i-took-this-class">Why I Took This Class</h1> <p>I had to take a systems class this semester to fulfill my graduation requirements for the MSCS program. I initially did include this class in my shortlist of systems classes to take, but then thought it was just going to be a paper reading class (not that I had been in one of such classes before, but it just did not sound very interesting and felt like something I could do by myself asynchronously after I graduate) and therefore was quite hesitant to take it.</p> <p>As such, during registration week I settled on <a href="https://www.cs.cmu.edu/~418/">15-618 Parallel Computer Architecture and Programming</a>, since it included topics on GPU programming that aligned with my current interests in machine learning. However, I did not feel like the class was sufficiently challenging for me after the first lecture, as it was a bit too slow-paced and simple for my liking as I already had exposure to most of the topics from other system classes that I had taken. I decided to switch to 15-712, and I knew immediately that it was the right class for me after the first lecture.</p> <p>In a sense, this class was a hidden gem and I was really glad that I ended up taking it.</p> <h1 id="acknowledgments">Acknowledgments</h1> <p>I would like to express my gratitude to <a href="https://adbforlife.github.io/">Albert Gao</a> for helping to proofread this article, who took the class with me this semester.</p>]]></content><author><name>fanpu</name></author><category term="courses"/><category term="cmu"/><category term="systems"/><summary type="html"><![CDATA[15-712 Advanced OS was an excellent seminar-based graduate course that took us on a whirlwind tour through many of the most seminal SigOps Hall of Fame papers across several systems domains. It will prepare you to be a great systems designer and researcher. In this post, I will share my experience in the class, the course structure and content, what I thought were the biggest takeaways, and who this class might be suitable for.]]></summary></entry><entry><title type="html">Score-Based Diffusion Models</title><link href="https://fanpu.io/blog/2023/score-based-diffusion-models/" rel="alternate" type="text/html" title="Score-Based Diffusion Models"/><published>2023-06-07T00:00:00+00:00</published><updated>2023-06-07T00:00:00+00:00</updated><id>https://fanpu.io/blog/2023/score-based-diffusion-models</id><content type="html" xml:base="https://fanpu.io/blog/2023/score-based-diffusion-models/"><![CDATA[<p>\(\newcommand{\E}{\mathbb{E}} \newcommand{\bone}{\boldsymbol{1}} \newcommand{\bbeta}{\boldsymbol{\beta}} \newcommand{\bdelta}{\boldsymbol{\delta}} \newcommand{\bepsilon}{\boldsymbol{\epsilon}} \newcommand{\blambda}{\boldsymbol{\lambda}} \newcommand{\bomega}{\boldsymbol{\omega}} \newcommand{\bpi}{\boldsymbol{\pi}} \newcommand{\bphi}{\boldsymbol{\phi}} \newcommand{\bvphi}{\boldsymbol{\varphi}} \newcommand{\bpsi}{\boldsymbol{\psi}} \newcommand{\bsigma}{\boldsymbol{\sigma}} \newcommand{\btheta}{\boldsymbol{\theta}} \newcommand{\btau}{\boldsymbol{\tau}} \newcommand{\ba}{\boldsymbol{a}} \newcommand{\bb}{\boldsymbol{b}} \newcommand{\bc}{\boldsymbol{c}} \newcommand{\bd}{\boldsymbol{d}} \newcommand{\be}{\boldsymbol{e}} \newcommand{\boldf}{\boldsymbol{f}} \newcommand{\bg}{\boldsymbol{g}} \newcommand{\bh}{\boldsymbol{h}} \newcommand{\bi}{\boldsymbol{i}} \newcommand{\bj}{\boldsymbol{j}} \newcommand{\bk}{\boldsymbol{k}} \newcommand{\bell}{\boldsymbol{\ell}} \newcommand{\bm}{\boldsymbol{m}} \newcommand{\bn}{\boldsymbol{n}} \newcommand{\bo}{\boldsymbol{o}} \newcommand{\bp}{\boldsymbol{p}} \newcommand{\bq}{\boldsymbol{q}} \newcommand{\br}{\boldsymbol{r}} \newcommand{\bs}{\boldsymbol{s}} \newcommand{\bt}{\boldsymbol{t}} \newcommand{\bu}{\boldsymbol{u}} \newcommand{\bv}{\boldsymbol{v}} \newcommand{\bw}{\boldsymbol{w}} \newcommand{\bx}{\boldsymbol{x}} \newcommand{\by}{\boldsymbol{y}} \newcommand{\bz}{\boldsymbol{z}} \newcommand{\bA}{\boldsymbol{A}} \newcommand{\bB}{\boldsymbol{B}} \newcommand{\bC}{\boldsymbol{C}} \newcommand{\bD}{\boldsymbol{D}} \newcommand{\bE}{\boldsymbol{E}} \newcommand{\bF}{\boldsymbol{F}} \newcommand{\bG}{\boldsymbol{G}} \newcommand{\bH}{\boldsymbol{H}} \newcommand{\bI}{\boldsymbol{I}} \newcommand{\bJ}{\boldsymbol{J}} \newcommand{\bK}{\boldsymbol{K}} \newcommand{\bL}{\boldsymbol{L}} \newcommand{\bM}{\boldsymbol{M}} \newcommand{\bN}{\boldsymbol{N}} \newcommand{\bP}{\boldsymbol{P}} \newcommand{\bQ}{\boldsymbol{Q}} \newcommand{\bR}{\boldsymbol{R}} \newcommand{\bS}{\boldsymbol{S}} \newcommand{\bT}{\boldsymbol{T}} \newcommand{\bU}{\boldsymbol{U}} \newcommand{\bV}{\boldsymbol{V}} \newcommand{\bW}{\boldsymbol{W}} \newcommand{\bX}{\boldsymbol{X}} \newcommand{\bY}{\boldsymbol{Y}} \newcommand{\bZ}{\boldsymbol{Z}} \newcommand{\boldf}{\boldsymbol{f}} \newcommand{\bpi}{\boldsymbol{\pi}} \newcommand{\btheta}{\boldsymbol{\theta}} \DeclareMathOperator{\tr}{tr} \newcommand{\pdata}{p_{\text{data}}(\bx)} \newcommand{\st}{\mathbf{s}_\mathbf{\theta}} \newcommand{\xt}{\tilde{\bx}} \newcommand{\stx}{\mathbf{s}_\mathbf{\theta}(\bx)} \newcommand{\sdx}{\mathbf{s}_\text{data}(\bx)} \newcommand{\stxt}{\mathbf{s}_\mathbf{\theta}(\xt, \sigma)} \newcommand{\pv}{p_{\bv}} \newcommand{\score}{\nabla_\bx \log \pdata} \newcommand{\bov}{\bar{\beta}}\) <em>Joint work with <a href="https://www.linkedin.com/in/owen-wang/">Owen Wang</a>.</em></p> <p>There has recently been a flurry of work in score-based diffusion models as part of the broader area of generative models. This is due to the recent success of such score-based methods, which has achieved results comparable to the state-of-the-art of generative adversarial networks (GANs).</p> <p>Past techniques in generative modeling have either relied on the approximation of the partition function of the probability density, or the combination of an implicit network representation of the probability density and adversarial training. The former suffers from having to either constrain the model to make the partition function tractable, or otherwise relies on approximations with surrogate losses that may be inaccurate, and the latter suffers from training instability and mode collapse.</p> <p>Score-based diffusion models try to address the cons of both approaches, and instead, use score-matching to learn a model of the gradient of the log of the probability density function. This allows it to avoid computing the partition function completely.</p> <p>One of the first such approaches that rely on using score-matching to perform generative modeling does so by generating new samples via Langevin dynamics <a href="https://arxiv.org/abs/1907.05600">(Song &amp; Ermon, 2019)</a>. A key observation is that naively applying score-matching is that the model of score function will be inaccurate in areas of low density with respect to the data distribution, which results in improper Langevin dynamics in low-density areas. The solution that was proposed is the injection of noise into the data, which provides additional training signal and increases the dimensionality of the data.</p> <p>The next major step introduced in <a href="https://arxiv.org/abs/2011.13456">(Song et al., 2021)</a> is to perturb the data using a diffusion process which is a form of a stochastic differential equation (SDEs). The SDE is then reversed using annealed Langevin dynamics in order to recover the generative process, where the reversal process makes use of score matching.</p> <p>Other recent refinements that have been proposed include re-casting the objective as a Schr√∂dinger bridge problem, which is an entropy-regularized optimal transport problem. The advantage of this approach is that it allows for fewer diffusion steps to be taken during the generative process.</p> <h1 id="survey-of-results">Survey of Results</h1> <p>We will be primarily focusing on the paper <a href="https://arxiv.org/abs/1907.05600">Generative Modeling by Estimating Gradients of the Data Distribution (Song &amp; Ermon, 2019)</a>.</p> <p>In this section, we provide the necessary background, provide derivations for important results, and explain the key ideas of score matching for diffusion models as proposed in the papers.</p> <h2 id="motivation-for-score-matching">Motivation for Score Matching</h2> <h3 id="limitations-of-likelihood-based-approaches">Limitations of Likelihood-Based Approaches</h3> <p>Score matching is motivated by the limitations of likelihood-based methods. In likelihood-based methods, we use a parameterized model \(f_\theta(\bx) \in \mathbb{R}\) and attempt it to recover the parameters \(\theta\) that best explains the observed data. For instance, in energy-based models, the probability mass function \(p_\theta(\bx)\) would be given as \begin{align} p_\theta(\bx) = \frac{\exp(-f_\theta(\bx))}{Z_\theta}, \end{align} where \(Z_\theta\) is the normalizing constant that causes the distribution to integrate to 1, i.e \begin{align} Z_\theta = \int \exp(-f_\theta(\bx)) \, d \bx. \end{align} The goal then is to maximize the log likelihood of the observed data \(\{\bx_i\}_i^N\), given by \begin{align} \max_\theta \sum_{i=1}^N \log p_\theta (\bx_i). \end{align}</p> <p>It is often computationally intractable to compute the partition function \(Z_\theta\) unless there are restrictions on what the model can be, since there are usually at least an exponential number of possible configurations. Examples of models where the partition function can be efficiently computed include causal convolutions in autoregressive models, and invertible networks in normalizing flow models However, such architecture restrictions are very undesirable as they limit the expressiveness of the models.</p> <p>A likelihood-based approach that tries to avoid computing the partition function is variational inference. In variational inference, we use the Evidence Lower Bound (ELBO) as a surrogate objective, where the approximation error is the smallest Kullback-Leibler divergence between the true distribution and a distribution that can be parameterized by our model.</p> <h3 id="limitations-of-adversarial-based-approaches">Limitations of Adversarial-Based Approaches</h3> <p>Adversarial-based approaches, like generative adversarial networks (GANs), have been shown to suffer from both instability in training and mode collapse.</p> <p>Training GANs can be viewed as finding a Nash equilibrium for a two-player non-cooperative game between the discriminator and the generator. Finding a Nash equilibrium is PPAD-complete which is computationally intractable, and therefore methods like gradient-based optimization techniques are used instead. However, the highly non-convex and high-dimensional optimization landscape means that small perturbations in the parameters of either player can change the cost function of the other player, which results in non-convergence.</p> <p>Another problem with training GANs is that when either the generator or discriminator becomes significantly better than the other, then the learning signal for the other player becomes very weak. For generators, this is when the discriminator is always able to tell it apart. For discriminators, this is when the generator performs so well it can hardly do better than random guessing.</p> <p>Finally, a common failure mode of GANs is mode collapse, where the generator only learns to produce a set of very similar outputs from a single mode instead of from all the modes. This is due to the non-convexity of the optimization landscape.</p> <h2 id="score-matching">Score Matching</h2> <p>Score matching is a non-likelihood-based method to perform sampling on an unknown data distribution, and seeks to address many of the limitations of likelihood-based methods and adversarial methods. This is achieved by learning the score of the probability density function, formally defined below:</p> <div class="definition"> <div class="theorem-title">Definition (Score Function) </div> <div class="theorem-contents"> The score function of a distribution \( \pdata \) is given by \begin{align*} f(\bx) = \nabla_\bx \log \pdata. \end{align*} </div> </div> <p>In practice, we try to learn the score function using a neural network \(\stx\) parameterized by \(\theta\).</p> <p>The objective of score matching is to minimize the Fisher Divergence between the score function and the score network:</p> \[\begin{align} \label{eq:score-matching-target-fisher-div} \argmin_\theta \frac{1}{2} \mathbb{E}_{\pdata} \left[ \| \stx - \nabla_\bx \log \pdata \|_2^2 \right]. \end{align}\] <p>However, the main problem here is that we do not know \(\nabla_\bx \log \pdata\), since it depends on knowing what \(\pdata\) is.</p> <p><a href="http://jmlr.org/papers/v6/hyvarinen05a.html">(Hyv√§rinen, 2005)</a> showed that Equation \ref{eq:score-matching-target-fisher-div} is equivalent to Equation \ref{eq:score-matching-target} below:</p> \[\begin{align} \label{eq:score-matching-target} \argmin_\theta \frac{1}{2} \mathbb{E}_{\pdata} \left[ \tr \left( \nabla_\bx \stx \right) + \frac{1}{2} \| \stx \|_2^2 \right]. \end{align}\] <p>We can now compute this using Monte Carlo methods by sampling from \(\pdata\), since it only depends on knowing \(\stx\).</p> <h2 id="sliced-score-matching">Sliced Score Matching</h2> <p>It is computationally difficult to compute the trace term \(\tr \left( \nabla_\bx \stx \right)\) in Equation \ref{eq:score-matching-target} when \(\bx\) is high-dimensional. This motivates another alternative cheaper approach for score matching, called sliced score matching <a href="http://arxiv.org/abs/1905.07088">(Song et al., 2019)</a>.</p> <p>In sliced score matching, we sample random vectors from some distribution \(\pv\) (such as the multivariate standard Gaussian) in order to optimize an analog of the Fisher Divergence:</p> \[\begin{align} L(\btheta, \pv) = \frac{1}{2} \mathbb{E}_{\pv} \mathbb{E}_{\pdata} \left[ (\bv^T \stx - \bv^T \sdx)^2 \right] \end{align}\] <p>We observe that</p> \[\begin{align} L(\btheta; \pv) &amp;= \frac{1}{2} \mathbb{E}_{\pv} \mathbb{E}_{\pdata} \left[ (\bv^T \stx - \bv^T \sdx)^2 \right]\\ &amp;=\frac{1}{2} \mathbb{E}_{\pv} \mathbb{E}_{\pdata} \left[ (\bv^T \stx )^2 + (\bv^T \sdx)^2 - 2(\bv^T \stx )(\bv^T \sdx) \right]\\ &amp;= \mathbb{E}_{\pv} \mathbb{E}_{\pdata} \left[ \frac{1}{2}(\bv^T \stx )^2 - (\bv^T \stx )(\bv^T \sdx) \right] + C\\ \end{align}\] <p>where the \(\sdx\) term is absorbed into \(C\) as it doesn‚Äôt depend on \(\theta\). Now note</p> \[\begin{align} &amp; -\mathbb{E}_{\pv} \mathbb{E}_{\pdata}\left[(\bv^T \stx )(\bv^T \sdx) \right] \\ =&amp; -\mathbb{E}_{\pv} \int \left[(\bv^T \stx )(\bv^T \sdx) \pdata d\bx\right]\\ =&amp; -\mathbb{E}_{\pv} \left[\int(\bv^T \stx )(\bv^T\nabla_{\bx}\log \pdata)\pdata d\bx\right] \\ =&amp; -\mathbb{E}_{\pv} \left[\int(\bv^T \stx )(\bv^T\nabla_{\bx}\pdata)d\bx\right] \\ =&amp; -\mathbb{E}_{\pv} \left[\int(\bv^T \stx )(\bv^T\nabla_{\bx}\pdata)d\bx\right] \\ =&amp; -\mathbb{E}_{\pv} \left[\sum_{i}\int(\bv^T \stx )(v_i\frac{\partial \pdata}{\partial x_i})d\bx\right] \\ =&amp; \mathbb{E}_{\pv} \left[\int \bv^T\stx\bv \cdot \pdata d\bx\right] \\ =&amp; \mathbb{E}_{\pv}\mathbb{E}_{\pdata}\left[\bv^T\stx\bv \right] \end{align}\] <p>where line 16 is obtained by applying multivariate integration by parts. This finally yields the equivalent objective:</p> \[\begin{align} J(\btheta; \pv) &amp;= \mathbb{E}_{\pv} \mathbb{E}_{\pdata} \left[ \bv^T \nabla_\bx \stx \bv + \frac{1}{2} \| \stx \|_2^2 \right] \end{align}\] <p>which no longer has a dependence on the unknown \(\nabla_{bx}\sdx\). This leads to the unbiased estimator:</p> \[\begin{align} \hat J_{N,M}(\btheta; \pv) &amp;=\frac{1}{N}\frac{1}{M}\sum_{i= 1}^N\sum_{j=1}^M \left[\bv_{ij}^T\nabla_{\bx}\mathbf{s}_\mathbf{\btheta}(\bx_i)\bv_{ij} + \frac{1}{2} \|\mathbf{s}_\mathbf{\btheta}(\bx_i)\|_2^2\right] \end{align}\] <p>where for each data point \(\bx_i\) we draw \(M\) projection vectors from \(\pv\).</p> <p><a href="http://arxiv.org/abs/1905.07088">(Song et al., 2019)</a> showed that under some regularity conditions, sliced score matching is an asymptotically consistent estimator:</p> \[\begin{align} \hat \btheta_{N,M} \overset{p}{\to} \btheta^* \text { as } \mathbb{N} \to \infty \end{align}\] <p>where</p> \[\begin{align} \btheta^* &amp;= \underset{\btheta}{\text{argmin }} J(\btheta; \pv), \\ \hat \btheta_{N,M} &amp;= \underset{\btheta}{\text{argmin }} \hat J_{N,M}(\btheta; \pv). \end{align}\] <p>Sliced score matching is computationally more efficient, since it now only involves Hessian-vector products, and continues to work well in high dimensions.</p> <h2 id="sampling-with-langevin-dynamics">Sampling with Langevin Dynamics</h2> <p>Once we have trained a score network, we can sample from the data distribution via Langevin dynamics. Langevin dynamics is a Markov Chain Monte Carlo method of sampling from a stationary distribution, where we can efficiently take gradients with respect to the probability of our samples \(\bx\). We satisfy this criteria since we have the trained score network.</p> <p>In Langevin dynamics, we start from some initial point \(\bx_0 \sim \bpi(\bx)\) sampled from some prior distribution \(\bpi\), and then iteratively obtain updated points based on the following recurrence: \begin{align} \xt_t = \xt_{t-1} + \frac{\epsilon}{2} \nabla_\bx \log p(\xt_{t-1}) + \sqrt{\epsilon} \bz_t, \end{align} where \(\bz_t \sim \mathcal{N}(0, I)\). The addition of the Gaussian noise is required, or otherwise the process simply converges to the nearest mode instead of converging to a stationary distribution.</p> <p>It can be shown that as \(\epsilon \to 0\) and \(T \to \infty\), we have that the distribution of the process \(\xt_T\) converges to \(\pdata\) <a href="https://dl.acm.org/doi/10.5555/3104482.3104568">(Welling &amp; Teh, 2011)</a>.</p> <h2 id="challenges-of-langevin-dynamics">Challenges of Langevin Dynamics</h2> <p>Langevin dynamics does not perform well with multi-modal distributions with poor conductance, since it will tend to stay in a single mode, which causes long mixing times. This is particularly a problem when the modes have disjoint supports, since there is very weak gradient information in the region where there is no support.</p> <h2 id="challenges-of-score-matching-for-generative-modeling">Challenges of Score Matching for Generative Modeling</h2> <h3 id="the-manifold-hypothesis">The Manifold Hypothesis</h3> <p>The manifold hypothesis postulates that real-world data often lies in a low-dimensional manifold embedded in a high-dimensional space. This has been empirically observed in many datasets.</p> <p>This poses problems for score matching. The first problem that the manifold hypothesis poses is that the score \(\score\) becomes undefined if \(\bx\) actually just lies in a low-dimensional manifold. The second problem is that the estimator in Equation \ref{eq:score-matching-target} is only consistent when the support of \(\pdata\) is that of the whole space.</p> <p>In order to increase the dimension of the data to match that of the ambient space, <a href="http://jmlr.org/papers/v6/hyvarinen05a.html">(Hyv√§rinen, 2005)</a> proposed injecting small amounts of Gaussian noise into the data, such that now the data distribution has full support. As long as the perturbation is sufficiently small (\(\mathcal{N}(0, 0.0001)\) was used in their paper), it is almost indistinguishable to humans.</p> <h3 id="low-data-density-regions">Low Data Density Regions</h3> <p>The other problem with score matching is that it may not be able to learn the score function in areas of low data density. This is due to the lack of samples drawn from these regions, resulting in the Monte Carlo estimation to have high variance.</p> <h2 id="noise-conditional-score-networks-ncsn">Noise Conditional Score Networks (NCSN)</h2> <p>The challenges mentioned in the previous sections are addressed by Noise Conditional Score Networks (NCSN).</p> <p>In NCSN, we define a geometric sequence of \(L\) noise levels \({\left\{ \sigma_i \right\}}_{i=1}^L\), with the property that \(\frac{\sigma_1}{\sigma_2} = \frac{\sigma_{L-1}}{\sigma_L} &gt; 1\). Each of these noise levels correspond to Gaussian noise that will be added to perturb the data distribution, i.e \(q_{\sigma_i} \sim \pdata + \mathcal{N}(0, \sigma_i)\).</p> <p>We augment the score network to also take the noise level \(\sigma\) into account, which is called the NCSN \(\stxt\). The goal of NCSN is then to estimate the score conditioned on the noise level. Once we have a trained NCSN, we use a similar apporach as simulated annealing in Langevin sampling, where we begin with a large noise level in order to cross the different modes easily, before gradually annealing down the noise to achieve convergence.</p> <p>The denoising score matching objective for each noise level \(\sigma_i\) is given as</p> \[\begin{align} \ell(\theta; \sigma) \triangleq \frac{1}{2} \mathbb{E}_{\pdata} \mathbb{E}_{\xt \sim \mathcal{N}(\bx, \sigma^2 I)} \left[ \left\| \stxt + \frac{\xt - \bx}{\sigma^2} \right\|_2^2 \right], \end{align}\] <p>and the unified objective for denoising across all levels is given as</p> \[\begin{align} \mathcal{L}\left(\theta; \left\{ \sigma_i\right\}_{i=1}^L \right) \triangleq \frac{1}{L} \sum_{i=1}^L \lambda(\sigma_i) \ell(\theta; \sigma_i). \end{align}\] <h2 id="score-based-generative-modeling-through-stochastic-differential-equations-song-et-al-2021">Score-Based Generative Modeling through Stochastic Differential Equations <a href="https://arxiv.org/abs/2011.13456">(Song et al., 2021)</a></h2> <p>We can extend the idea of having a finite number of noise scales to having an infinite continuous number of such noise scales by modeling the process as a diffusion process, which can be formalized as a stochastic differential equation (SDE). Such an SDE is given in the following form:</p> \[\begin{align} d\bx = \boldf(\bx, t) \, dt + g(t) \, d\bw. \end{align}\] <p>Here, \(\boldf\) represents the drift coefficient, which models the deterministic part of the SDE, and determines the rate at which the process \(d\bx\) is expected to change over time on average. \(g(t)\) is called the diffusion coefficient, which represents the random part of the SDE, and determines the magnitude of the noising process over time. Finally, \(\bw\) is Brownian motion. Thus \(g(t) \, d \bw\) represents the noising process.</p> <p>We want our diffusion process to be such that \(\bx(0) \sim p_0\) is the original data distribution, and \(\bx(T) \sim p_T\) is the Gaussian noise distribution that is independent of \(p_0\). Then since every SDE has a corresponding reverse SDE, we can start from the final noise distribution and run the reverse-time SDE in order to recover a sample from \(p_0\), given by the following process:</p> \[\begin{align} d \bx = [\boldf (\bx, t) - g(t)^2 \nabla_{\bx} \log_{p_t} (\bx) ] \, dt + g(t) \,d \overline{w}, \end{align}\] <p>where \(\overline{w}\) is Brownian motion that flows backwards in time from \(T\) to \(0\), and \(dt\) is an infinitesimal negative timestep.</p> <p>The objective function for score matching for the SDE is then given by</p> \[\begin{align} \argmin_{\theta} \mathbb{E}_t \left[ \lambda (t) \mathbb{E}_{\bx(0)} \mathbb{E}_{\bx (t) \mid \bx(0)} \left[ \| \bs_\theta (\bx(t), t) - \nabla_{\bx(t)} \log p_{0t}(\bx (t) \mid \bx(0)) \|_2^2 \right] \right]. \end{align}\] <h3 id="score-based-generative-modeling-techniques">Score-based Generative Modeling Techniques</h3> <p><a href="https://arxiv.org/abs/2011.13456">(Song et al., 2021)</a> covers two score-based generative models that uses SDEs to perform generative modeling. The first is called score matching with Langevin dynamics (SMLD), which performs score estimation at different noise scales and then performs sampling using Langevin dynamics with decreasing noise scales. The second is denoising diffusion probabilistic modeling (DDPM)</p> <p><a href="https://arxiv.org/abs/2006.11239">(Ho et al., 2020)</a>, which uses a parameterized Markov chain that is trained with a re-weighted variant of the evidence lower bound (ELBO), which is an instance of variational inference. The Markov chain is trained to reverse the noise diffusion process, which then allows sampling from the chain using standard Markov Chain Monte Carlo techniques.</p> <p><a href="https://arxiv.org/abs/2011.13456">(Song et al., 2021)</a> shows that SMLD and DDPM actually corresponds to discretizations of the Variance Exploding (VE) and Variance Preserving (VP) SDEs, which is the focus of the next two section. We believe expanding on this will be illuminating as it highlights the connections between SDEs and the discretized approaches that are used in practice.</p> <h3 id="smld-as-discretization-of-variance-exploding-ve-sde">SMLD As Discretization of Variance Exploding (VE) SDE</h3> <p>Recall that we use a geometric sequence of \(L\) noise levels \({\left\{ \sigma_i \right\}}_{i=1}^L\). that is added to the data distribution</p> <p>We can recursively define the distribution for each noise level \(i\) by incrementally adding noise:</p> \[\begin{align} \bx_i = \bx_{i-1} + \sqrt{\sigma_i^2 - \sigma_{i-1}^2} \bz_{i-1}, \qquad \qquad i = 1, \dots, L, \end{align}\] <p>where \(\bz_{i-1} \sim \mathcal{N}(\mathbf{0}, \bI)\), and \(\sigma_0 = 0\) so \(\bx_0 \sim \pdata\).</p> <p>If we view the noise levels as gradually changing in time, then the continuous time limit of the process is given by the following SDE: \begin{align} \bx(t + \Delta t) = \bx(t) + \sqrt{\sigma^2 (t + \Delta t ) - \sigma^2 (t)} \bz(t) \approx \bx(t) + \sqrt{\frac{d [\sigma^2 (t)]}{dt} \Delta t } \bz (t), \end{align} where the approximation holds when \(\Delta t \ll 1\). If we take \(\Delta t \to 0\), we recover the VE SDE: \begin{align} d \bx = \sqrt{\frac{d [\sigma^2 (t)]}{dt} } d \bw, \end{align} which causes the variance of \(d \bx(t)\) to go to infinity as \(t \to \infty\) due to its geometric growth, hence its name.</p> <h3 id="ddpm-as-discretization-of-variance-preserving-vp-sde">DDPM As Discretization of Variance Preserving (VP) SDE</h3> <p>Similarly, the Markov chain of the perturbation kernel of DDPM is given by \begin{align} \bx_i = \sqrt{1 - \beta_i} \bx_{i-1} + \sqrt{\beta_i} \bz_{i-1}, \qquad i = 1, \cdots, L, \end{align} where \(\left\{ \beta_i \right\}_{i=1}^L\) are the noise scales, and if we take \(L \to \infty\) with scaled noise scales \(\overline{\beta_i} = N \beta_i\), we get \begin{align} \bx_i = \sqrt{1 - \frac{\bov_i}{N} } \bx_{i-1} + \sqrt{ \frac{\bov_i}{N} } \bz_{i-1}, \qquad i = 1, \cdots, L. \end{align} Now taking limits with \(L \to \infty\), we get \begin{align} \bx(t + \Delta t) \approx \bx(t) - \frac{1}{2} \beta(t) \Delta t \bx(t) + \sqrt{\beta(t) \Delta t} \bz(t), \end{align} where the approximation comes from the second degree Taylor expansion of \(\sqrt{1 - \beta(t + \Delta t) \Delta t}\). Then taking the limit of \(\Delta t \to 0\), we obtain the VP SDE \begin{align} d \bx = - \frac{1}{2} \beta(t) \bx dt + \sqrt{\beta(t)} d \bw. \end{align} This process thus has bounded variance since \(\beta_i\) is bounded.</p> <h1 id="experiments">Experiments</h1> <p>We conduct the following preliminary series of experiments, based on released work by <a href="https://arxiv.org/abs/1907.05600">(Song &amp; Ermon, 2019)</a>.</p> <h2 id="investigating-the-manifold-hypothesis">Investigating the manifold hypothesis</h2> <figure id="fig-1"> <picture> <img src="/assets/img/posts/score-based-diffusion-models/sample_dist.webp" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"><i>Figure 1.</i> Comparison between true data density and sampling </figcaption> </figure> <p>In this experiment, we have plotted the true data density of a toy distribution along with samples drawn in three ways. The i.i.d samples are drawn directly from the underlying distribution and we can see that more samples are drawn in the area of high data density. However, applying Langevin dynamics without annealing, we see that there is an almost equal number of points in the top left and bottom right corners. This is evidence that the sampling method doesn‚Äôt conform to the true distribution. Finally, by injecting and decreasing the amount of noise through the annealing process, we can recover a representative sample of the distribution.</p> <h2 id="importance-of-annealing-when-sampling-via-langevin-dynamics">Importance of annealing when sampling via Langevin Dynamics</h2> <p>To better visualize the effects of annealing when sampling via Langevin Dynamics, we generated images from a model trained on the CelebA dataset. We first tried applying Langevin Dynamics with a fixed noise and then used annealing to gradually decrease the noise.</p> <figure id="fig-2"> <picture> <img src="/assets/img/posts/score-based-diffusion-models/annealing_ablation.webp" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"><i>Figure 2.</i> Langevin Dynamics with no annealing (top) and annealing (bottom) </figcaption> </figure> <p>Figure 2 shows that the results with annealing are significantly clearer and more varied, matching the performance of GANs in 2019.</p> <figure id="fig-3"> <picture> <img src="/assets/img/posts/score-based-diffusion-models/left_right.webp" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"><i>Figure 3.</i> Closer comparison of no annealing (left) and annealing (right) </figcaption> </figure> <p>We notice that the image generated without annealing manages to produce the structure of a human face but fails to capture finer details such as the hair, and the surrounding backdrop. There is also little variation in color between different samples. This is in agreement with our theory that without annealing, Langevin dynamics cannot properly explore regions of lower data density.</p> <h2 id="effect-of-noise-parameters-for-annealed-langevin-dynamics">Effect of noise parameters for annealed Langevin Dynamics</h2> <p>We also investigated the effect of changing the lowest noise standard deviation \(\sigma\) while keeping the number of different noises injected fixed at \(10\). The 10 noise values are determined by an interpolation in log scale.</p> <figure id="fig-4"> <picture> <img src="/assets/img/posts/score-based-diffusion-models/vary_sigma.webp" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"><i>Figure 4.</i> Left to right: \( \sigma_{\text{end}} = \{0.1, 0.01, 0.001\} \) </figcaption> </figure> <p>Our experiment shows that the effect of starting, ending, and the interval between noise values has a significant effect on the convergence of annealed Langevin sampling.</p> <h1 id="discussion-and-future-work">Discussion and Future Work</h1> <p>Having completed a survey of score-based diffusion models, and having run some experiments on them, we now turn our attention to discussing the pros and cons of this approach.</p> <p>As mentioned previously in this paper, the main draw of score-based diffusion models is that it has shown to be capable of generating impressive high-quality samples that is on-par with the state-of-the-art with GANs. We hence focus on its limitations and how they might be overcome, drawing from work in <a href="https://arxiv.org/abs/2209.02646">(Cao et al., 2022)</a>.</p> <h2 id="computation-cost">Computation Cost</h2> <p>A common refrain of score-based diffusion model is the high computational complexity in both training and sampling. This is because it requires thousands of small diffusion steps in order to ensure that the forward and reverse SDEs hold in their approximations <a href="https://arxiv.org/abs/2202.09671">(Zheng et al., 2022)</a>. If the diffusion steps are too large, then the Gaussian noise assumption may not hold, resulting in poor score estimates. This makes it significantly more expensive than other generative methods like GANs and VAEs. To this end, there are some directions being explored to improve its computation cost.</p> <p>The first technique seeks to reduce the number of sampling steps required by a method known as knowledge distillation <a href="http://arxiv.org/abs/1710.07535">(Lopes et al., 2017)</a>. In knowledge distillation, knowledge is transferred from a larger and more complex model (called the teacher), to one that is smaller and simpler (called the student). This technique has found success in other domains such as image classification, and has also been shown to result in improvements in diffusion models <a href="https://arxiv.org/abs/2202.00512">(Salimans &amp; Ho, 2022)</a>. It would be interesting to see how far we can take this optimization.</p> <p>Another technique known as truncated diffusion probabilistic modeling (TDPM) <a href="https://arxiv.org/abs/2202.09671">(Zheng et al., 2022)</a>. In this approach, instead of considering the diffusion process until it becomes pure noise, the process is stopped once it reaches a hidden noisy-data distribution that can be learnt by an auto-encoder by adversarial training. Then in order to produce samples, a sample is first drawn from the learnt noisy-data distribution, before being passed through the reverse-SDE diffusion steps.</p> <p>It also suffers from poor explainability and interpretability, but this is a common problem across other generative models.</p> <p><a href="https://arxiv.org/abs/2011.13456">(Song et al., 2021)</a> also notes that it is currently difficult to tune the myriad of hyperparameters introduced by the choice of noise levels and specific samplers chosen, and new methods to automatically select and tune these hyperparameters would make score-based diffusion models more easily deployable in practice.</p> <h2 id="modality-diversity">Modality Diversity</h2> <p>Diffusion models have mostly only seen applications for generating image data, and its potential for generating other data modalities has not been as thoroughly investigated. <a href="https://arxiv.org/abs/2107.03006">(Austin et al., 2021)</a> introduces Discrete Denoising Diffusion Probabilistic Models (D3PMs), which develops a diffusion process for corrupting text data into noise. It would be interesting to see how well diffusion models can be stretched to perform compared to state-of-the-art transformer models in text generation.</p> <h2 id="dimensionality-reduction">Dimensionality Reduction</h2> <p>Dimensionality reduction is another technique that can be used to speed up training and sampling speeds of diffusion models. Diffusion models are typically trained directly in data space. <a href="https://arxiv.org/abs/2106.05931">(Vahdat et al., 2021)</a> instead proposes for them to be trained in latent space, which results in dimensionality reduction in the representation learnt, and also potentially increases the expressiveness of the framework. In a similar vein, <a href="https://arxiv.org/abs/2211.16032">(Zhang et al., 2022)</a> argues that due to redundancy in spatial data, it is not necessary to learn in data space, and instead proposes a dimensionality-varying diffusion process (DVDP), where the dimensionality of the signal is dynamically adjusted during the both the diffusion and denoising process.</p> <h1 id="conclusion">Conclusion</h1> <p>We showed that score matching presents a promising new direction for generative models, which avoids many of the limitations of other approaches such as training instability and mode collapse in GANs, and poor approximation guarantees in variational inference. While score matching has several flaws, such as suffering from the manifold hypothesis and requiring an expensive Langevin dynamics process in order to draw samples, successive work has done well in addressing these limitations to make score matching on diffusion models a viable contender to displace GANs as the state-of-the-art for generative modeling.</p> <p>Our experiments in this blog post help to provide empirical context to the theoretical results we have derived. Most notably, we have shown how annealing is an essential part of sampling via Langevin dynamics.</p> <p>Finally, we discuss some future directions that can help to improve the viability of using score-based diffusion models, which includes improving its computational cost in both training and sampling and increasing the diversity of applicable modalities.</p> <h1 id="citation">Citation</h1> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{zeng2023diffusion,
  title   = {Score-Based Diffusion Models},
  author  = {Fan Pu Zeng and Owen Wang},
  journal = {fanpu.io},
  year    = {2023},
  month   = {Jun},
  url     = {https://fanpu.io/blog/2023/score-based-diffusion-models/}
}
</code></pre></div></div> <h1 id="references">References</h1> <ul> <li>Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den Berg, R. <a href="https://arxiv.org/abs/2107.03006">Structured denoising diffusion models in discrete state-spaces.</a> CoRR, abs/2107.03006, 2021.</li> <li>Cao, H., Tan, C., Gao, Z., Chen, G., Heng, P.-A., and Li, S. Z. <a href="https://arxiv.org/abs/2209.02646">A survey on generative diffusion model</a>, 2022.</li> <li>Ho, J., Jain, A., and Abbeel, P. <a href="https://arxiv.org/abs/2006.11239">Denoising diffusion probabilistic models</a>. CoRR, abs/2006.11239, 2020. URL https://arxiv.org/abs/2006.11239.</li> <li>Hyva Ãàrinen, A. <a href="http://jmlr.org/papers/v6/hyvarinen05a.html">Estimation of non-normalized statistical models by score matching</a>. Journal of Machine Learning Research, 6(24):695‚Äì709, 2005.</li> <li>Lopes, R. G., Fenu, S., and Starner, T. <a href="http://arxiv.org/abs/1710.07535">Data-free knowledge distillation for deep neural networks</a>. CoRR, abs/1710.07535, 2017.</li> <li>Salimans, T. and Ho, J. <a href="https://arxiv.org/abs/2202.00512">Progressive distillation for fast sampling of diffusion models</a>. CoRR, abs/2202.00512, 2022.</li> <li>Song, Y. and Ermon, S. <a href="http://arxiv.org/abs/1907.05600">Generative modeling by estimating gradients of the data distribution</a>. CoRR, abs/1907.05600, 2019.</li> <li>Song, Y., Garg, S., Shi, J., and Ermon, S. <a href="http://arxiv.org/abs/1905.07088">Sliced score matching: A scalable approach to density and score estimation</a>. CoRR, abs/1905.07088, 2019. URL http://arxiv.org/abs/1905.07088.</li> <li>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. <a href="https://arxiv.org/abs/2011.13456">Score-based generative modeling through stochastic differential equations</a>. ICLR, abs/1907.05600, 2021.</li> <li>Vahdat, A., Kreis, K., and Kautz, J. <a href="https://arxiv.org/abs/2106.05931">Score-based generative modeling in latent space</a>, 2021.</li> <li>Welling, M. and Teh, Y. W. <a href="https://dl.acm.org/doi/10.5555/3104482.3104568">Bayesian learning via stochastic gradient langevin dynamics</a>. In Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML‚Äô11, pp. 681‚Äì688, Madison, WI, USA, 2011. Omnipress. ISBN 9781450306195.</li> <li>Zhang, H., Feng, R., Yang, Z., Huang, L., Liu, Y., Zhang, Y., Shen, Y., Zhao, D., Zhou, J., and Cheng, F. <a href="https://arxiv.org/abs/2211.16032">Dimensionality-varying diffusion process</a>, 2022.</li> <li>Zheng, H., He, P., Chen, W., and Zhou, M. <a href="https://arxiv.org/abs/2202.09671">Truncated diffusion probabilistic models and diffusion-based adversarial auto-encoders</a>, 2022.</li> </ul>]]></content><author><name>fanpu</name></author><category term="machine-learning"/><summary type="html"><![CDATA[Score-based diffusion models are a promising direction for generative models, as they improve on both likelihood-based approaches like variational autoencoders, as well as adversarial methods like Generative Adversarial Networks (GANs). In this blog post, we survey recent developments in the field centered around the line of results developed in (Song & Ermon, 2019), analyze the current strengths and limitations of score-based diffusion models, and discuss possible future directions that can address its drawbacks. Joint work with Owen Wang.]]></summary></entry><entry><title type="html">a post with custom blockquotes</title><link href="https://fanpu.io/blog/2023/custom-blockquotes/" rel="alternate" type="text/html" title="a post with custom blockquotes"/><published>2023-05-12T19:53:00+00:00</published><updated>2023-05-12T19:53:00+00:00</updated><id>https://fanpu.io/blog/2023/custom-blockquotes</id><content type="html" xml:base="https://fanpu.io/blog/2023/custom-blockquotes/"><![CDATA[<p>This post shows how to add custom styles for blockquotes. Based on <a href="https://github.com/sighingnow/jekyll-gitbook">jekyll-gitbook</a> implementation.</p> <p>We decided to support the same custom blockquotes as in <a href="https://sighingnow.github.io/jekyll-gitbook/jekyll/2022-06-30-tips_warnings_dangers.html">jekyll-gitbook</a>, which are also found in a lot of other sites‚Äô styles. The styles definitions can be found on the <a href="https://github.com/alshedivat/al-folio/blob/master/_sass/_base.scss">_base.scss</a> file, more specifically:</p> <div class="language-scss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Tips, warnings, and dangers */</span>
<span class="nc">.post</span> <span class="nc">.post-content</span> <span class="nt">blockquote</span> <span class="p">{</span>
    <span class="k">&amp;</span><span class="nc">.block-tip</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-warning</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-danger</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>A regular blockquote can be used as following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; This is a regular blockquote</span>
<span class="gt">&gt; and it can be used as usual</span>
</code></pre></div></div> <blockquote> <p>This is a regular blockquote and it can be used as usual</p> </blockquote> <p>These custom styles can be used by adding the specific class to the blockquote, as follows:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### TIP</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; A tip can be used when you want to give advice</span>
<span class="gt">&gt; related to a certain content.</span>
{: .block-tip }
</code></pre></div></div> <blockquote class="block-tip"> <h5 id="tip">TIP</h5> <p>A tip can be used when you want to give advice related to a certain content.</p> </blockquote> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### WARNING</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a warning, and thus should</span>
<span class="gt">&gt; be used when you want to warn the user</span>
{: .block-warning }
</code></pre></div></div> <blockquote class="block-warning"> <h5 id="warning">WARNING</h5> <p>This is a warning, and thus should be used when you want to warn the user</p> </blockquote> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### DANGER</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a danger zone, and thus should</span>
<span class="gt">&gt; be used carefully</span>
{: .block-danger }
</code></pre></div></div> <blockquote class="block-danger"> <h5 id="danger">DANGER</h5> <p>This is a danger zone, and thus should be used carefully</p> </blockquote>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="blockquotes"/><summary type="html"><![CDATA[an example of a blog post with custom blockquotes]]></summary></entry></feed>