<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Scaling Laws for Neural Language Models | Fan Pu  Zeng</title>
    <meta name="author" content="Fan Pu  Zeng">
    <meta name="description" content="Homepage
">
    <meta name="keywords" content="fanpu, fan pu, fanpu zeng, fan pu zeng, zengfanpu, fanpuzeng, fzeng, CMU, cmu, carnegie mellon, cmu courses, school of computer science, scs, machine learning, computer science, ml, theory, courses, course reviews, CS, Jane Street">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon_new.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fanpu.io/summaries/2024-03-23-scaling-laws-for-neural-language-models/">
    <!-- Dark Mode -->
    

    <!-- Twitter cards -->
    <meta name="twitter:site" content="@FanPu_Zeng">
    <meta name="twitter:creator" content="@">
    <meta name="og:title" content="Scaling Laws for Neural Language Models">

    
    <meta name="twitter:card" content="summary">
    <meta name="og:image" content="">
    

    
    <meta name="og:description" content="Homepage
">
    

    <!-- end of Twitter cards -->
  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Fan Pu </span>Zeng</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/courses/">CMU Course Reviews</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cmu-online/">CMU Online</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/summaries/">ML Paper Summaries</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">Repositories</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/summary.html -->



<div style="display:none">
    $$
    \newcommand{\bone}{\mathbf{1}}
    \newcommand{\bbeta}{\mathbf{\beta}}
    \newcommand{\bdelta}{\mathbf{\delta}}
    \newcommand{\bepsilon}{\mathbf{\epsilon}}
    \newcommand{\blambda}{\mathbf{\lambda}}
    \newcommand{\bomega}{\mathbf{\omega}}
    \newcommand{\bpi}{\mathbf{\pi}}
    \newcommand{\bphi}{\mathbf{\phi}}
    \newcommand{\bvphi}{\mathbf{\varphi}}
    \newcommand{\bpsi}{\mathbf{\psi}}
    \newcommand{\bsigma}{\mathbf{\sigma}}
    \newcommand{\btheta}{\mathbf{\theta}}
    \newcommand{\btau}{\mathbf{\tau}}
    \newcommand{\ba}{\mathbf{a}}
    \newcommand{\bb}{\mathbf{b}}
    \newcommand{\bc}{\mathbf{c}}
    \newcommand{\bd}{\mathbf{d}}
    \newcommand{\be}{\mathbf{e}}
    \newcommand{\boldf}{\mathbf{f}}
    \newcommand{\bg}{\mathbf{g}}
    \newcommand{\bh}{\mathbf{h}}
    \newcommand{\bi}{\mathbf{i}}
    \newcommand{\bj}{\mathbf{j}}
    \newcommand{\bk}{\mathbf{k}}
    \newcommand{\bell}{\mathbf{\ell}}
    \newcommand{\bm}{\mathbf{m}}
    \newcommand{\bn}{\mathbf{n}}
    \newcommand{\bo}{\mathbf{o}}
    \newcommand{\bp}{\mathbf{p}}
    \newcommand{\bq}{\mathbf{q}}
    \newcommand{\br}{\mathbf{r}}
    \newcommand{\bs}{\mathbf{s}}
    \newcommand{\bt}{\mathbf{t}}
    \newcommand{\bu}{\mathbf{u}}
    \newcommand{\bv}{\mathbf{v}}
    \newcommand{\bw}{\mathbf{w}}
    \newcommand{\bx}{\mathbf{x}}
    \newcommand{\by}{\mathbf{y}}
    \newcommand{\bz}{\mathbf{z}}
    \newcommand{\bA}{\mathbf{A}}
    \newcommand{\bB}{\mathbf{B}}
    \newcommand{\bC}{\mathbf{C}}
    \newcommand{\bD}{\mathbf{D}}
    \newcommand{\bE}{\mathbf{E}}
    \newcommand{\bF}{\mathbf{F}}
    \newcommand{\bG}{\mathbf{G}}
    \newcommand{\bH}{\mathbf{H}}
    \newcommand{\bI}{\mathbf{I}}
    \newcommand{\bJ}{\mathbf{J}}
    \newcommand{\bK}{\mathbf{K}}
    \newcommand{\bL}{\mathbf{L}}
    \newcommand{\bM}{\mathbf{M}}
    \newcommand{\bN}{\mathbf{N}}
    \newcommand{\bP}{\mathbf{P}}
    \newcommand{\bQ}{\mathbf{Q}}
    \newcommand{\bR}{\mathbf{R}}
    \newcommand{\bS}{\mathbf{S}}
    \newcommand{\bT}{\mathbf{T}}
    \newcommand{\bU}{\mathbf{U}}
    \newcommand{\bV}{\mathbf{V}}
    \newcommand{\bW}{\mathbf{W}}
    \newcommand{\bX}{\mathbf{X}}
    \newcommand{\bY}{\mathbf{Y}}
    \newcommand{\bZ}{\mathbf{Z}}

    \newcommand{\bsa}{\boldsymbol{a}}
    \newcommand{\bsb}{\boldsymbol{b}}
    \newcommand{\bsc}{\boldsymbol{c}}
    \newcommand{\bsd}{\boldsymbol{d}}
    \newcommand{\bse}{\boldsymbol{e}}
    \newcommand{\bsoldf}{\boldsymbol{f}}
    \newcommand{\bsg}{\boldsymbol{g}}
    \newcommand{\bsh}{\boldsymbol{h}}
    \newcommand{\bsi}{\boldsymbol{i}}
    \newcommand{\bsj}{\boldsymbol{j}}
    \newcommand{\bsk}{\boldsymbol{k}}
    \newcommand{\bsell}{\boldsymbol{\ell}}
    \newcommand{\bsm}{\boldsymbol{m}}
    \newcommand{\bsn}{\boldsymbol{n}}
    \newcommand{\bso}{\boldsymbol{o}}
    \newcommand{\bsp}{\boldsymbol{p}}
    \newcommand{\bsq}{\boldsymbol{q}}
    \newcommand{\bsr}{\boldsymbol{r}}
    \newcommand{\bss}{\boldsymbol{s}}
    \newcommand{\bst}{\boldsymbol{t}}
    \newcommand{\bsu}{\boldsymbol{u}}
    \newcommand{\bsv}{\boldsymbol{v}}
    \newcommand{\bsw}{\boldsymbol{w}}
    \newcommand{\bsx}{\boldsymbol{x}}
    \newcommand{\bsy}{\boldsymbol{y}}
    \newcommand{\bsz}{\boldsymbol{z}}
    \newcommand{\bsA}{\boldsymbol{A}}
    \newcommand{\bsB}{\boldsymbol{B}}
    \newcommand{\bsC}{\boldsymbol{C}}
    \newcommand{\bsD}{\boldsymbol{D}}
    \newcommand{\bsE}{\boldsymbol{E}}
    \newcommand{\bsF}{\boldsymbol{F}}
    \newcommand{\bsG}{\boldsymbol{G}}
    \newcommand{\bsH}{\boldsymbol{H}}
    \newcommand{\bsI}{\boldsymbol{I}}
    \newcommand{\bsJ}{\boldsymbol{J}}
    \newcommand{\bsK}{\boldsymbol{K}}
    \newcommand{\bsL}{\boldsymbol{L}}
    \newcommand{\bsM}{\boldsymbol{M}}
    \newcommand{\bsN}{\boldsymbol{N}}
    \newcommand{\bsP}{\boldsymbol{P}}
    \newcommand{\bsQ}{\boldsymbol{Q}}
    \newcommand{\bsR}{\boldsymbol{R}}
    \newcommand{\bsS}{\boldsymbol{S}}
    \newcommand{\bsT}{\boldsymbol{T}}
    \newcommand{\bsU}{\boldsymbol{U}}
    \newcommand{\bsV}{\boldsymbol{V}}
    \newcommand{\bsW}{\boldsymbol{W}}
    \newcommand{\bsX}{\boldsymbol{X}}
    \newcommand{\bsY}{\boldsymbol{Y}}
    \newcommand{\bsZ}{\boldsymbol{Z}}

    \newcommand{\calA}{\mathcal{A}}
    \newcommand{\calB}{\mathcal{B}}
    \newcommand{\calC}{\mathcal{C}}
    \newcommand{\calD}{\mathcal{D}}
    \newcommand{\calE}{\mathcal{E}}
    \newcommand{\calF}{\mathcal{F}}
    \newcommand{\calG}{\mathcal{G}}
    \newcommand{\calH}{\mathcal{H}}
    \newcommand{\calI}{\mathcal{I}}
    \newcommand{\calJ}{\mathcal{J}}
    \newcommand{\calK}{\mathcal{K}}
    \newcommand{\calL}{\mathcal{L}}
    \newcommand{\calM}{\mathcal{M}}
    \newcommand{\calN}{\mathcal{N}}
    \newcommand{\calO}{\mathcal{O}}
    \newcommand{\calP}{\mathcal{P}}
    \newcommand{\calQ}{\mathcal{Q}}
    \newcommand{\calR}{\mathcal{R}}
    \newcommand{\calS}{\mathcal{S}}
    \newcommand{\calT}{\mathcal{T}}
    \newcommand{\calU}{\mathcal{U}}
    \newcommand{\calV}{\mathcal{V}}
    \newcommand{\calW}{\mathcal{W}}
    \newcommand{\calX}{\mathcal{X}}
    \newcommand{\calY}{\mathcal{Y}}
    \newcommand{\calZ}{\mathcal{Z}}

    \newcommand{\R}{\mathbb{R}}
    \newcommand{\C}{\mathbb{C}}
    \newcommand{\N}{\mathbb{N}}
    \newcommand{\Z}{\mathbb{Z}}
    \newcommand{\F}{\mathbb{F}}
    \newcommand{\Q}{\mathbb{Q}}

    \DeclareMathOperator*{\argmax}{arg\,max}
    \DeclareMathOperator*{\argmin}{arg\,min}
    \newcommand{\nnz}[1]{\mbox{nnz}(#1)}
    \newcommand{\dotprod}[2]{\langle #1, #2 \rangle}

    \newcommand{\ignore}[1]{}

    \let\Pr\relax
    \DeclareMathOperator*{\Pr}{\mathbf{Pr}}
    \newcommand{\E}{\mathbb{E}}
    \DeclareMathOperator*{\Ex}{\mathbf{E}}
    \DeclareMathOperator*{\Var}{\mathbf{Var}}
    \DeclareMathOperator*{\Cov}{\mathbf{Cov}}
    \DeclareMathOperator*{\stddev}{\mathbf{stddev}}
    \DeclareMathOperator*{\avg}{avg}

    \DeclareMathOperator{\poly}{poly}
    \DeclareMathOperator{\polylog}{polylog}
    \DeclareMathOperator{\size}{size}
    \DeclareMathOperator{\sgn}{sgn}
    \DeclareMathOperator{\dist}{dist}
    \DeclareMathOperator{\vol}{vol}
    \DeclareMathOperator{\spn}{span}
    \DeclareMathOperator{\supp}{supp}
    \DeclareMathOperator{\tr}{tr}
    \DeclareMathOperator{\Tr}{Tr}
    \DeclareMathOperator{\codim}{codim}
    \DeclareMathOperator{\diag}{diag}

    \newcommand{\PTIME}{\mathsf{P}}
    \newcommand{\LOGSPACE}{\mathsf{L}}
    \newcommand{\ZPP}{\mathsf{ZPP}}
    \newcommand{\RP}{\mathsf{RP}}
    \newcommand{\BPP}{\mathsf{BPP}}
    \newcommand{\P}{\mathsf{P}}
    \newcommand{\NP}{\mathsf{NP}}
    \newcommand{\TC}{\mathsf{TC}}
    \newcommand{\AC}{\mathsf{AC}}
    \newcommand{\SC}{\mathsf{SC}}
    \newcommand{\SZK}{\mathsf{SZK}}
    \newcommand{\AM}{\mathsf{AM}}
    \newcommand{\IP}{\mathsf{IP}}
    \newcommand{\PSPACE}{\mathsf{PSPACE}}
    \newcommand{\EXP}{\mathsf{EXP}}
    \newcommand{\MIP}{\mathsf{MIP}}
    \newcommand{\NEXP}{\mathsf{NEXP}}
    \newcommand{\BQP}{\mathsf{BQP}}
    \newcommand{\distP}{\mathsf{dist\textbf{P}}}
    \newcommand{\distNP}{\mathsf{dist\textbf{NP}}}

    \newcommand{\eps}{\epsilon}
    \newcommand{\lam}{\lambda}
    \newcommand{\dleta}{\delta}
    \newcommand{\simga}{\sigma}
    \newcommand{\vphi}{\varphi}
    \newcommand{\la}{\langle}
    \newcommand{\ra}{\rangle}
    \newcommand{\wt}[1]{\widetilde{#1}}
    \newcommand{\wh}[1]{\widehat{#1}}
    \newcommand{\ol}[1]{\overline{#1}}
    \newcommand{\ul}[1]{\underline{#1}}
    \newcommand{\ot}{\otimes}
    \newcommand{\zo}{\{0,1\}}
    \newcommand{\co}{:} %\newcommand{\co}{\colon}
    \newcommand{\bdry}{\partial}
    \newcommand{\grad}{\nabla}
    \newcommand{\transp}{^\intercal}
    \newcommand{\inv}{^{-1}}
    \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff}
    \newcommand{\half}{\tfrac{1}{2}}
    \newcommand{\bbone}{\mathbbm 1}
    \newcommand{\Id}{\bbone}

    \newcommand{\SAT}{\mathsf{SAT}}

    \newcommand{\bcalG}{\boldsymbol{\calG}}
    \newcommand{\calbG}{\bcalG}
    \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX}
    \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY}
    \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ}
    $$
</div>

<!-- hack to inherit CSS styles to suppress list counts -->
<div class="publications">
    <ol class="bibliography"><li>
<!-- _layouts/custom_bib.html -->
      <div class="row no-count">
        <!-- Entry bib key -->
        <div id="2001.08361v1" class="col-sm-12">
        <!-- Title -->
        <div class="title"><h2>
            <a href="http://arxiv.org/abs/2001.08361v1" rel="external nofollow noopener" target="_blank">
                Scaling Laws for Neural Language Models
            </a>
          </h2></div>
        <!-- Author -->
        <div class="author">
        

        Jared Kaplan, Sam McCandlish, Tom Henighan, and
          <span class="more-authors" title="click to view 7 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '7 more authors' ? 'Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei' : '7 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">7 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em></em> Jan 2020
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          <!--
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> -->
            <a href="http://arxiv.org/abs/2001.08361v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
          <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            
            <a href="https://arxiv.org/pdf/2001.08361v1.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          <!-- -->
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2001.08361"></span>
          </div>

          <!-- Hidden abstract block -->
          <h3>Paper Abstract</h3>
          <div class="abstract">
            <p>We study empirical scaling laws for language model performance on the
                   cross-entropy loss. The loss scales as a power-law with model size, dataset
                   size, and the amount of compute used for training, with some trends spanning
                   more than seven orders of magnitude. Other architectural details such as
                   network width or depth have minimal effects within a wide range. Simple
                   equations govern the dependence of overfitting on model/dataset size and the
                   dependence of training speed on model size. These relationships allow us to
                   determine the optimal allocation of a fixed compute budget. Larger models are
                   significantly more sample-efficient, such that optimally compute-efficient
                   training involves training very large models on a relatively modest amount of
                   data and stopping significantly before convergence.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">2001.08361v1</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Scaling Laws for Neural Language Models}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2001.08361v1}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">eprintnover</span> <span class="p">=</span> <span class="s">{2001.08361}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>
</div>

<div class="post">
  <article class="post-content">
    
    <div id="markdown-content">
      <h3 id="four-important-things">Four Important Things</h3>

<h4 id="1-power-law-relationship-of-l-with-respect-to-n-c-d">1. Power Law Relationship of \(L\) With Respect To \(N, C, D\)</h4>

<p>The paper shows empirically that language modeling loss \(L\) is a predictable function of:</p>

<ul>
  <li>\(N\), the number of model parameters (excluding embeddings).</li>
  <li>\(D\), the size of the dataset,</li>
  <li>\(C\), the amount of compute used for training,</li>
</ul>

<p>when they are not bottlenecked by any of the other two parameters.</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/summaries/scaling_laws_graph.webp" class="z-depth-1 center" width="800px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>
</figure>

<p>The paper also observed that larger models are more sample-efficient than smaller models,
being able to reach the same loss with fewer samples.</p>

<p>In the graph below, we see that the curves corresponding to the \(10^9\)
parameter model achieves a much smaller test loss compared to the \(10^3\) parameter model
at all regimes:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/summaries/scaling_laws_samples.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>
</figure>

<p>In addition, they surprisingly found that for a fixed compute budget and no
restrictions on model size or available data, the optimal loss that can be
achieved is not by training a model to convergence, but rather training a very
large model that significantly stops short of convergence. One’s intuition
for smaller models would be that models should be trained to convergence,
and this often happens in practice due to hardware constraints.</p>

<p>In the graph below, we see that the curves flatten out towards the end with
increasing compute, which provides intuition on why it is preferable to
over-parameterize the model further in order to stay within the non-flattening
regime of decaying test loss:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/summaries/scaling_laws_optimal.webp" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>
</figure>

<h4 id="2-transformer-performance-depends-weakly-on-shape-parameters">2. Transformer Performance Depends Weakly on Shape Parameters</h4>

<p>We parameterize the Transformer architecture by the following hyperparameters:</p>

<ul>
  <li>\(n_{\textrm{layer}}\), the number of Transformer block layers</li>
  <li>\(d_{\textrm{model}}\), the dimension of the residual stream (i.e the dimensions output
by the residual connection and layer normalization components, which is the dimension preserved
between each Transformer block)</li>
  <li>\(d_{\textrm{ff}}\), the dimension of the feed-forward layer</li>
  <li>\(d_{\textrm{attn}}\), the dimension of the attention output</li>
  <li>\(n_{\textrm{heads}}\), the number of attention heads per layer</li>
</ul>

<p>To jolt your memory of how these different components come together again,
here’s the original Transformer architecture from Vaswani et. al:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/summaries/scaling_laws_transformer_recap.webp" class="z-depth-1 center" width="500px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>
</figure>

<p>The authors showed that the performance of Transformers only depends weakly
on \(n_{\textrm{layer}}\), \(n_{\textrm{heads}}\), and \(d_{\textrm{ff}}\)
when the total number of parameters is held fixed. It was not elaborated
what “weakly” means precisely, but I understood it to mean that the relationship
is something that would be swallowed in the context of big-O analysis.</p>

<p>To study the relationships of performance on these hyperparameters,
they always scaled
\(d_{\mathrm{model}}\) accordingly to keep \(N \approx 12 n_{\mathrm{layer}}
d_{\mathrm{model}}^2\) fixed.
\(d\_{\mathrm{model}}\) controls the width of the Transformer, and makes
sense to be the one modified since it can allow the most significant parameter.
changes without changing its own value too significantly.</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/summaries/scaling_laws_model_shape.webp" class="z-depth-1 center" width="800px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>
</figure>

<ol>
  <li>
    <p>(Right plot) For \(n_{\mathrm{heads}}\), we see that loss increases as
\(n_{\mathrm{heads}}\) decreases, but this can be offset by increasing
\(d_{\mathrm{model}}\), which leads to their remark in the plot.</p>
  </li>
  <li>
    <p>(Middle plot) For \(n_{\mathrm{layer}}\), as the number of layer decreases
and \(d_{\mathrm{model}}\) increases, the loss increase goes down and then
goes up again across a range of architectures with different \(N\).
This means there is a sweet spot for trading off between the two parameters.</p>
  </li>
  <li>
    <p>(Left plot) For \(d_{\mathrm{ff}}\), as \(d_{\mathrm{ff}}\) increases
and \(d_{\mathrm{model}}\) decreases, the increase in loss goes down
slightly, before growing a lot more. This also hints at a sweet
spot between the two parameters.</p>
  </li>
</ol>

<p>The fact that the loss only weakly depends on model shape is desirable since it
makes the analysis of scaling laws more robust (minor changes in architecture should
not result in major changes in the results, much like how minor tweaks to the Turing
machine architecture should not change asymptotic analysis significantly).</p>

<h4 id="3-comparisons-with-lstms">3. Comparisons with LSTMs</h4>

<p>It would be interesting to understand how the scaling laws of Transformers compare to the classical LSTM.</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/summaries/scaling_laws_lstm.webp" class="z-depth-1 center" width="800px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>
</figure>

<p>On the left plot, we see that the test loss of Transformers asymptotically beat
that of LSTMs.</p>

<p>On the right plot (red plots are for LSTM, blue plots for Transformers), we see that
while LSTM and Transformer curves have lower per-token losses at all indices as the number of parameters
increases, this trend plateaus for LSTMs beyond some token index, but continues to decay
for Transformers. The per-token loss should decrease as the index increases since there is more
context to constrain the possibilities of the next token.</p>

<h4 id="4-generalization-among-data-distributions">4. Generalization Among Data Distributions</h4>

<p>The authors also verified that the scaling laws hold
for different data distributions:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/summaries/scaling_laws_data.webp" class="z-depth-1 center" width="500px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>
</figure>

<h3 id="most-glaring-deficiency">Most Glaring Deficiency</h3>

<p>I thought the paper was generally very comprehensive and analyzed the scaling
laws from many different dimensions. However, it could have gone a step further
to distill the learnings from the results to provide concrete guidelines on
how to choose \(N, C, D\) when training LLMs.</p>

<h3 id="conclusions-for-future-work">Conclusions for Future Work</h3>

<p>Given a fixed compute budget, one can choose \(N, D\) appropriately to achieve a desired loss.</p>

<p>While the paper provided empirical evidence for the scaling laws, there is not
yet any theoretical understanding of why this is the case, which would be an interesting
line of future research.</p>

    </div>
  </article>

  <p class="post-meta">Written March 23, 2024</p>
<div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "fanpu/website",
        "data-repo-id": "R_kgDOIpOodA",
        "data-category": "General",
        "data-category-id": "DIC_kwDOIpOodM4CTKDC",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>


      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Fan Pu  Zeng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.
Last updated: August 09, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Enable Tooltips -->
  <script type="text/javascript">
  $(function () {$('[data-toggle="tooltip"]').tooltip()})
  </script>
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      loader: {load: ['[tex]/mathtools']},
      tex: {
        packages: {'[+]': ['mathtools', 'ams']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          addMenu: []
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
